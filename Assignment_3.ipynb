{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-bhutan",
   "metadata": {},
   "source": [
    "#  Predicting Root Causes of Safety Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-packing",
   "metadata": {},
   "source": [
    "## Classifying Safety Observations using Natural Language Learning Process to predict root causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "above-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-planner",
   "metadata": {},
   "source": [
    "## 1. Importing Data\n",
    "\n",
    "The data which I am using here is a dump from the Safety Observation System of a Major Company. The each row of data consists of the observation description along with pre-selected category and root cause (Classes).\n",
    "\n",
    "The objective of this program would be to develop a Machine Learning Model which would be able to predict the root cause based on this historical data for any new observation. By implication this exercise is an Natural Language Processing Exercise. As we will be extracting features from text (Observation Description) and using those for training our classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-passenger",
   "metadata": {},
   "source": [
    "The overall plan includes\n",
    "\n",
    "1. Cleaning the data by removing stops and performing EDA.\n",
    "2. Feature extraction to arrive on most important features.\n",
    "3. Training our different models and tuning hyper-parameters.\n",
    "4. Evaluation of Models and selection of best model.\n",
    "5. Summary and Future action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chemical-idaho",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aa-minhaj\\sklearn-venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3155: DtypeWarning: Columns (0,1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UA/UC</th>\n",
       "      <th>Observation</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>worker used drilling machine without green tag.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>Worker used a plug top for multiple  electrica...</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>worker use drilling machine at site without gr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>Site supervisor is not available at site whi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>worker use electric grinder without green tag.</td>\n",
       "      <td>No Inspection Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 UA/UC  \\\n",
       "0  Electrically Unsafe (Safety Hazard)   \n",
       "1  Electrically Unsafe (Safety Hazard)   \n",
       "2  Electrically Unsafe (Safety Hazard)   \n",
       "3  Electrically Unsafe (Safety Hazard)   \n",
       "4  Electrically Unsafe (Safety Hazard)   \n",
       "\n",
       "                                         Observation  \\\n",
       "0    worker used drilling machine without green tag.   \n",
       "1  Worker used a plug top for multiple  electrica...   \n",
       "2  worker use drilling machine at site without gr...   \n",
       "3    Site supervisor is not available at site whi...   \n",
       "4     worker use electric grinder without green tag.   \n",
       "\n",
       "                                Sub-Category  \n",
       "0                                        NaN  \n",
       "1   No Plug top                               \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                         No Inspection Done  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us import the data\n",
    "df_obs = pd.read_csv('ehs_obs_data.csv')\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protecting-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the dataset is 3 and the no of rows is 1028493\n"
     ]
    }
   ],
   "source": [
    "#Let us check the no of columns and rows\n",
    "\n",
    "print(f'The number of columns in the dataset is {df_obs.shape[1]} and the no of rows is {df_obs.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-contamination",
   "metadata": {},
   "source": [
    "The target variable here will be 'Sub-Category' and the input variable will be 'Observation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-advantage",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-indie",
   "metadata": {},
   "source": [
    "In this step we will try to \n",
    "\n",
    "1. Bring all words on same case (lower case).\n",
    "2. Detect the missing values and treat them accordingly.\n",
    "3. Drop various Stop words which don't add any information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personal-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us first bring in all the required modules\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fuzzy-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first drop the 'UA/UC' Column which is not required for this exercise.\n",
    "\n",
    "df_obs.drop('UA/UC', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "undefined-thunder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worker used drilling machine without green tag.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worker used a plug top for multiple  electrica...</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worker use drilling machine at site without gr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Site supervisor is not available at site whi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worker use electric grinder without green tag.</td>\n",
       "      <td>No Inspection Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Observation  \\\n",
       "0    worker used drilling machine without green tag.   \n",
       "1  Worker used a plug top for multiple  electrica...   \n",
       "2  worker use drilling machine at site without gr...   \n",
       "3    Site supervisor is not available at site whi...   \n",
       "4     worker use electric grinder without green tag.   \n",
       "\n",
       "                                Sub-Category  \n",
       "0                                        NaN  \n",
       "1   No Plug top                               \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                         No Inspection Done  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fourth-taste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Observation     961881\n",
       "Sub-Category    996100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the missing values\n",
    "\n",
    "df_obs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-vehicle",
   "metadata": {},
   "source": [
    "Looks like we have lots of blank rows. However, this may be due to faulty data extraction process and we can easily drop these without losing any meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping values\n",
    "df_obs.dropna(subset = ['Observation'], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mechanical-tract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66612,\n",
       " Observation         0\n",
       " Sub-Category    34219\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check how many values are missing in target variable after dropping missing feature rows\n",
    "df_obs.shape[0], df_obs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-officer",
   "metadata": {},
   "source": [
    "We can see that we missing lables for almost half of the data. Now this leaves a lot of room for improvement in data collection and labelling process. Unfortunately, we have to leave out all these rows from our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "another-regulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32393,\n",
       " Observation     0\n",
       " Sub-Category    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#droping values\n",
    "df_obs.dropna(subset = ['Sub-Category'], axis = 0, inplace = True)\n",
    "df_obs.shape[0], df_obs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-spanish",
   "metadata": {},
   "source": [
    "So, in the end we are now left with 32,393 Nos of individual observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spoken-press",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check for the duplicates in the observation column\n",
    "\n",
    "df_obs.duplicated(subset = ['Observation'], keep = 'first').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-great",
   "metadata": {},
   "source": [
    "Looks like we have quite a few number of duplicated observation data. It would be better to drop these duplicate columns to avoid any bias in our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "circular-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs.drop_duplicates(subset = 'Observation', keep = 'first', ignore_index = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authorized-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the dataset is 2 and the no of rows is 25587\n"
     ]
    }
   ],
   "source": [
    "# final size of the dataframe\n",
    "print(f'The number of columns in the dataset is {df_obs.shape[1]} and the no of rows is {df_obs.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-nitrogen",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling - Summary\n",
    "\n",
    "* We dropped the null values and duplicated observations to reduce the dimensions of the data\n",
    "* This also ensures that we avoid assigining higher weightage to certain duplicated observations at the time of feature extraction.\n",
    "* We also dropped UA/UC columns as we want to explore this dataset purely from NLP perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-occasion",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-google",
   "metadata": {},
   "source": [
    "In this step, we will\n",
    "\n",
    "1. Remove punctuation marks and anyother extra space.\n",
    "2. Lemmatize all the words so that different words stemming from same root word are not counted as separate features.\n",
    "3. Label Encode our target variables\n",
    "4. Check the weight of each class and discuss on the possibility of the oversampling, Undersampling or resampling\n",
    "5. Extract features from our observation text using tfidf method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-industry",
   "metadata": {},
   "source": [
    "First we will remove the punctuation marks and extra spaces in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unable-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = map(str, list(range(0, 100)))\n",
    "for num in numbers:\n",
    "    df_obs.Observation =  df_obs.Observation.str.replace(num, ' ', regex = False)\n",
    "special_charecters = ['?', '#', '-', \"'\", '&', '/', '.', ',', '(', ')', ':',';']\n",
    "for char in special_charecters:\n",
    "     df_obs.Observation =  df_obs.Observation.str.replace(char, ' ', regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chronic-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs.Observation = df_obs.Observation.apply(lambda x: \" \".join(x.split()))\n",
    "df_obs.Observation = df_obs.Observation.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-insider",
   "metadata": {},
   "source": [
    "Lets lemmatize our observation text for better results. \n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-ghana",
   "metadata": {},
   "source": [
    "We will use WordNetLemmatizer from NTLK library for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "settled-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlemmatize = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "experimental-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(word_list):\n",
    "    lem_word_list = [wordlemmatize.lemmatize(word, pos = 'v') for word in word_list.split()]\n",
    "    new_lem_word_list = [wordlemmatize.lemmatize(word) for word in lem_word_list]\n",
    "    return \" \".join(new_lem_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "portable-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs.Observation = df_obs.Observation.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "medieval-calendar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15734</th>\n",
       "      <td>Jcb operataor engage without screen</td>\n",
       "      <td>Workmen engaged without screening/ Induction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24276</th>\n",
       "      <td>Adequate approach be not provide to go inside ...</td>\n",
       "      <td>Unsafe/ Blocked Access &amp; Egress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10713</th>\n",
       "      <td>Back fill be not do near of the SPS wall</td>\n",
       "      <td>No/ Improper Backfilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>Single core wire be use for electrical connection</td>\n",
       "      <td>Unsafe Electrical connections/Practices       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5778</th>\n",
       "      <td>Electrical wire find without pluck top</td>\n",
       "      <td>Unsafe Electrical connections/Practices       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16349</th>\n",
       "      <td>workman Mr Aniket kailash be find to be engage...</td>\n",
       "      <td>Workmen engaged without screening/ Induction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>insulation provide to damage electrical cable ...</td>\n",
       "      <td>Cable Insulation Issue/Bare Wires             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17352</th>\n",
       "      <td>Backfilling be not do yesterday</td>\n",
       "      <td>No/ Improper Backfilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>worker use electric cutter without guard</td>\n",
       "      <td>Unsafe Electrical connections/Practices       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>Trench be keep unattended after lower the pipe...</td>\n",
       "      <td>No Hard Barricading</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Observation  \\\n",
       "15734                Jcb operataor engage without screen   \n",
       "24276  Adequate approach be not provide to go inside ...   \n",
       "10713           Back fill be not do near of the SPS wall   \n",
       "5833   Single core wire be use for electrical connection   \n",
       "5778              Electrical wire find without pluck top   \n",
       "16349  workman Mr Aniket kailash be find to be engage...   \n",
       "1003   insulation provide to damage electrical cable ...   \n",
       "17352                    Backfilling be not do yesterday   \n",
       "434             worker use electric cutter without guard   \n",
       "10885  Trench be keep unattended after lower the pipe...   \n",
       "\n",
       "                                            Sub-Category  \n",
       "15734       Workmen engaged without screening/ Induction  \n",
       "24276                    Unsafe/ Blocked Access & Egress  \n",
       "10713                           No/ Improper Backfilling  \n",
       "5833   Unsafe Electrical connections/Practices       ...  \n",
       "5778   Unsafe Electrical connections/Practices       ...  \n",
       "16349       Workmen engaged without screening/ Induction  \n",
       "1003   Cable Insulation Issue/Bare Wires             ...  \n",
       "17352                           No/ Improper Backfilling  \n",
       "434    Unsafe Electrical connections/Practices       ...  \n",
       "10885                                No Hard Barricading  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check our cleaned dataframe\n",
    "\n",
    "df_obs.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-eating",
   "metadata": {},
   "source": [
    "If you watch closely lemmatization has taken affect on many words for ex- against index 7778 the word 'been' has been stemmed to 'be', the word 'kept' has been changed back to 'keep' at 7656. In reality, these words would have been extracted as separate features thereby increasing the overall feature dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-teacher",
   "metadata": {},
   "source": [
    "Now lets encode out target variables to prepare them for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "understood-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df_obs['Sub-Category'] = labelencoder.fit_transform(df_obs['Sub-Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "missing-interference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_obs['Sub-Category'].value_counts(normalize = True), bins = 25,);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-footage",
   "metadata": {},
   "source": [
    "This shows that our classes have quite severe imbalance. And this can impact our final model's performance. Instead of opting for any sampling approach lets try to model our data using imbalanced classes by maintaining class imbalance in the train and test split. We will use class_weight feature wherever available to penalize the imbalance. Also lets save all our label names and codes for future references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hired-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Code</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Backhoe Operator engaged without screening and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cable Insulation Issue/Bare Wires             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cable issues(Routing, Sub-standard etc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cable/Wire run across accessway</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label Code                                              Label\n",
       "0           0           No Plug top                             \n",
       "1           1  Backhoe Operator engaged without screening and...\n",
       "2           2  Cable Insulation Issue/Bare Wires             ...\n",
       "3           3            Cable issues(Routing, Sub-standard etc)\n",
       "4           4                    Cable/Wire run across accessway"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_codes = pd.DataFrame(labelencoder.classes_).reset_index()\n",
    "label_codes.columns = ['Label Code', 'Label']\n",
    "label_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-miracle",
   "metadata": {},
   "source": [
    "Next and probably most important step for modelling is to extract features from text data. Now there are many methods to acheive this objective but the most important from all is Term Frequency and Inverse Document Frequnecy Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-antenna",
   "metadata": {},
   "source": [
    "In information retrieval, tf–idf, TF*IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-providence",
   "metadata": {},
   "source": [
    "Another important term that you will come across would be N-grams. In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
    "\n",
    "Using Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\".\n",
    "\n",
    "For example a feature - 'Safety' is unigram whereas a feature 'Safety Helmet' would be bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "connected-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets instantiate our tfidfvectorizer by setting feature output for both unigrams and bigrams and Stop words from English\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,2),\n",
    "                        stop_words = 'english',\n",
    "                        min_df = 10,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-microwave",
   "metadata": {},
   "source": [
    "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Also, I have set min_df = 10, which means words that occurred in too few documents, in this case less than 10, should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-directory",
   "metadata": {},
   "source": [
    "I would higly recommend the reader to go through sklearn's working with text data resources. https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-scotland",
   "metadata": {},
   "source": [
    "At this point it would be prudent to first split our data into training and test sets and then applying fit_tranform only on the training set using the fit to tranform our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "final-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "patent-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_obs.Observation, df_obs['Sub-Category'],\n",
    "                                                    test_size = 0.3, random_state = 1000,\n",
    "                                                    stratify = df_obs['Sub-Category']\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bound-shock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets check the class imbalance in both of train and test splits\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.hist(y_train.value_counts(normalize = True), bins = 25,);\n",
    "ax1.set(title = 'Training Set');\n",
    "ax2.hist(y_test.value_counts(normalize = True), bins = 25,);\n",
    "ax2.set(title = 'Test Set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-mobility",
   "metadata": {},
   "source": [
    "We have used the stratify attribute in train_test_split. So, the class distribution is maintained in train and test splits. we can move ahead and extract feature using our tfidfvectorizer class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "atmospheric-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix contains 17910 observations with 2582 features\n"
     ]
    }
   ],
   "source": [
    "features = tfidf.fit_transform(X_train, y_train).toarray()\n",
    "\n",
    "print(f'The feature matrix contains {features.shape[0]} observations with {features.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "academic-deputy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>absence</th>\n",
       "      <th>access</th>\n",
       "      <th>access approach</th>\n",
       "      <th>access area</th>\n",
       "      <th>access arrangement</th>\n",
       "      <th>access available</th>\n",
       "      <th>access block</th>\n",
       "      <th>access egress</th>\n",
       "      <th>access filter</th>\n",
       "      <th>...</th>\n",
       "      <th>workplace screen</th>\n",
       "      <th>wtp</th>\n",
       "      <th>xi</th>\n",
       "      <th>xi im</th>\n",
       "      <th>yadav</th>\n",
       "      <th>yadav engage</th>\n",
       "      <th>yard</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone area</th>\n",
       "      <th>zone site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2582 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  absence  access  access approach  access area  access arrangement  \\\n",
       "0  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "1  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "2  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "3  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "4  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "\n",
       "   access available  access block  access egress  access filter  ...  \\\n",
       "0               0.0           0.0            0.0            0.0  ...   \n",
       "1               0.0           0.0            0.0            0.0  ...   \n",
       "2               0.0           0.0            0.0            0.0  ...   \n",
       "3               0.0           0.0            0.0            0.0  ...   \n",
       "4               0.0           0.0            0.0            0.0  ...   \n",
       "\n",
       "   workplace screen  wtp   xi  xi im  yadav  yadav engage  yard  zone  \\\n",
       "0               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "1               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "2               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "3               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "4               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "\n",
       "   zone area  zone site  \n",
       "0        0.0        0.0  \n",
       "1        0.0        0.0  \n",
       "2        0.0        0.0  \n",
       "3        0.0        0.0  \n",
       "4        0.0        0.0  \n",
       "\n",
       "[5 rows x 2582 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us create the feature matrix for our future reference\n",
    "\n",
    "feature_matrix = pd.DataFrame(data = features, columns = tfidf.get_feature_names())\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-shelf",
   "metadata": {},
   "source": [
    "That's a lot of features and it would be tedious and iefficient to fit all these features to the model. So, it would be better at this stage that to select features that are most important and that can acheive maximum gains for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-broadcasting",
   "metadata": {},
   "source": [
    "We will do this by computing chi-squared stats between each non-negative feature and class.\n",
    "\n",
    "This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ancient-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "developmental-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running chi2 test and storing the chi statistics and p_value\n",
    "\n",
    "chi2_features, p_value = chi2(feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pharmaceutical-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forming the dataframe with features, chi statistics, and p_value\n",
    "\n",
    "df_chi2_features = pd.DataFrame(data = [feature_matrix.columns, chi2_features, p_value]).T\n",
    "df_chi2_features.columns = ['Feature_name', 'Chi value', 'p_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-punch",
   "metadata": {},
   "source": [
    "Now, this is the time for final feature selection. We form our hyothesis here with the significance level of alpha = 0.05.\n",
    "\n",
    "Thus our Null hypothesis is that there is no significance association between the input variable and target variable. While our alternate hypothesis is that there is significance association between the input variable and target variable.\n",
    "\n",
    "Consequently, we will select only those features which have p_value less that 0.05 (Significance level), so that we can reject our null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "varying-horizon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_name</th>\n",
       "      <th>Chi value</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>access</td>\n",
       "      <td>2632.696183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>harness</td>\n",
       "      <td>3083.436262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>height pas</td>\n",
       "      <td>2170.761754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>helmet</td>\n",
       "      <td>6901.796873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>hoe</td>\n",
       "      <td>2082.010137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>slab area</td>\n",
       "      <td>89.87553</td>\n",
       "      <td>0.046525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>ram nagar</td>\n",
       "      <td>89.862447</td>\n",
       "      <td>0.046616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>block</td>\n",
       "      <td>89.811884</td>\n",
       "      <td>0.04697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>cable observe</td>\n",
       "      <td>89.54714</td>\n",
       "      <td>0.048858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>near excavation</td>\n",
       "      <td>89.476313</td>\n",
       "      <td>0.049374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature_name    Chi value   p_value\n",
       "2              access  2632.696183       0.0\n",
       "898           harness  3083.436262       0.0\n",
       "925        height pas  2170.761754       0.0\n",
       "930            helmet  6901.796873       0.0\n",
       "939               hoe  2082.010137       0.0\n",
       "...               ...          ...       ...\n",
       "2124        slab area     89.87553  0.046525\n",
       "1847        ram nagar    89.862447  0.046616\n",
       "202             block    89.811884   0.04697\n",
       "277     cable observe     89.54714  0.048858\n",
       "1425  near excavation    89.476313  0.049374\n",
       "\n",
       "[1303 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chi2_features.loc[df_chi2_features['p_value']< 0.05].sort_values(by = 'p_value', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-essex",
   "metadata": {},
   "source": [
    "Here we can see that we have around 1303 features which are statistically significant thus we can retain them for further modeling. Accordingly, we will drop all other features from our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "characteristic-botswana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1303"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features = df_chi2_features.loc[df_chi2_features['p_value']< 0.05].sort_values(by = 'p_value', ascending = True)\n",
    "final_feature_columns = list(final_features['Feature_name'])\n",
    "len(final_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-detective",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Feature Extraction -Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-blanket",
   "metadata": {},
   "source": [
    "* We realised that our data consists of highly imbalanced classes. We tried to take that into consideration by splitting our data using startified option under train_test_split function.\n",
    "* This ensured that we maintained the equal amount of imbalance in our test as well as train split.\n",
    "* Next step was to prepare the text data for further feature extraction. We applied various transformations such as lemmatization amd acheiving homogenization by converting all words into lower case.\n",
    "* We encoded our target variables.\n",
    "* Finally, we used tfidfvectorizer to extract features from our text data. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "* As usual we fitted and transformed tfidfvectorizer over train data and used that fiited model to transform our test data.\n",
    "* Finally, we perfromed chi2 test to extract the best associated features with p-value less than 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-timing",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-thailand",
   "metadata": {},
   "source": [
    "In this step, we will,\n",
    "\n",
    "1. Drop columns which we rejected based on hypothesis testing.\n",
    "2. Use GridSearchCV to iterate through the models and parameters.\n",
    "3. Select the best model and subsequently the best values for hyperparameters.\n",
    "4. Also, we will try to reach best compromise over the precision and recall values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "conservative-spouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17910, 1303)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feature_matrix = feature_matrix[final_feature_columns].copy()\n",
    "final_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-independence",
   "metadata": {},
   "source": [
    "Before moving ahead its time also to transform test set as well as drop unnecessary features so that we can use it for obtaining predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adverse-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_features = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "endless-window",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>absence</th>\n",
       "      <th>access</th>\n",
       "      <th>access approach</th>\n",
       "      <th>access area</th>\n",
       "      <th>access arrangement</th>\n",
       "      <th>access available</th>\n",
       "      <th>access block</th>\n",
       "      <th>access egress</th>\n",
       "      <th>access filter</th>\n",
       "      <th>...</th>\n",
       "      <th>workplace screen</th>\n",
       "      <th>wtp</th>\n",
       "      <th>xi</th>\n",
       "      <th>xi im</th>\n",
       "      <th>yadav</th>\n",
       "      <th>yadav engage</th>\n",
       "      <th>yard</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone area</th>\n",
       "      <th>zone site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2582 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  absence  access  access approach  access area  access arrangement  \\\n",
       "0  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "1  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "2  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "3  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "4  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "\n",
       "   access available  access block  access egress  access filter  ...  \\\n",
       "0               0.0           0.0            0.0            0.0  ...   \n",
       "1               0.0           0.0            0.0            0.0  ...   \n",
       "2               0.0           0.0            0.0            0.0  ...   \n",
       "3               0.0           0.0            0.0            0.0  ...   \n",
       "4               0.0           0.0            0.0            0.0  ...   \n",
       "\n",
       "   workplace screen  wtp   xi  xi im  yadav  yadav engage  yard  zone  \\\n",
       "0               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "1               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "2               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "3               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "4               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "\n",
       "   zone area  zone site  \n",
       "0        0.0        0.0  \n",
       "1        0.0        0.0  \n",
       "2        0.0        0.0  \n",
       "3        0.0        0.0  \n",
       "4        0.0        0.0  \n",
       "\n",
       "[5 rows x 2582 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features_matrix = pd.DataFrame(data = test_set_features, columns = tfidf.get_feature_names())\n",
    "test_set_features_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "creative-disney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7677, 1303)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_final_features_matrix = test_set_features_matrix[final_feature_columns].copy()\n",
    "test_set_final_features_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "international-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us import all the required models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "radical-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-panic",
   "metadata": {},
   "source": [
    "### 4.a. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-south",
   "metadata": {},
   "source": [
    "First Model that I am going to try to train is a classical logistic Regression Model. I am using GridSearch to find out the best hyperparameters for my model. As explained earlier, I am using stratified kfold for maintaining class imbalance in the cross validated sets.\n",
    "\n",
    "Additionally, to optimize the model I am going to set class_weight parameter to 'balanced' to account for the imbalance in class and set warm_start to True to ensure we don't lose our previous coefficients. More importantly this will ensure that our model is efficient and training time is less.\n",
    "\n",
    "Another key decision which I make here is that, I am choosing weighted f1-score as the scoring metric for my GridSearchCV Object. As we know there is huge imbalance in dataset and trying to classify each minority class correctly may lead to overfitting. Additionally, my target is to get more relevant results rather than just more results, thus I would prefer precision over recall, which means I may get fewer results but I want to be right. \n",
    "\n",
    "In short, I don't want false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "after-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_pipeline = Pipeline([('standardscaler', StandardScaler()),\n",
    "#                              ('logisticregressor', LogisticRegression(solver = 'lbfgs',\n",
    "#                                                                       class_weight = 'balanced',\n",
    "#                                                                       random_state = 72,\n",
    "#                                                                       warm_start = True,\n",
    "#                                                                       n_jobs = -1, \n",
    "#                                                                       max_iter=500))\n",
    "#                             ])\n",
    "\n",
    "# params = {'logisticregressor__C': np.geomspace(0.001,1, 5)}\n",
    "\n",
    "# grid_logistic = GridSearchCV(logistic_pipeline, params, cv = kf, scoring = 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "filled-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_logistic.fit(final_feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accepted-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally lets save the model in pickled form for future reference\n",
    "\n",
    "def pickle_model(model, filename):\n",
    "    pkl.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "def de_pickle(model):\n",
    "    return pkl.load(open(model, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-america",
   "metadata": {},
   "source": [
    "Note: It is better to save the model using pickle method to preserve the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "conditional-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_model(grid_logistic, 'finallogisticmodel.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "phantom-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "finallogisticmodel = de_pickle('finallogisticmodel.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "superb-pension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregressor',\n",
       "                 LogisticRegression(C=0.03162277660168379,\n",
       "                                    class_weight='balanced', n_jobs=-1,\n",
       "                                    random_state=72, warm_start=True))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us check the final hyperparameters selected by the GridSearchCV method\n",
    "finallogisticmodel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "incredible-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = finallogisticmodel.predict(test_set_final_features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "surface-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us import the evaluation metrics for checking the model performance\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "amino-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us obtain the dataframe of the classification report for easy reference at a later stage\n",
    "\n",
    "logistic_regression = pd.DataFrame(classification_report(y_test, y_pred, \n",
    "                                                         output_dict = True, target_names = list(labelencoder.classes_)), ).T\n",
    "level0 = pd.MultiIndex.from_product([['logistic regression'], logistic_regression.columns])\n",
    "logistic_regression.columns = level0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "speaking-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">logistic regression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.829621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.784598</td>\n",
       "      <td>0.839341</td>\n",
       "      <td>0.798757</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.844730</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.834284</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             logistic regression                                 \n",
       "                       precision    recall  f1-score      support\n",
       "accuracy                0.829621  0.829621  0.829621     0.829621\n",
       "macro avg               0.784598  0.839341  0.798757  7677.000000\n",
       "weighted avg            0.844730  0.829621  0.834284  7677.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key metrics for this model are\n",
    "\n",
    "logistic_regression.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-respect",
   "metadata": {},
   "source": [
    "This suggests that the model is good at returning more relevant results (High Precision) as well as good at returning more correct results than wrong ones (High Recall). However, this is true only for weighted average. But we do see lower f1-macro scores that means certainly minority classess are worse off when it comes to True predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "opening-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">logistic regression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cable/Wire run across accessway</th>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Child/ Underage Labour at site</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Child/ Underage labour at Site</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ear Plugs not provided</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electrician engaged without Screening and Skill Test</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Excavated debris not removed</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard Barricades not provided</th>\n",
       "      <td>0.373457</td>\n",
       "      <td>0.454887</td>\n",
       "      <td>0.410169</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Improper/ No Backfilling</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No/ Improper Backfilling</th>\n",
       "      <td>0.554795</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No/ Improper ladder arrangement</th>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-insulated tools/ equipment</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nose Masks not worn</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unscreened Operator at Site</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water logging</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   logistic regression  \\\n",
       "                                                             precision   \n",
       "Cable/Wire run across accessway                               0.295775   \n",
       "Child/ Underage Labour at site                                0.555556   \n",
       "Child/ Underage labour at Site                                0.000000   \n",
       "Ear Plugs not provided                                        0.214286   \n",
       "Electrician engaged without Screening and Skill...            0.555556   \n",
       "Excavated debris not removed                                  0.571429   \n",
       "Hard Barricades not provided                                  0.373457   \n",
       "Improper/ No Backfilling                                      0.555556   \n",
       "No/ Improper Backfilling                                      0.554795   \n",
       "No/ Improper ladder arrangement                               0.568182   \n",
       "Non-insulated tools/ equipment                                0.545455   \n",
       "Nose Masks not worn                                           1.000000   \n",
       "Unscreened Operator at Site                                   0.363636   \n",
       "Water logging                                                 0.714286   \n",
       "\n",
       "                                                                                \n",
       "                                                      recall  f1-score support  \n",
       "Cable/Wire run across accessway                     0.512195  0.375000    41.0  \n",
       "Child/ Underage Labour at site                      0.714286  0.625000     7.0  \n",
       "Child/ Underage labour at Site                      0.000000  0.000000     4.0  \n",
       "Ear Plugs not provided                              1.000000  0.352941     3.0  \n",
       "Electrician engaged without Screening and Skill...  0.833333  0.666667    12.0  \n",
       "Excavated debris not removed                        0.590164  0.580645    61.0  \n",
       "Hard Barricades not provided                        0.454887  0.410169   266.0  \n",
       "Improper/ No Backfilling                            0.638298  0.594059    47.0  \n",
       "No/ Improper Backfilling                            0.778846  0.648000   104.0  \n",
       "No/ Improper ladder arrangement                     0.649351  0.606061    77.0  \n",
       "Non-insulated tools/ equipment                      0.545455  0.545455    22.0  \n",
       "Nose Masks not worn                                 0.333333  0.500000     3.0  \n",
       "Unscreened Operator at Site                         1.000000  0.533333     4.0  \n",
       "Water logging                                       0.625000  0.666667     8.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us check labels with lowest f1-score\n",
    "poor_predictions = logistic_regression.loc[logistic_regression[('logistic regression', 'f1-score')]<0.7]\n",
    "poor_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "technological-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 70 labels our LogisticRegressionmodel predicts 14 labels poorly (i.e. with f1-score less than 0.7).\n"
     ]
    }
   ],
   "source": [
    "print(f'Out of {len(labelencoder.classes_)} labels our {LogisticRegression().__class__.__name__}\\\n",
    "model predicts {poor_predictions.shape[0]} labels poorly (i.e. with f1-score less than 0.7).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-morrison",
   "metadata": {},
   "source": [
    "So, there are atleast 14 labels with f1 score lower than 0.7. We will use this also as one of the metric before final model selection. This may be a little higher number but I believe its still better way to identify which predicted labels I can trust more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-double",
   "metadata": {},
   "source": [
    "### 4.b. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-marks",
   "metadata": {},
   "source": [
    "As usual I am using GridSearchCV to determine the best hyperparameter. In this case instead of using SVC() with rbf kernel, I have opted to use Nystreom Kernel approximator to acheive efficiency during training. I have again chosen to keep class_weights to be balanced to account for imbalance in labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "palestinian-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearsvc_pipeline = Pipeline([('standardscaler', StandardScaler()),\n",
    "#                                 ('nystreom', Nystroem(random_state = 42,\n",
    "#                                                       n_jobs = -1)),\n",
    "#                                 ('svc', LinearSVC(class_weight = 'balanced',\n",
    "#                                                   random_state = 1000,\n",
    "#                                                   max_iter = 5000\n",
    "#                                                  ))])\n",
    "\n",
    "# params = {'nystreom__n_components': [50, 100, 200, 400],\n",
    "#           'nystreom__gamma': np.geomspace(0.001,0.1,5),\n",
    "#           'svc__C' : np.geomspace(0.001,1, 5)\n",
    "#          }\n",
    "\n",
    "# grid_svm = GridSearchCV(linearsvc_pipeline, params, cv = kf, scoring = 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ordered-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_svm.fit(final_feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "clinical-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_model(grid_svm, 'finalsvcmodel.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "third-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsvcmodel = de_pickle('finalsvcmodel.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "related-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('nystreom',\n",
       "                 Nystroem(gamma=0.001, n_components=400, n_jobs=-1,\n",
       "                          random_state=42)),\n",
       "                ('svc',\n",
       "                 LinearSVC(class_weight='balanced', max_iter=5000,\n",
       "                           random_state=1000))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalsvcmodel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "trained-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = finalsvcmodel.predict(test_set_final_features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "frozen-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let us obtain the dataframe of the classification report for easy reference at a later stage\n",
    "\n",
    "linear_svc_kernel = pd.DataFrame(classification_report(y_test, y_pred2,\n",
    "                                                       output_dict = True, target_names = list(labelencoder.classes_)), ).T\n",
    "level0 = pd.MultiIndex.from_product([['SVC Kernel'], linear_svc_kernel.columns])\n",
    "linear_svc_kernel.columns = level0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "phantom-capitol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">SVC Kernel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cable Insulation Issue/Bare Wires</th>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.544248</td>\n",
       "      <td>0.607407</td>\n",
       "      <td>226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cable/Wire run across accessway</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Child/ Underage Labour at site</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Child/ Underage labour at Site</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crane operator engaged without screening and skill test</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DG Set Issues</th>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domestic extension/ Electrical Board</th>\n",
       "      <td>0.622449</td>\n",
       "      <td>0.743902</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drawings not available or not complied</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ear Plugs not provided</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electrician engaged without Screening and Skill Test</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Excavated debris not removed</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Handgloves not worn</th>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard Barricades not provided</th>\n",
       "      <td>0.457447</td>\n",
       "      <td>0.161654</td>\n",
       "      <td>0.238889</td>\n",
       "      <td>266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height pass not obtained</th>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helper/Worker not wearing mandatory PPE</th>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Improper Hard Barricading</th>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.628743</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Improper/ No Backfilling</th>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inadequate Hard Barricading</th>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inadequate illumination arrangement</th>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incomplete Screening Form/ Form 11</th>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Materials found scattered</th>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.343558</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Caution Boards</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Hard Barricading</th>\n",
       "      <td>0.746627</td>\n",
       "      <td>0.627204</td>\n",
       "      <td>0.681725</td>\n",
       "      <td>794.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Inspection Done</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Parking/ Stop Lights</th>\n",
       "      <td>0.005495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010929</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No/ Improper Backfilling</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.534413</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No/ Improper Safety Nets</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No/ Improper ladder arrangement</th>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-insulated tools/ equipment</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nose Masks not worn</th>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poor/ Improper material stacking</th>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.663717</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RCCB/ DB/ Panel issues</th>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.551351</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rubber Hand Gloves not provided</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEC/ Excavation Permit not followed or available</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Safety Helmet not worn</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Safety Shoes not worn</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Settlement or Chances of Settlement at backfilled area</th>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sloping not provided as per requirement</th>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traffic Marshal not deployed</th>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uninsulated Crowbar</th>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unsafe Electrical connections/Practices</th>\n",
       "      <td>0.656805</td>\n",
       "      <td>0.415730</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unscreened Operator at Site</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unscreened Operator at site</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water logging</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Workmen found without Valid/ Proper ID Card</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.510299</td>\n",
       "      <td>0.576438</td>\n",
       "      <td>0.507677</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.733455</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.693850</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   SVC Kernel            \\\n",
       "                                                    precision    recall   \n",
       "Cable Insulation Issue/Bare Wires              ...   0.687151  0.544248   \n",
       "Cable/Wire run across accessway                      0.200000  0.317073   \n",
       "Child/ Underage Labour at site                       0.000000  0.000000   \n",
       "Child/ Underage labour at Site                       0.000000  0.000000   \n",
       "Crane operator engaged without screening and sk...   0.636364  0.259259   \n",
       "DG Set Issues                                        0.541176  0.686567   \n",
       "Domestic extension/ Electrical Board                 0.622449  0.743902   \n",
       "Drawings not available or not complied               0.083333  0.250000   \n",
       "Ear Plugs not provided                               0.200000  0.333333   \n",
       "Electrician engaged without Screening and Skill...   0.125000  0.166667   \n",
       "Excavated debris not removed                         0.434783  0.327869   \n",
       "Handgloves not worn                                  0.150000  0.300000   \n",
       "Hard Barricades not provided                         0.457447  0.161654   \n",
       "Height pass not obtained                             0.312500  0.250000   \n",
       "Helper/Worker not wearing mandatory PPE              0.418182  0.851852   \n",
       "Improper Hard Barricading                            0.681818  0.583333   \n",
       "Improper/ No Backfilling                             0.302326  0.276596   \n",
       "Inadequate Hard Barricading                          0.552083  0.602273   \n",
       "Inadequate illumination arrangement                  0.103448  0.750000   \n",
       "Incomplete Screening Form/ Form 11                   0.190476  0.307692   \n",
       "Materials found scattered                            0.658824  0.343558   \n",
       "No Caution Boards                                    0.440000  0.733333   \n",
       "No Hard Barricading                                  0.746627  0.627204   \n",
       "No Inspection Done                                   0.666667  0.162162   \n",
       "No Parking/ Stop Lights                              0.005495  1.000000   \n",
       "No/ Improper Backfilling                             0.461538  0.634615   \n",
       "No/ Improper Safety Nets                             0.206897  0.375000   \n",
       "No/ Improper ladder arrangement                      0.540984  0.428571   \n",
       "Non-insulated tools/ equipment                       0.235294  0.181818   \n",
       "Nose Masks not worn                                  0.009524  0.333333   \n",
       "Poor/ Improper material stacking                     0.590551  0.757576   \n",
       "RCCB/ DB/ Panel issues                               0.733813  0.551351   \n",
       "Rubber Hand Gloves not provided                      0.142857  0.266667   \n",
       "SEC/ Excavation Permit not followed or available     0.388889  0.482759   \n",
       "Safety Helmet not worn                               0.294118  0.555556   \n",
       "Safety Shoes not worn                                0.133333  0.250000   \n",
       "Settlement or Chances of Settlement at backfill...   0.368421  0.274510   \n",
       "Sloping not provided as per requirement              0.590909  0.406250   \n",
       "Traffic Marshal not deployed                         0.121951  0.500000   \n",
       "Uninsulated Crowbar                                  0.476190  0.645161   \n",
       "Unsafe Electrical connections/Practices        ...   0.656805  0.415730   \n",
       "Unscreened Operator at Site                          0.133333  1.000000   \n",
       "Unscreened Operator at site                          0.125000  0.250000   \n",
       "Water logging                                        0.205882  0.875000   \n",
       "Workmen found without Valid/ Proper ID Card          0.235294  0.320000   \n",
       "accuracy                                             0.674612  0.674612   \n",
       "macro avg                                            0.510299  0.576438   \n",
       "weighted avg                                         0.733455  0.674612   \n",
       "\n",
       "                                                                           \n",
       "                                                    f1-score      support  \n",
       "Cable Insulation Issue/Bare Wires              ...  0.607407   226.000000  \n",
       "Cable/Wire run across accessway                     0.245283    41.000000  \n",
       "Child/ Underage Labour at site                      0.000000     7.000000  \n",
       "Child/ Underage labour at Site                      0.000000     4.000000  \n",
       "Crane operator engaged without screening and sk...  0.368421    27.000000  \n",
       "DG Set Issues                                       0.605263    67.000000  \n",
       "Domestic extension/ Electrical Board                0.677778    82.000000  \n",
       "Drawings not available or not complied              0.125000     4.000000  \n",
       "Ear Plugs not provided                              0.250000     3.000000  \n",
       "Electrician engaged without Screening and Skill...  0.142857    12.000000  \n",
       "Excavated debris not removed                        0.373832    61.000000  \n",
       "Handgloves not worn                                 0.200000    10.000000  \n",
       "Hard Barricades not provided                        0.238889   266.000000  \n",
       "Height pass not obtained                            0.277778    20.000000  \n",
       "Helper/Worker not wearing mandatory PPE             0.560976    27.000000  \n",
       "Improper Hard Barricading                           0.628743   180.000000  \n",
       "Improper/ No Backfilling                            0.288889    47.000000  \n",
       "Inadequate Hard Barricading                         0.576087    88.000000  \n",
       "Inadequate illumination arrangement                 0.181818     4.000000  \n",
       "Incomplete Screening Form/ Form 11                  0.235294    13.000000  \n",
       "Materials found scattered                           0.451613   163.000000  \n",
       "No Caution Boards                                   0.550000    15.000000  \n",
       "No Hard Barricading                                 0.681725   794.000000  \n",
       "No Inspection Done                                  0.260870    37.000000  \n",
       "No Parking/ Stop Lights                             0.010929     2.000000  \n",
       "No/ Improper Backfilling                            0.534413   104.000000  \n",
       "No/ Improper Safety Nets                            0.266667    16.000000  \n",
       "No/ Improper ladder arrangement                     0.478261    77.000000  \n",
       "Non-insulated tools/ equipment                      0.205128    22.000000  \n",
       "Nose Masks not worn                                 0.018519     3.000000  \n",
       "Poor/ Improper material stacking                    0.663717    99.000000  \n",
       "RCCB/ DB/ Panel issues                              0.629630   185.000000  \n",
       "Rubber Hand Gloves not provided                     0.186047    15.000000  \n",
       "SEC/ Excavation Permit not followed or available    0.430769    58.000000  \n",
       "Safety Helmet not worn                              0.384615     9.000000  \n",
       "Safety Shoes not worn                               0.173913     8.000000  \n",
       "Settlement or Chances of Settlement at backfill...  0.314607    51.000000  \n",
       "Sloping not provided as per requirement             0.481481    64.000000  \n",
       "Traffic Marshal not deployed                        0.196078    10.000000  \n",
       "Uninsulated Crowbar                                 0.547945    31.000000  \n",
       "Unsafe Electrical connections/Practices        ...  0.509174   267.000000  \n",
       "Unscreened Operator at Site                         0.235294     4.000000  \n",
       "Unscreened Operator at site                         0.166667     4.000000  \n",
       "Water logging                                       0.333333     8.000000  \n",
       "Workmen found without Valid/ Proper ID Card         0.271186    25.000000  \n",
       "accuracy                                            0.674612     0.674612  \n",
       "macro avg                                           0.507677  7677.000000  \n",
       "weighted avg                                        0.693850  7677.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Let us check labels with lowest f1-score\n",
    "\n",
    "poor_predictions = linear_svc_kernel.loc[linear_svc_kernel[('SVC Kernel', 'f1-score')]<0.7]\n",
    "poor_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "through-switzerland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 70 labels our LinearSVCmodel predicts 48 labels poorly (i.e. with f1-score less than 0.7).\n"
     ]
    }
   ],
   "source": [
    "print(f'Out of {len(labelencoder.classes_)} labels our {LinearSVC().__class__.__name__}\\\n",
    "model predicts {poor_predictions.shape[0]} labels poorly (i.e. with f1-score less than 0.7).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "basic-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">SVC Kernel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.510299</td>\n",
       "      <td>0.576438</td>\n",
       "      <td>0.507677</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.733455</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.693850</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SVC Kernel                                 \n",
       "              precision    recall  f1-score      support\n",
       "accuracy       0.674612  0.674612  0.674612     0.674612\n",
       "macro avg      0.510299  0.576438  0.507677  7677.000000\n",
       "weighted avg   0.733455  0.674612  0.693850  7677.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key metrics for this model are\n",
    "\n",
    "linear_svc_kernel.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-stereo",
   "metadata": {},
   "source": [
    "For SVC we see that surprisingly, the weighted f1 score has worsened off to 0.69 and the macro f1 score is even worst (0.5). Which means this model doesn't give any good estimate for minority classes, also too many of them have poor f1 score (48 labels are poorly classified)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-gauge",
   "metadata": {},
   "source": [
    "### 4.c. Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-minutes",
   "metadata": {},
   "source": [
    "This is arguably the most popular among the methods that I have used in this assignment. \n",
    "\n",
    "As a first step of modelling, I am using a simple decision tree classifier to determine the maximum depth of the tree which I will use to tune my hyperparameter later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "anticipated-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "super-airline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum depth reached for standalone Decision tree is 203\n"
     ]
    }
   ],
   "source": [
    "#For getting initiatizing values of max depth of the random forrest, I will simply run a decision tree\n",
    "\n",
    "raw_decision_tree = DecisionTreeClassifier(random_state = 43)\n",
    "raw_decision_tree.fit(final_feature_matrix, y_train)\n",
    "print(f'The maximum depth reached for standalone Decision tree is {raw_decision_tree.tree_.max_depth}')\n",
    "max_depth_tree = raw_decision_tree.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-carry",
   "metadata": {},
   "source": [
    "With above information lets start modelling our Random Forrest Classifier. I have chosen estimators in the range from 50 to 200. Additionally, I have set maximum features to 'log2' instead of square root to reduce the features used for decision making. Another important information here is that, you should not keep class_weight 'balanced' if you want to warm_start your random forrest as it will raise warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "persistent-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forrest_pipeline = Pipeline([('standardscaler', StandardScaler()),\n",
    "                                    ('randomforrest', RandomForestClassifier(random_state = 42,\n",
    "                                                                             warm_start = True,\n",
    "                                                                             max_features = 'log2',\n",
    "                                                                             oob_score = True,\n",
    "                                                                             n_jobs = -1))\n",
    "                                   ])\n",
    "\n",
    "params = {'randomforrest__n_estimators': [50, 100, 200, 400],\n",
    "          'randomforrest__max_depth': np.geomspace(1, max_depth_tree+1, 5).astype(int)\n",
    "         }\n",
    "\n",
    "grid_randomforrest = GridSearchCV(random_forrest_pipeline, params, cv = kf, scoring = 'f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "swedish-housing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=42000, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('randomforrest',\n",
       "                                        RandomForestClassifier(max_features='log2',\n",
       "                                                               n_jobs=-1,\n",
       "                                                               oob_score=True,\n",
       "                                                               random_state=42,\n",
       "                                                               warm_start=True))]),\n",
       "             param_grid={'randomforrest__max_depth': array([  1,   3,  14,  53, 204]),\n",
       "                         'randomforrest__n_estimators': [50, 100, 200, 400]},\n",
       "             scoring='f1_weighted')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_randomforrest.fit(final_feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "rolled-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_model(grid_randomforrest, 'finalrfcmodel.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "surprised-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalrfcmodel = de_pickle('finalrfcmodel.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "boring-focus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforrest',\n",
       "                 RandomForestClassifier(max_depth=204, max_features='log2',\n",
       "                                        n_estimators=200, n_jobs=-1,\n",
       "                                        oob_score=True, random_state=42,\n",
       "                                        warm_start=True))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalrfcmodel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "turkish-aruba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.849357900614182"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets us check the best OOB Score acheived by our model.\n",
    "\n",
    "finalrfcmodel.best_estimator_.named_steps['randomforrest'].oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "processed-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = finalrfcmodel.predict(test_set_final_features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "mental-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aa-minhaj\\sklearn-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\aa-minhaj\\sklearn-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\aa-minhaj\\sklearn-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# # let us obtain the dataframe of the classification report for easy reference at a later stage\n",
    "\n",
    "random_forrest = pd.DataFrame(classification_report(y_test, y_pred3,\n",
    "                                                    output_dict = True, target_names = list(labelencoder.classes_)), ).T\n",
    "level0 = pd.MultiIndex.from_product([['Random Forrest'], random_forrest.columns])\n",
    "random_forrest.columns = level0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "pacific-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Random Forrest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cable/Wire run across accessway</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Child/ Underage Labour at site</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Child/ Underage labour at Site</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drawings not available or not complied</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ear Plugs not provided</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electrician engaged without Screening and Skill Test</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Excavated debris not removed</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.618557</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Face Shield not available</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard Barricades not provided</th>\n",
       "      <td>0.602837</td>\n",
       "      <td>0.319549</td>\n",
       "      <td>0.417690</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height pass not obtained</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helper/Worker not wearing mandatory PPE</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Improper/ No Backfilling</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inadequate illumination arrangement</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No/ Improper ladder arrangement</th>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-insulated tools/ equipment</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unscreened Operator at Site</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unscreened Operator at site</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water logging</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Random Forrest            \\\n",
       "                                                        precision    recall   \n",
       "Cable/Wire run across accessway                          0.571429  0.292683   \n",
       "Child/ Underage Labour at site                           1.000000  0.142857   \n",
       "Child/ Underage labour at Site                           0.000000  0.000000   \n",
       "Drawings not available or not complied                   0.000000  0.000000   \n",
       "Ear Plugs not provided                                   1.000000  0.333333   \n",
       "Electrician engaged without Screening and Skill...       0.800000  0.333333   \n",
       "Excavated debris not removed                             0.833333  0.491803   \n",
       "Face Shield not available                                1.000000  0.500000   \n",
       "Hard Barricades not provided                             0.602837  0.319549   \n",
       "Height pass not obtained                                 1.000000  0.500000   \n",
       "Helper/Worker not wearing mandatory PPE                  0.882353  0.555556   \n",
       "Improper/ No Backfilling                                 0.717949  0.595745   \n",
       "Inadequate illumination arrangement                      0.000000  0.000000   \n",
       "No/ Improper ladder arrangement                          0.746032  0.610390   \n",
       "Non-insulated tools/ equipment                           0.818182  0.409091   \n",
       "Unscreened Operator at Site                              0.500000  0.750000   \n",
       "Unscreened Operator at site                              1.000000  0.500000   \n",
       "Water logging                                            1.000000  0.375000   \n",
       "\n",
       "                                                                      \n",
       "                                                    f1-score support  \n",
       "Cable/Wire run across accessway                     0.387097    41.0  \n",
       "Child/ Underage Labour at site                      0.250000     7.0  \n",
       "Child/ Underage labour at Site                      0.000000     4.0  \n",
       "Drawings not available or not complied              0.000000     4.0  \n",
       "Ear Plugs not provided                              0.500000     3.0  \n",
       "Electrician engaged without Screening and Skill...  0.470588    12.0  \n",
       "Excavated debris not removed                        0.618557    61.0  \n",
       "Face Shield not available                           0.666667     2.0  \n",
       "Hard Barricades not provided                        0.417690   266.0  \n",
       "Height pass not obtained                            0.666667    20.0  \n",
       "Helper/Worker not wearing mandatory PPE             0.681818    27.0  \n",
       "Improper/ No Backfilling                            0.651163    47.0  \n",
       "Inadequate illumination arrangement                 0.000000     4.0  \n",
       "No/ Improper ladder arrangement                     0.671429    77.0  \n",
       "Non-insulated tools/ equipment                      0.545455    22.0  \n",
       "Unscreened Operator at Site                         0.600000     4.0  \n",
       "Unscreened Operator at site                         0.666667     4.0  \n",
       "Water logging                                       0.545455     8.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Let us check labels with lowest f1-score\n",
    "\n",
    "poor_predictions = random_forrest.loc[random_forrest[('Random Forrest', 'f1-score')]<0.7]\n",
    "poor_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "periodic-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 70 labels our RandomForestClassifiermodel predicts 18 labels poorly (i.e. with f1-score less than 0.7).\n"
     ]
    }
   ],
   "source": [
    "print(f'Out of {len(labelencoder.classes_)} labels our {RandomForestClassifier().__class__.__name__}\\\n",
    "model predicts {poor_predictions.shape[0]} labels poorly (i.e. with f1-score less than 0.7).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "atomic-breeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Random Forrest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.856454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.847672</td>\n",
       "      <td>0.724757</td>\n",
       "      <td>0.764129</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.854782</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Random Forrest                                 \n",
       "                  precision    recall  f1-score      support\n",
       "accuracy           0.856454  0.856454  0.856454     0.856454\n",
       "macro avg          0.847672  0.724757  0.764129  7677.000000\n",
       "weighted avg       0.854782  0.856454  0.849200  7677.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key metrics for this model are\n",
    "\n",
    "random_forrest.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-literacy",
   "metadata": {},
   "source": [
    "Here we can see that we have substaintially improved our weighted precision and recall scores but at the cost of poorly classifying around 18 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "tribal-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us place all our model parameters next to each other to compare them.\n",
    "\n",
    "comparision_table = pd.concat([logistic_regression.tail(3), linear_svc_kernel.tail(3), random_forrest.tail(3)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "lovely-cooler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">logistic regression</th>\n",
       "      <th colspan=\"4\" halign=\"left\">SVC Kernel</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Random Forrest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.856454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.784598</td>\n",
       "      <td>0.839341</td>\n",
       "      <td>0.798757</td>\n",
       "      <td>7677.000000</td>\n",
       "      <td>0.510299</td>\n",
       "      <td>0.576438</td>\n",
       "      <td>0.507677</td>\n",
       "      <td>7677.000000</td>\n",
       "      <td>0.847672</td>\n",
       "      <td>0.724757</td>\n",
       "      <td>0.764129</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.844730</td>\n",
       "      <td>0.829621</td>\n",
       "      <td>0.834284</td>\n",
       "      <td>7677.000000</td>\n",
       "      <td>0.733455</td>\n",
       "      <td>0.674612</td>\n",
       "      <td>0.693850</td>\n",
       "      <td>7677.000000</td>\n",
       "      <td>0.854782</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>7677.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             logistic regression                                  SVC Kernel  \\\n",
       "                       precision    recall  f1-score      support  precision   \n",
       "accuracy                0.829621  0.829621  0.829621     0.829621   0.674612   \n",
       "macro avg               0.784598  0.839341  0.798757  7677.000000   0.510299   \n",
       "weighted avg            0.844730  0.829621  0.834284  7677.000000   0.733455   \n",
       "\n",
       "                                              Random Forrest            \\\n",
       "                recall  f1-score      support      precision    recall   \n",
       "accuracy      0.674612  0.674612     0.674612       0.856454  0.856454   \n",
       "macro avg     0.576438  0.507677  7677.000000       0.847672  0.724757   \n",
       "weighted avg  0.674612  0.693850  7677.000000       0.854782  0.856454   \n",
       "\n",
       "                                     \n",
       "              f1-score      support  \n",
       "accuracy      0.856454     0.856454  \n",
       "macro avg     0.764129  7677.000000  \n",
       "weighted avg  0.849200  7677.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparision_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-change",
   "metadata": {},
   "source": [
    "Here we can see that Random Forrest provides superior results when it comes to weighted scores. Also Random forrest has better Precision than recall (macro average), which means we get way less false positives. However, when weighted against the support for each label, it easily outperforms logistic Regression model. This meets both of our requirement that we had set forth at the beginning of modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-upgrade",
   "metadata": {},
   "source": [
    "However, by choosing Random Forrest classifier we risk losing some meaningful predictions for minority classes (Random forrest poorly classifies 18 labels against only 14 labels poorly classified by logistic regression.\n",
    "\n",
    "But, in our use case, the cost of missing such True positives is not high, at the same time we need better classification performance for our majority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-climate",
   "metadata": {},
   "source": [
    "## Model Evaluation and Selection - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-albuquerque",
   "metadata": {},
   "source": [
    "* We tried to train 3 models and chose hyperparameters using GridSearchCV Method\n",
    ">1. Logistic Regression\n",
    ">2. Support Vector Classification with kernel approximation\n",
    ">3. Random Forrest Classification\n",
    "* We set forth our expectations clearly,\n",
    ">1. High Precision.\n",
    ">2. Higher weighted average f1-score.\n",
    "\n",
    ">Both of which were met by Random Forrest Classifier. We end up selecting a model which had weighted precison, recall and f1  score of 0.854, 0.856 and 0.849 respectively.\n",
    "\n",
    "* As explained earlier, there is huge imbalance in dataset and trying to classify each minority class correctly may lead to overfitting. Thus it would be better to avoid false alarms by opting for better precision. Additionally, correct prediction of majority class was must; let me explain in details\n",
    ">Almost all safety systems are based on basic philosphy of identifying the root causes and correcting them. This means that if you have truly identified a root cause and treated it appropiately, there would be no repition of similar observation. This is of course an ideal situation. However, in real life these incidents may stll recur and it may not be possible to completely eliminate them due to various factors. Thus the next best measure which is adopted by most organisations is counting repetition of root causes. Higher the count more serious the issue and thus more immediate action required. This frequency is then multiplied with probable consequence to get the actual risk of incident. However, this model is concerned with only getting the frequency right.\n",
    "<P>Thus, it make sense to identify majority classes with higher precision and recall, hence the chose of measure - Weighted F1 Score for selecting best estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-logic",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-brown",
   "metadata": {},
   "source": [
    "In conclusion, we chose Random Forrest Model for purpose of deployment for future predictions. The model not only shows great prediction power, it gives lower false positives at the same time has higher weighted f1-score, precision and recall.\n",
    "\n",
    "Going further, in future we may improve our modelling performance by,\n",
    "\n",
    "* Oversampling or Undersampling, or combining both to check whether we can acheive better classification.\n",
    "* At the time of feature extraction we only used unigrams and bigrams, we can also use further higher degree of feature such as trigram (combination of three words)\n",
    "* While training logistic regression model I got lot of failure to convergence warning, we may try to increase the number of iterations to acheive better results.\n",
    "* In case of SVM, we use kernel approximation to reduce training time and however, in future if we have enough computing resources we can ditch the approximation go ahead with full scale kernel sampling.\n",
    "* Again in Random Forrest Classifier we can again check for intermediate max_depth parameters to see if the results could be acheived using a less complex (less depth) model.\n",
    "* Another useful thing to do would be to check prediction probabilities and probably get soem insights out of it.\n",
    "* Finally we can actaully re-work on our input data to maybe eliminate some of the labels by combining with other similar labels, thus reducing some granularity in data when not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-operator",
   "metadata": {},
   "source": [
    "## Thank You, for visiting my page and going through this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-encoding",
   "metadata": {},
   "source": [
    "# CIAO !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
