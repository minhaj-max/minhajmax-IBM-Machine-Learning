{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "immediate-marine",
   "metadata": {},
   "source": [
    "#  Predicting Root Causes of Safety Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-ballot",
   "metadata": {},
   "source": [
    "## Predicting Root Causes of Safety Observations using Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-train",
   "metadata": {},
   "source": [
    "## 1. Importing Data\n",
    "\n",
    "The data which I am using here is a dump from the Safety Observation System of a Major Company. The each row of data consists of the observation description along with pre-selected category and root cause (Classes). These labelings have been done manually. Thus, we aleady have labels in our dataset.\n",
    "\n",
    "The objective of this program would be to develop a Deep Learning Model which would be able to predict the EHS Observations based on the historical data. We will try to fit different Deep Learning Models, tuning them for optimal number of input parameters, to arrive at the best model.\n",
    "\n",
    "By implication this exercise is an Natural Language Processing Exercise, as we will be extracting features from text (Observation Description) and using those for training our clustering model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-scheme",
   "metadata": {},
   "source": [
    "The overall plan includes\n",
    "\n",
    "1. Cleaning the data by removing stops and performing EDA.\n",
    "2. Feature extraction to arrive on most important features.\n",
    "3. Feature selection and dimension reduction using Non-Negative Matrix Factorization.\n",
    "3. Training our different models and measure the performance using pre-identified class labels\n",
    "4. Evaluation of Models and selection of best model.\n",
    "5. Summary and Future action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aa-minhaj\\sklearn-venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3155: DtypeWarning: Columns (0,1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UA/UC</th>\n",
       "      <th>Observation</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>worker used drilling machine without green tag.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>Worker used a plug top for multiple  electrica...</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>worker use drilling machine at site without gr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>Site supervisor is not available at site whi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electrically Unsafe (Safety Hazard)</td>\n",
       "      <td>worker use electric grinder without green tag.</td>\n",
       "      <td>No Inspection Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 UA/UC  \\\n",
       "0  Electrically Unsafe (Safety Hazard)   \n",
       "1  Electrically Unsafe (Safety Hazard)   \n",
       "2  Electrically Unsafe (Safety Hazard)   \n",
       "3  Electrically Unsafe (Safety Hazard)   \n",
       "4  Electrically Unsafe (Safety Hazard)   \n",
       "\n",
       "                                         Observation  \\\n",
       "0    worker used drilling machine without green tag.   \n",
       "1  Worker used a plug top for multiple  electrica...   \n",
       "2  worker use drilling machine at site without gr...   \n",
       "3    Site supervisor is not available at site whi...   \n",
       "4     worker use electric grinder without green tag.   \n",
       "\n",
       "                                Sub-Category  \n",
       "0                                        NaN  \n",
       "1   No Plug top                               \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                         No Inspection Done  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us import the data\n",
    "df_obs = pd.read_csv('ehs_obs_data.csv')\n",
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bizarre-particle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the dataset is 3 and the no of rows is 1028493\n"
     ]
    }
   ],
   "source": [
    "#Let us check the no of columns and rows\n",
    "\n",
    "print(f'The number of columns in the dataset is {df_obs.shape[1]} and the no of rows is {df_obs.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-inspiration",
   "metadata": {},
   "source": [
    "The target variable here will be 'Sub-Category' and the input variable will be 'Observation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-answer",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-diagnosis",
   "metadata": {},
   "source": [
    "In this step we will try to \n",
    "\n",
    "1. Bring all words on same case (lower case).\n",
    "2. Detect the missing values and treat them accordingly.\n",
    "3. Drop various Stop words which don't add any information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "peripheral-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us first bring in all the required modules\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first drop the 'UA/UC' Column which is not required for this exercise.\n",
    "\n",
    "df_obs.drop('UA/UC', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "diverse-paraguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worker used drilling machine without green tag.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worker used a plug top for multiple  electrica...</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worker use drilling machine at site without gr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Site supervisor is not available at site whi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worker use electric grinder without green tag.</td>\n",
       "      <td>No Inspection Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Observation  \\\n",
       "0    worker used drilling machine without green tag.   \n",
       "1  Worker used a plug top for multiple  electrica...   \n",
       "2  worker use drilling machine at site without gr...   \n",
       "3    Site supervisor is not available at site whi...   \n",
       "4     worker use electric grinder without green tag.   \n",
       "\n",
       "                                Sub-Category  \n",
       "0                                        NaN  \n",
       "1   No Plug top                               \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                         No Inspection Done  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "strong-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Observation     961881\n",
       "Sub-Category    996100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the missing values\n",
    "\n",
    "df_obs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-contractor",
   "metadata": {},
   "source": [
    "Looks like we have lots of blank rows. However, this may be due to faulty data extraction process and we can easily drop these without losing any meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping values\n",
    "df_obs.dropna(subset = ['Observation'], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "higher-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66612,\n",
       " Observation         0\n",
       " Sub-Category    34219\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check how many values are missing in target variable after dropping missing feature rows\n",
    "df_obs.shape[0], df_obs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-freeware",
   "metadata": {},
   "source": [
    "We can see that we are missing labels for almost half of the data. Now this leaves a lot of room for improvement in data collection and labelling process. Unfortunately, we have to leave out all these rows from our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brave-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32393,\n",
       " Observation     0\n",
       " Sub-Category    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#droping values\n",
    "df_obs.dropna(subset = ['Sub-Category'], axis = 0, inplace = True)\n",
    "df_obs.shape[0], df_obs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-registration",
   "metadata": {},
   "source": [
    "So, in the end we are now left with 32,393 Nos of individual observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subjective-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check for the duplicates in the observation column\n",
    "\n",
    "df_obs.duplicated(subset = ['Observation'], keep = 'first').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-radio",
   "metadata": {},
   "source": [
    "Looks like we have quite a few number of duplicated observation data. It would be better to drop these duplicate columns to avoid any bias in our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increasing-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs.drop_duplicates(subset = 'Observation', keep = 'first', ignore_index = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reflected-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the dataset is 2 and the no of rows is 25587\n"
     ]
    }
   ],
   "source": [
    "# final size of the dataframe\n",
    "print(f'The number of columns in the dataset is {df_obs.shape[1]} and the no of rows is {df_obs.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-pixel",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling - Summary\n",
    "\n",
    "* We dropped the null values and duplicated observations to reduce the dimensions of the data\n",
    "* This also ensures that we avoid assigining higher weightage to certain duplicated observations at the time of feature extraction.\n",
    "* We also dropped UA/UC columns as we want to explore this dataset purely from NLP perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-semiconductor",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-gilbert",
   "metadata": {},
   "source": [
    "In this step, we will\n",
    "\n",
    "1. Remove punctuation marks and anyother extra space.\n",
    "2. Lemmatize all the words so that different words stemming from same root word are not counted as separate features.\n",
    "3. Extract features from our observation text using tfidf method.\n",
    "4. Perform Dimensionality Reduction using NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-index",
   "metadata": {},
   "source": [
    "First we will remove the punctuation marks and extra spaces in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "above-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = map(str, list(range(0, 100)))\n",
    "for num in numbers:\n",
    "    df_obs.Observation =  df_obs.Observation.str.replace(num, ' ', regex = False)\n",
    "special_charecters = ['?', '#', '-', \"'\", '&', '/', '.', ',', '(', ')', ':',';']\n",
    "for char in special_charecters:\n",
    "     df_obs.Observation =  df_obs.Observation.str.replace(char, ' ', regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retired-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs.Observation = df_obs.Observation.apply(lambda x: \" \".join(x.split()))\n",
    "df_obs.Observation = df_obs.Observation.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-forestry",
   "metadata": {},
   "source": [
    "Lets lemmatize our observation text for better results. \n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-butler",
   "metadata": {},
   "source": [
    "We will use WordNetLemmatizer from NTLK library for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "framed-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlemmatize = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dried-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(word_list):\n",
    "    lem_word_list = [wordlemmatize.lemmatize(word, pos = 'v') for word in word_list.split()]\n",
    "    new_lem_word_list = [wordlemmatize.lemmatize(word) for word in lem_word_list]\n",
    "    return \" \".join(new_lem_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "regular-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs.Observation = df_obs.Observation.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "racial-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>weld machine be have metallic body and it be n...</td>\n",
       "      <td>No/ Improper Earthing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12203</th>\n",
       "      <td>CC Debris and excavate material have be keep e...</td>\n",
       "      <td>Excavated soil kept on edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>Bare wire find insert in D B May result in ele...</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>Portable angle grinder machine be observe with...</td>\n",
       "      <td>Domestic extension/ Electrical Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23433</th>\n",
       "      <td>Rebar cap have not be provide for the reinforc...</td>\n",
       "      <td>Unprotected rebar edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>MS Pipes lay alignment Ramasamdhram weld pit l...</td>\n",
       "      <td>No/ Improper ladder arrangement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22565</th>\n",
       "      <td>Housekeeping be not complete in CCT room at STP</td>\n",
       "      <td>Poor Housekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10635</th>\n",
       "      <td>soil place on the edge</td>\n",
       "      <td>Excavated soil kept on edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9021</th>\n",
       "      <td>Visitor find inside plant area without have vi...</td>\n",
       "      <td>Workmen engaged without screening/ Induction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>Loose soil keep at the edge of excavate pit</td>\n",
       "      <td>Excavated soil kept on edge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Observation  \\\n",
       "2790   weld machine be have metallic body and it be n...   \n",
       "12203  CC Debris and excavate material have be keep e...   \n",
       "3893   Bare wire find insert in D B May result in ele...   \n",
       "3951   Portable angle grinder machine be observe with...   \n",
       "23433  Rebar cap have not be provide for the reinforc...   \n",
       "8736   MS Pipes lay alignment Ramasamdhram weld pit l...   \n",
       "22565    Housekeeping be not complete in CCT room at STP   \n",
       "10635                             soil place on the edge   \n",
       "9021   Visitor find inside plant area without have vi...   \n",
       "9461         Loose soil keep at the edge of excavate pit   \n",
       "\n",
       "                                       Sub-Category  \n",
       "2790                          No/ Improper Earthing  \n",
       "12203                   Excavated soil kept on edge  \n",
       "3893       No Plug top                               \n",
       "3951           Domestic extension/ Electrical Board  \n",
       "23433                        Unprotected rebar edge  \n",
       "8736                No/ Improper ladder arrangement  \n",
       "22565                             Poor Housekeeping  \n",
       "10635                   Excavated soil kept on edge  \n",
       "9021   Workmen engaged without screening/ Induction  \n",
       "9461                    Excavated soil kept on edge  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check our cleaned dataframe\n",
    "\n",
    "df_obs.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-electricity",
   "metadata": {},
   "source": [
    "If you watch closely lemmatization has taken affect on many words for ex- against index 7778 the word 'been' has been stemmed to 'be', the word 'kept' has been changed back to 'keep' at 7656. In reality, these words would have been extracted as separate features thereby increasing the overall feature dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-colonial",
   "metadata": {},
   "source": [
    "Now lets encode out target variables to prepare them for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sufficient-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df_obs['Sub-Category'] = labelencoder.fit_transform(df_obs['Sub-Category'])\n",
    "plt.hist(df_obs['Sub-Category'].value_counts(normalize = True), bins = 25,);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-radical",
   "metadata": {},
   "source": [
    "This shows that our classes have quite severe imbalance. And this can impact our final model's performance. Instead of opting for any sampling approach lets try to model our data using imbalanced classes by maintaining class imbalance in the train and test split. We will use class_weight feature wherever available to penalize the imbalance. Also lets save all our label names and codes for future references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "accurate-exclusive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Code</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No Plug top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Backhoe Operator engaged without screening and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cable Insulation Issue/Bare Wires             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cable issues(Routing, Sub-standard etc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cable/Wire run across accessway</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label Code                                              Label\n",
       "0           0           No Plug top                             \n",
       "1           1  Backhoe Operator engaged without screening and...\n",
       "2           2  Cable Insulation Issue/Bare Wires             ...\n",
       "3           3            Cable issues(Routing, Sub-standard etc)\n",
       "4           4                    Cable/Wire run across accessway"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_codes = pd.DataFrame(labelencoder.classes_).reset_index()\n",
    "label_codes.columns = ['Label Code', 'Label']\n",
    "label_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-drama",
   "metadata": {},
   "source": [
    "Next and probably most important step for modelling is to extract features from text data. Now there are many methods to acheive this objective but the most important from all is Term Frequency and Inverse Document Frequnecy Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-bottle",
   "metadata": {},
   "source": [
    "In information retrieval, tf–idf, TF*IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-compatibility",
   "metadata": {},
   "source": [
    "Another important term that you will come across would be N-grams. In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
    "\n",
    "Using Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\".\n",
    "\n",
    "For example a feature - 'Safety' is unigram whereas a feature 'Safety Helmet' would be bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "configured-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets instantiate our tfidfvectorizer by setting feature output for both unigrams and bigrams and Stop words from English\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,2),\n",
    "                        stop_words = 'english',\n",
    "                        min_df = 10,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-heading",
   "metadata": {},
   "source": [
    "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Also, I have set min_df = 10, which means words that occurred in too few documents, in this case less than 10, should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-freeze",
   "metadata": {},
   "source": [
    "I would higly recommend the reader to go through sklearn's working with text data resources. https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-issue",
   "metadata": {},
   "source": [
    "At this point it would be prudent to first split our data into training and test sets and then applying fit_tranform only on the training set using the fit to tranform our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "referenced-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_obs.Observation, df_obs['Sub-Category'],\n",
    "                                                    test_size = 0.3, random_state = 1000,\n",
    "                                                    stratify = df_obs['Sub-Category']\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-damages",
   "metadata": {},
   "source": [
    "We have used the stratify attribute in train_test_split. So, the class distribution is maintained in train and test splits. we can move ahead and extract feature using our tfidfvectorizer class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "strategic-james",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets check the class imbalance in both of train and test splits\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.hist(y_train.value_counts(normalize = True), bins = 25,);\n",
    "ax1.set(title = 'Training Set');\n",
    "ax2.hist(y_test.value_counts(normalize = True), bins = 25,);\n",
    "ax2.set(title = 'Test Set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "arbitrary-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix contains 17910 observations with 2582 features\n"
     ]
    }
   ],
   "source": [
    "features = tfidf.fit_transform(X_train).toarray()\n",
    "\n",
    "print(f'The feature matrix contains {features.shape[0]} observations with {features.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "shaped-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>absence</th>\n",
       "      <th>access</th>\n",
       "      <th>access approach</th>\n",
       "      <th>access area</th>\n",
       "      <th>access arrangement</th>\n",
       "      <th>access available</th>\n",
       "      <th>access block</th>\n",
       "      <th>access egress</th>\n",
       "      <th>access filter</th>\n",
       "      <th>...</th>\n",
       "      <th>workplace screen</th>\n",
       "      <th>wtp</th>\n",
       "      <th>xi</th>\n",
       "      <th>xi im</th>\n",
       "      <th>yadav</th>\n",
       "      <th>yadav engage</th>\n",
       "      <th>yard</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone area</th>\n",
       "      <th>zone site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2582 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  absence  access  access approach  access area  access arrangement  \\\n",
       "0  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "1  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "2  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "3  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "4  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "\n",
       "   access available  access block  access egress  access filter  ...  \\\n",
       "0               0.0           0.0            0.0            0.0  ...   \n",
       "1               0.0           0.0            0.0            0.0  ...   \n",
       "2               0.0           0.0            0.0            0.0  ...   \n",
       "3               0.0           0.0            0.0            0.0  ...   \n",
       "4               0.0           0.0            0.0            0.0  ...   \n",
       "\n",
       "   workplace screen  wtp   xi  xi im  yadav  yadav engage  yard  zone  \\\n",
       "0               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "1               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "2               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "3               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "4               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "\n",
       "   zone area  zone site  \n",
       "0        0.0        0.0  \n",
       "1        0.0        0.0  \n",
       "2        0.0        0.0  \n",
       "3        0.0        0.0  \n",
       "4        0.0        0.0  \n",
       "\n",
       "[5 rows x 2582 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us create the feature matrix for our future reference\n",
    "\n",
    "feature_matrix = pd.DataFrame(data = features, columns = tfidf.get_feature_names())\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-thickness",
   "metadata": {},
   "source": [
    "That's a lot of features and it would be tedious and inefficient to fit all these features to the model. So, it would be better at this stage that to select features that are most important and that can acheive maximum gains for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-confusion",
   "metadata": {},
   "source": [
    "We will do this in two steps,\n",
    "\n",
    "* by computing chi-squared stats between each non-negative feature and class.\n",
    "* Non-negative matrix factorization\n",
    "\n",
    "Chi2 score can be used to select the features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "taken-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "advance-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running chi2 test and storing the chi statistics and p_value\n",
    "\n",
    "chi2_features, p_value = chi2(feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "greenhouse-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forming the dataframe with features, chi statistics, and p_value\n",
    "\n",
    "df_chi2_features = pd.DataFrame(data = [feature_matrix.columns, chi2_features, p_value]).T\n",
    "df_chi2_features.columns = ['Feature_name', 'Chi value', 'p_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-honduras",
   "metadata": {},
   "source": [
    "We form our hyothesis here with the significance level of alpha = 0.05.\n",
    "\n",
    "Thus our Null hypothesis is that there is no significance association between the input variable and target variable. While our alternate hypothesis is that there is significance association between the input variable and target variable.\n",
    "\n",
    "Consequently, we will select only those features which have p_value less that 0.05 (Significance level), so that we can reject our null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "hungarian-jonathan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_name</th>\n",
       "      <th>Chi value</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>access</td>\n",
       "      <td>2632.696183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>harness</td>\n",
       "      <td>3083.436262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>height pas</td>\n",
       "      <td>2170.761754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>helmet</td>\n",
       "      <td>6901.796873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>hoe</td>\n",
       "      <td>2082.010137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>slab area</td>\n",
       "      <td>89.87553</td>\n",
       "      <td>0.046525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>ram nagar</td>\n",
       "      <td>89.862447</td>\n",
       "      <td>0.046616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>block</td>\n",
       "      <td>89.811884</td>\n",
       "      <td>0.04697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>cable observe</td>\n",
       "      <td>89.54714</td>\n",
       "      <td>0.048858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>near excavation</td>\n",
       "      <td>89.476313</td>\n",
       "      <td>0.049374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature_name    Chi value   p_value\n",
       "2              access  2632.696183       0.0\n",
       "898           harness  3083.436262       0.0\n",
       "925        height pas  2170.761754       0.0\n",
       "930            helmet  6901.796873       0.0\n",
       "939               hoe  2082.010137       0.0\n",
       "...               ...          ...       ...\n",
       "2124        slab area     89.87553  0.046525\n",
       "1847        ram nagar    89.862447  0.046616\n",
       "202             block    89.811884   0.04697\n",
       "277     cable observe     89.54714  0.048858\n",
       "1425  near excavation    89.476313  0.049374\n",
       "\n",
       "[1303 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chi2_features.loc[df_chi2_features['p_value']< 0.05].sort_values(by = 'p_value', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-thought",
   "metadata": {},
   "source": [
    "Here we can see that we have around 1303 features which are statistically significant thus we can retain them for further modeling. Accordingly, we will drop all other features from our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "verbal-cannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1303"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features = df_chi2_features.loc[df_chi2_features['p_value']< 0.05].sort_values(by = 'p_value', ascending = True)\n",
    "final_feature_columns = list(final_features['Feature_name'])\n",
    "len(final_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "super-volleyball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17910, 1303)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feature_matrix = feature_matrix[final_feature_columns].copy()\n",
    "final_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "tired-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test set for future modelling usage\n",
    "\n",
    "test_set_features = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "grave-malpractice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>absence</th>\n",
       "      <th>access</th>\n",
       "      <th>access approach</th>\n",
       "      <th>access area</th>\n",
       "      <th>access arrangement</th>\n",
       "      <th>access available</th>\n",
       "      <th>access block</th>\n",
       "      <th>access egress</th>\n",
       "      <th>access filter</th>\n",
       "      <th>...</th>\n",
       "      <th>workplace screen</th>\n",
       "      <th>wtp</th>\n",
       "      <th>xi</th>\n",
       "      <th>xi im</th>\n",
       "      <th>yadav</th>\n",
       "      <th>yadav engage</th>\n",
       "      <th>yard</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone area</th>\n",
       "      <th>zone site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2582 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  absence  access  access approach  access area  access arrangement  \\\n",
       "0  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "1  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "2  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "3  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "4  0.0      0.0     0.0              0.0          0.0                 0.0   \n",
       "\n",
       "   access available  access block  access egress  access filter  ...  \\\n",
       "0               0.0           0.0            0.0            0.0  ...   \n",
       "1               0.0           0.0            0.0            0.0  ...   \n",
       "2               0.0           0.0            0.0            0.0  ...   \n",
       "3               0.0           0.0            0.0            0.0  ...   \n",
       "4               0.0           0.0            0.0            0.0  ...   \n",
       "\n",
       "   workplace screen  wtp   xi  xi im  yadav  yadav engage  yard  zone  \\\n",
       "0               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "1               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "2               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "3               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "4               0.0  0.0  0.0    0.0    0.0           0.0   0.0   0.0   \n",
       "\n",
       "   zone area  zone site  \n",
       "0        0.0        0.0  \n",
       "1        0.0        0.0  \n",
       "2        0.0        0.0  \n",
       "3        0.0        0.0  \n",
       "4        0.0        0.0  \n",
       "\n",
       "[5 rows x 2582 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_features_matrix = pd.DataFrame(data = test_set_features, columns = tfidf.get_feature_names())\n",
    "test_set_features_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "political-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7677, 1303)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying chi2 test results on test set\n",
    "\n",
    "test_set_final_features_matrix = test_set_features_matrix[final_feature_columns].copy()\n",
    "test_set_final_features_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-instrumentation",
   "metadata": {},
   "source": [
    "Now, its time to reduce the dimensionality of the feature matrix so that the process of further modelling can be eased and improved.\n",
    "\n",
    "We will use Non-Negative Matrix Factorization (NMF).\n",
    "\n",
    "Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\n",
    "\n",
    "We will reduce dimensions to 5 and 10 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "mechanical-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing NMF library\n",
    "\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "together-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the nmf model 1\n",
    "\n",
    "nmf_model_1 = NMF(n_components = 5, \n",
    "                  init = 'random',\n",
    "                  random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "valued-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_features_1 = nmf_model_1.fit_transform(final_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "corrected-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the nmf model 2\n",
    "\n",
    "nmf_model_2 = NMF(n_components = 10, \n",
    "                  init = 'random',\n",
    "                  random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "forward-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_features_2 = nmf_model_2.fit_transform(final_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "curious-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_test_features_1 = nmf_model_1.transform(test_set_final_features_matrix)\n",
    "nmf_test_features_2 = nmf_model_2.transform(test_set_final_features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "developed-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save the model in pickled form for future reference\n",
    "\n",
    "# def pickle_model(model, filename):\n",
    "#     pkl.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "# def de_pickle(model):\n",
    "#     return pkl.load(open(model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "electronic-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_model(nmf_features, 'nmf_features_pkl.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "framed-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nmf_features = de_pickle('nmf_features_pkl.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "objective-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train set - independent variables using NMF Model 1 (17910, 5)\n",
      "The shape of train set - independent variables using NMF Model 2 (17910, 10)\n",
      "The shape of test set - independent variables using NMF Model 1 (7677, 5)\n",
      "The shape of test set - independent variables using NMF Model 2 (7677, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of train set - independent variables using NMF Model 1 {nmf_features_1.shape}')\n",
    "print(f'The shape of train set - independent variables using NMF Model 2 {nmf_features_2.shape}')\n",
    "print(f'The shape of test set - independent variables using NMF Model 1 {nmf_test_features_1.shape}')\n",
    "print(f'The shape of test set - independent variables using NMF Model 2 {nmf_test_features_2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "embedded-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The reconstruction error for NMF Model 1 (components = 5) 109.76651373126356\n",
      " The reconstruction error for NMF Model 2 (components = 10) 107.10156803272466\n"
     ]
    }
   ],
   "source": [
    "# Lets check the reconstruction error\n",
    "print(f' The reconstruction error for NMF Model 1 (components = 5) {nmf_model_1.reconstruction_err_}')\n",
    "print(f' The reconstruction error for NMF Model 2 (components = 10) {nmf_model_2.reconstruction_err_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "centered-drilling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access</th>\n",
       "      <th>harness</th>\n",
       "      <th>height pas</th>\n",
       "      <th>helmet</th>\n",
       "      <th>hoe</th>\n",
       "      <th>housekeep</th>\n",
       "      <th>hydra</th>\n",
       "      <th>hydra operator</th>\n",
       "      <th>id</th>\n",
       "      <th>id card</th>\n",
       "      <th>...</th>\n",
       "      <th>trench safe</th>\n",
       "      <th>unprotected</th>\n",
       "      <th>alignment</th>\n",
       "      <th>chamber barricade</th>\n",
       "      <th>open lead</th>\n",
       "      <th>slab area</th>\n",
       "      <th>ram nagar</th>\n",
       "      <th>block</th>\n",
       "      <th>cable observe</th>\n",
       "      <th>near excavation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.020056</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.02088</td>\n",
       "      <td>0.020289</td>\n",
       "      <td>0.019136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.09262</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.060064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.040636</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.012077</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.029418</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.015676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    access   harness  height pas    helmet       hoe  housekeep     hydra  \\\n",
       "0  0.00000  0.020056    0.005811  0.007584  0.023165   0.008859  0.027371   \n",
       "1  1.09262  0.054026    0.003654  0.004108  0.000248   0.060064  0.000000   \n",
       "2  0.00000  0.000000    0.000000  0.000904  0.001649   0.040636  0.010285   \n",
       "3  0.00000  0.000000    0.000000  0.001939  0.003264   0.006920  0.000886   \n",
       "4  0.00000  0.000000    0.000000  0.000140  0.000000   0.035935  0.000254   \n",
       "\n",
       "   hydra operator        id   id card  ...  trench safe  unprotected  \\\n",
       "0         0.02088  0.020289  0.019136  ...     0.000000     0.000216   \n",
       "1         0.00000  0.002685  0.001751  ...     0.012183     0.005609   \n",
       "2         0.00000  0.000000  0.000000  ...     0.000000     0.000135   \n",
       "3         0.00000  0.000894  0.000000  ...     0.000000     0.001730   \n",
       "4         0.00000  0.000000  0.000000  ...     0.005758     0.003416   \n",
       "\n",
       "   alignment  chamber barricade  open lead  slab area  ram nagar     block  \\\n",
       "0   0.000000           0.000000   0.000300   0.000000   0.001469  0.000000   \n",
       "1   0.002824           0.000000   0.001618   0.002840   0.000000  0.031076   \n",
       "2   0.002949           0.000000   0.000198   0.000734   0.000476  0.006410   \n",
       "3   0.000441           0.029418   0.002710   0.003308   0.001986  0.002750   \n",
       "4   0.009272           0.000000   0.001668   0.005833   0.002471  0.015676   \n",
       "\n",
       "   cable observe  near excavation  \n",
       "0       0.000000         0.000800  \n",
       "1       0.000000         0.000000  \n",
       "2       0.012077         0.000000  \n",
       "3       0.000000         0.012856  \n",
       "4       0.000000         0.041624  \n",
       "\n",
       "[5 rows x 1303 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets visualize them in term of dataframe for NMF Model 1\n",
    "\n",
    "nmf_features_components_df = pd.DataFrame(nmf_model_1.components_, columns = final_feature_columns)\n",
    "nmf_features_components_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "judicial-waterproof",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Names</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>screen</td>\n",
       "      <td>1.198645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>access</td>\n",
       "      <td>1.092620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>electrical</td>\n",
       "      <td>1.101425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barricade</td>\n",
       "      <td>1.285637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trench</td>\n",
       "      <td>1.060997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Names    Values\n",
       "0        screen  1.198645\n",
       "1        access  1.092620\n",
       "2    electrical  1.101425\n",
       "3     barricade  1.285637\n",
       "4        trench  1.060997"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check for each components identified which is the largest contributing featur\n",
    "\n",
    "column_1 = pd.Series(nmf_features_components_df.idxmax(axis = 1))\n",
    "column_2 = pd.Series(nmf_features_components_df.max(axis = 1))\n",
    "\n",
    "largest_component_nmf = pd.concat([column_1, column_2], axis = 1)\n",
    "largest_component_nmf.columns = ['Feature Names', 'Values']\n",
    "largest_component_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "adopted-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(y_train.unique())\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-member",
   "metadata": {},
   "source": [
    "So the top contributing features to our components look quite rational, indicating that we may proceed ahead with the 70 components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-diesel",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Feature Extraction -Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-deposit",
   "metadata": {},
   "source": [
    "* We realised that our data consists of highly imbalanced classes. We tried to take that into consideration by splitting our data using startified option under train_test_split function.\n",
    "* This ensured that we maintained the equal amount of imbalance in our test as well as train split.\n",
    "* Next step was to prepare the text data for further feature extraction. We applied various transformations such as lemmatization amd acheiving homogenization by converting all words into lower case.\n",
    "* We encoded our target variables.\n",
    "* Finally, we used tfidfvectorizer to extract features from our text data. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "* As usual we fitted and transformed tfidfvectorizer over train data and used that fiited model to transform our test data.\n",
    "* We performed chi2 test to extract the best associated features with p-value less than 0.05.\n",
    "* Finally we used NMF method to decompose our features into 5 and 10 Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-yugoslavia",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-preparation",
   "metadata": {},
   "source": [
    "In this step, we will,\n",
    "\n",
    "1. Select the best model and subsequently the best values for model parameters.\n",
    "2. Also, we will try to reach best compromise over the precision and recall values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "measured-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us import all the required modules\n",
    "\n",
    "## Import Keras objects for Deep Learning\n",
    "from tensorflow.keras.models  import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-entrance",
   "metadata": {},
   "source": [
    "#### Model 1 (5 Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "finite-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 5-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(1000,input_shape = (5,),activation = 'relu'))\n",
    "model_1.add(Dropout(0.25, seed = 1000))\n",
    "model_1.add(Dense(1000,activation = 'relu'))\n",
    "model_1.add(Dropout(0.25, seed = 500))\n",
    "model_1.add(Dense(70,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "homeless-tobacco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 1000)              6000      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 70)                70070     \n",
      "=================================================================\n",
      "Total params: 1,077,070\n",
      "Trainable params: 1,077,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "surface-gather",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "597/597 [==============================] - 7s 10ms/step - loss: 4.1808 - auc: 0.6491 - accuracy: 0.1429 - val_loss: 4.1081 - val_auc: 0.7437 - val_accuracy: 0.1391\n",
      "Epoch 2/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 4.0300 - auc: 0.7548 - accuracy: 0.1392 - val_loss: 3.9407 - val_auc: 0.7702 - val_accuracy: 0.1391\n",
      "Epoch 3/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.8469 - auc: 0.7709 - accuracy: 0.1392 - val_loss: 3.7478 - val_auc: 0.7757 - val_accuracy: 0.1391\n",
      "Epoch 4/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.6814 - auc: 0.7999 - accuracy: 0.1392 - val_loss: 3.6206 - val_auc: 0.8043 - val_accuracy: 0.1391\n",
      "Epoch 5/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.5892 - auc: 0.7975 - accuracy: 0.1392 - val_loss: 3.5526 - val_auc: 0.7974 - val_accuracy: 0.1391\n",
      "Epoch 6/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.5355 - auc: 0.8000 - accuracy: 0.1400 - val_loss: 3.5099 - val_auc: 0.8077 - val_accuracy: 0.1391\n",
      "Epoch 7/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.5028 - auc: 0.8139 - accuracy: 0.1412 - val_loss: 3.4837 - val_auc: 0.8216 - val_accuracy: 0.1391\n",
      "Epoch 8/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.4811 - auc: 0.8205 - accuracy: 0.1426 - val_loss: 3.4666 - val_auc: 0.8230 - val_accuracy: 0.1391\n",
      "Epoch 9/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.4680 - auc: 0.8217 - accuracy: 0.1455 - val_loss: 3.4543 - val_auc: 0.8239 - val_accuracy: 0.1391\n",
      "Epoch 10/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.4566 - auc: 0.8224 - accuracy: 0.1469 - val_loss: 3.4445 - val_auc: 0.8235 - val_accuracy: 0.1391\n",
      "Epoch 11/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 3.4474 - auc: 0.8229 - accuracy: 0.1492 - val_loss: 3.4362 - val_auc: 0.8246 - val_accuracy: 0.1392\n",
      "Epoch 12/200\n",
      "597/597 [==============================] - 6s 11ms/step - loss: 3.4405 - auc: 0.8234 - accuracy: 0.1516 - val_loss: 3.4288 - val_auc: 0.8252 - val_accuracy: 0.1411\n",
      "Epoch 13/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.4333 - auc: 0.8240 - accuracy: 0.1549 - val_loss: 3.4218 - val_auc: 0.8253 - val_accuracy: 0.1421\n",
      "Epoch 14/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.4245 - auc: 0.8243 - accuracy: 0.1586 - val_loss: 3.4148 - val_auc: 0.8261 - val_accuracy: 0.1514\n",
      "Epoch 15/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.4199 - auc: 0.8247 - accuracy: 0.1656 - val_loss: 3.4078 - val_auc: 0.8272 - val_accuracy: 0.1477\n",
      "Epoch 16/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.4112 - auc: 0.8253 - accuracy: 0.1678 - val_loss: 3.4005 - val_auc: 0.8266 - val_accuracy: 0.1540\n",
      "Epoch 17/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.4053 - auc: 0.8257 - accuracy: 0.1724 - val_loss: 3.3928 - val_auc: 0.8273 - val_accuracy: 0.1662\n",
      "Epoch 18/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.3971 - auc: 0.8259 - accuracy: 0.1795 - val_loss: 3.3846 - val_auc: 0.8272 - val_accuracy: 0.1692\n",
      "Epoch 19/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.3905 - auc: 0.8265 - accuracy: 0.1840 - val_loss: 3.3757 - val_auc: 0.8275 - val_accuracy: 0.1641\n",
      "Epoch 20/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.3801 - auc: 0.8273 - accuracy: 0.1839 - val_loss: 3.3660 - val_auc: 0.8288 - val_accuracy: 0.2112\n",
      "Epoch 21/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.3692 - auc: 0.8278 - accuracy: 0.1944 - val_loss: 3.3553 - val_auc: 0.8312 - val_accuracy: 0.2171\n",
      "Epoch 22/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.3565 - auc: 0.8283 - accuracy: 0.2012 - val_loss: 3.3432 - val_auc: 0.8314 - val_accuracy: 0.2141\n",
      "Epoch 23/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.3450 - auc: 0.8294 - accuracy: 0.2048 - val_loss: 3.3308 - val_auc: 0.8320 - val_accuracy: 0.2264\n",
      "Epoch 24/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.3341 - auc: 0.8298 - accuracy: 0.2111 - val_loss: 3.3163 - val_auc: 0.8324 - val_accuracy: 0.2224\n",
      "Epoch 25/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.3184 - auc: 0.8309 - accuracy: 0.2161 - val_loss: 3.3013 - val_auc: 0.8335 - val_accuracy: 0.2257\n",
      "Epoch 26/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.3042 - auc: 0.8318 - accuracy: 0.2220 - val_loss: 3.2853 - val_auc: 0.8351 - val_accuracy: 0.2337\n",
      "Epoch 27/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.2869 - auc: 0.8325 - accuracy: 0.2271 - val_loss: 3.2683 - val_auc: 0.8346 - val_accuracy: 0.2359\n",
      "Epoch 28/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.2729 - auc: 0.8338 - accuracy: 0.2301 - val_loss: 3.2512 - val_auc: 0.8359 - val_accuracy: 0.2427\n",
      "Epoch 29/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.2534 - auc: 0.8345 - accuracy: 0.2374 - val_loss: 3.2325 - val_auc: 0.8372 - val_accuracy: 0.2435\n",
      "Epoch 30/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.2368 - auc: 0.8359 - accuracy: 0.2384 - val_loss: 3.2143 - val_auc: 0.8388 - val_accuracy: 0.2457\n",
      "Epoch 31/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.2165 - auc: 0.8378 - accuracy: 0.2437 - val_loss: 3.1960 - val_auc: 0.8405 - val_accuracy: 0.2478\n",
      "Epoch 32/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.1983 - auc: 0.8387 - accuracy: 0.2485 - val_loss: 3.1781 - val_auc: 0.8419 - val_accuracy: 0.2511\n",
      "Epoch 33/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.1843 - auc: 0.8404 - accuracy: 0.2523 - val_loss: 3.1603 - val_auc: 0.8432 - val_accuracy: 0.2561\n",
      "Epoch 34/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 3.1651 - auc: 0.8417 - accuracy: 0.2549 - val_loss: 3.1433 - val_auc: 0.8442 - val_accuracy: 0.2592\n",
      "Epoch 35/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.1483 - auc: 0.8436 - accuracy: 0.2586 - val_loss: 3.1271 - val_auc: 0.8457 - val_accuracy: 0.2647\n",
      "Epoch 36/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.1317 - auc: 0.8454 - accuracy: 0.2644 - val_loss: 3.1110 - val_auc: 0.8471 - val_accuracy: 0.2647\n",
      "Epoch 37/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.1167 - auc: 0.8466 - accuracy: 0.2684 - val_loss: 3.0951 - val_auc: 0.8487 - val_accuracy: 0.2657\n",
      "Epoch 38/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.1007 - auc: 0.8478 - accuracy: 0.2720 - val_loss: 3.0798 - val_auc: 0.8501 - val_accuracy: 0.2778\n",
      "Epoch 39/200\n",
      "597/597 [==============================] - 6s 9ms/step - loss: 3.0866 - auc: 0.8492 - accuracy: 0.2821 - val_loss: 3.0653 - val_auc: 0.8515 - val_accuracy: 0.2721\n",
      "Epoch 40/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 3.0706 - auc: 0.8515 - accuracy: 0.2839 - val_loss: 3.0504 - val_auc: 0.8526 - val_accuracy: 0.2883\n",
      "Epoch 41/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.0563 - auc: 0.8525 - accuracy: 0.2898 - val_loss: 3.0358 - val_auc: 0.8539 - val_accuracy: 0.3038\n",
      "Epoch 42/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 3.0405 - auc: 0.8542 - accuracy: 0.2966 - val_loss: 3.0211 - val_auc: 0.8555 - val_accuracy: 0.3069\n",
      "Epoch 43/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 3.0252 - auc: 0.8558 - accuracy: 0.3025 - val_loss: 3.0071 - val_auc: 0.8565 - val_accuracy: 0.3098\n",
      "Epoch 44/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 3.0095 - auc: 0.8574 - accuracy: 0.3101 - val_loss: 2.9921 - val_auc: 0.8580 - val_accuracy: 0.3191\n",
      "Epoch 45/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.9979 - auc: 0.8589 - accuracy: 0.3139 - val_loss: 2.9780 - val_auc: 0.8602 - val_accuracy: 0.3378\n",
      "Epoch 46/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9796 - auc: 0.8605 - accuracy: 0.3209 - val_loss: 2.9629 - val_auc: 0.8615 - val_accuracy: 0.3333\n",
      "Epoch 47/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.9675 - auc: 0.8617 - accuracy: 0.3239 - val_loss: 2.9489 - val_auc: 0.8624 - val_accuracy: 0.3418\n",
      "Epoch 48/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9524 - auc: 0.8632 - accuracy: 0.3250 - val_loss: 2.9331 - val_auc: 0.8642 - val_accuracy: 0.3464\n",
      "Epoch 49/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.9361 - auc: 0.8647 - accuracy: 0.3279 - val_loss: 2.9182 - val_auc: 0.8653 - val_accuracy: 0.3495\n",
      "Epoch 50/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.9217 - auc: 0.8662 - accuracy: 0.3312 - val_loss: 2.9028 - val_auc: 0.8674 - val_accuracy: 0.3445\n",
      "Epoch 51/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.9046 - auc: 0.8681 - accuracy: 0.3352 - val_loss: 2.8875 - val_auc: 0.8689 - val_accuracy: 0.3456\n",
      "Epoch 52/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.8927 - auc: 0.8695 - accuracy: 0.3355 - val_loss: 2.8721 - val_auc: 0.8703 - val_accuracy: 0.3465\n",
      "Epoch 53/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.8743 - auc: 0.8713 - accuracy: 0.3404 - val_loss: 2.8574 - val_auc: 0.8711 - val_accuracy: 0.3481\n",
      "Epoch 54/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.8609 - auc: 0.8726 - accuracy: 0.3406 - val_loss: 2.8412 - val_auc: 0.8742 - val_accuracy: 0.3505\n",
      "Epoch 55/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.8441 - auc: 0.8745 - accuracy: 0.3450 - val_loss: 2.8256 - val_auc: 0.8756 - val_accuracy: 0.3484\n",
      "Epoch 56/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.8290 - auc: 0.8759 - accuracy: 0.3452 - val_loss: 2.8100 - val_auc: 0.8773 - val_accuracy: 0.3501\n",
      "Epoch 57/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.8145 - auc: 0.8768 - accuracy: 0.3487 - val_loss: 2.7948 - val_auc: 0.8792 - val_accuracy: 0.3494\n",
      "Epoch 58/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.7965 - auc: 0.8791 - accuracy: 0.3470 - val_loss: 2.7792 - val_auc: 0.8807 - val_accuracy: 0.3530\n",
      "Epoch 59/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 2.7821 - auc: 0.8804 - accuracy: 0.3520 - val_loss: 2.7639 - val_auc: 0.8826 - val_accuracy: 0.3561\n",
      "Epoch 60/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7680 - auc: 0.8824 - accuracy: 0.3558 - val_loss: 2.7489 - val_auc: 0.8841 - val_accuracy: 0.3539\n",
      "Epoch 61/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.7513 - auc: 0.8838 - accuracy: 0.3555 - val_loss: 2.7336 - val_auc: 0.8855 - val_accuracy: 0.3593\n",
      "Epoch 62/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.7363 - auc: 0.8851 - accuracy: 0.3611 - val_loss: 2.7186 - val_auc: 0.8868 - val_accuracy: 0.3663\n",
      "Epoch 63/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7219 - auc: 0.8869 - accuracy: 0.3619 - val_loss: 2.7037 - val_auc: 0.8883 - val_accuracy: 0.3822\n",
      "Epoch 64/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7087 - auc: 0.8884 - accuracy: 0.3671 - val_loss: 2.6894 - val_auc: 0.8896 - val_accuracy: 0.3852\n",
      "Epoch 65/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.6935 - auc: 0.8902 - accuracy: 0.3711 - val_loss: 2.6744 - val_auc: 0.8916 - val_accuracy: 0.3850\n",
      "Epoch 66/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6791 - auc: 0.8913 - accuracy: 0.3710 - val_loss: 2.6606 - val_auc: 0.8930 - val_accuracy: 0.3879\n",
      "Epoch 67/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.6653 - auc: 0.8937 - accuracy: 0.3733 - val_loss: 2.6466 - val_auc: 0.8946 - val_accuracy: 0.3891\n",
      "Epoch 68/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6517 - auc: 0.8948 - accuracy: 0.3760 - val_loss: 2.6328 - val_auc: 0.8958 - val_accuracy: 0.3944\n",
      "Epoch 69/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6378 - auc: 0.8958 - accuracy: 0.3774 - val_loss: 2.6199 - val_auc: 0.8970 - val_accuracy: 0.3935\n",
      "Epoch 70/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.6236 - auc: 0.8974 - accuracy: 0.3792 - val_loss: 2.6065 - val_auc: 0.8982 - val_accuracy: 0.3948\n",
      "Epoch 71/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.6122 - auc: 0.8987 - accuracy: 0.3829 - val_loss: 2.5935 - val_auc: 0.8996 - val_accuracy: 0.3962\n",
      "Epoch 72/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5989 - auc: 0.9001 - accuracy: 0.3844 - val_loss: 2.5815 - val_auc: 0.9007 - val_accuracy: 0.3965\n",
      "Epoch 73/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.5878 - auc: 0.9017 - accuracy: 0.3863 - val_loss: 2.5691 - val_auc: 0.9020 - val_accuracy: 0.3979\n",
      "Epoch 74/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5734 - auc: 0.9034 - accuracy: 0.3899 - val_loss: 2.5574 - val_auc: 0.9033 - val_accuracy: 0.4005\n",
      "Epoch 75/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.5636 - auc: 0.9042 - accuracy: 0.3902 - val_loss: 2.5456 - val_auc: 0.9048 - val_accuracy: 0.4012\n",
      "Epoch 76/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.5521 - auc: 0.9054 - accuracy: 0.3915 - val_loss: 2.5357 - val_auc: 0.9056 - val_accuracy: 0.4000\n",
      "Epoch 77/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.5429 - auc: 0.9061 - accuracy: 0.3930 - val_loss: 2.5240 - val_auc: 0.9069 - val_accuracy: 0.4026\n",
      "Epoch 78/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5309 - auc: 0.9077 - accuracy: 0.3931 - val_loss: 2.5130 - val_auc: 0.9082 - val_accuracy: 0.4037\n",
      "Epoch 79/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.5192 - auc: 0.9093 - accuracy: 0.3942 - val_loss: 2.5028 - val_auc: 0.9091 - val_accuracy: 0.4024\n",
      "Epoch 80/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.5092 - auc: 0.9099 - accuracy: 0.3950 - val_loss: 2.4931 - val_auc: 0.9100 - val_accuracy: 0.4054\n",
      "Epoch 81/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4988 - auc: 0.9110 - accuracy: 0.3968 - val_loss: 2.4833 - val_auc: 0.9110 - val_accuracy: 0.4050\n",
      "Epoch 82/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4904 - auc: 0.9122 - accuracy: 0.3950 - val_loss: 2.4745 - val_auc: 0.9121 - val_accuracy: 0.4056\n",
      "Epoch 83/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4789 - auc: 0.9133 - accuracy: 0.3989 - val_loss: 2.4652 - val_auc: 0.9129 - val_accuracy: 0.4065\n",
      "Epoch 84/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4716 - auc: 0.9138 - accuracy: 0.3971 - val_loss: 2.4569 - val_auc: 0.9138 - val_accuracy: 0.4084\n",
      "Epoch 85/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4620 - auc: 0.9152 - accuracy: 0.4003 - val_loss: 2.4478 - val_auc: 0.9149 - val_accuracy: 0.4082\n",
      "Epoch 86/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4550 - auc: 0.9160 - accuracy: 0.4000 - val_loss: 2.4386 - val_auc: 0.9163 - val_accuracy: 0.4078\n",
      "Epoch 87/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4460 - auc: 0.9167 - accuracy: 0.3987 - val_loss: 2.4313 - val_auc: 0.9170 - val_accuracy: 0.4084\n",
      "Epoch 88/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 2.4360 - auc: 0.9181 - accuracy: 0.4016 - val_loss: 2.4233 - val_auc: 0.9181 - val_accuracy: 0.4094\n",
      "Epoch 89/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4324 - auc: 0.9188 - accuracy: 0.3993 - val_loss: 2.4155 - val_auc: 0.9188 - val_accuracy: 0.4078\n",
      "Epoch 90/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4216 - auc: 0.9202 - accuracy: 0.4012 - val_loss: 2.4077 - val_auc: 0.9194 - val_accuracy: 0.4072\n",
      "Epoch 91/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4136 - auc: 0.9201 - accuracy: 0.4023 - val_loss: 2.4008 - val_auc: 0.9204 - val_accuracy: 0.4090\n",
      "Epoch 92/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4079 - auc: 0.9212 - accuracy: 0.4000 - val_loss: 2.3929 - val_auc: 0.9211 - val_accuracy: 0.4102\n",
      "Epoch 93/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.4009 - auc: 0.9218 - accuracy: 0.4015 - val_loss: 2.3857 - val_auc: 0.9219 - val_accuracy: 0.4110\n",
      "Epoch 94/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3919 - auc: 0.9230 - accuracy: 0.4039 - val_loss: 2.3797 - val_auc: 0.9225 - val_accuracy: 0.4104\n",
      "Epoch 95/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3863 - auc: 0.9234 - accuracy: 0.4018 - val_loss: 2.3728 - val_auc: 0.9230 - val_accuracy: 0.4120\n",
      "Epoch 96/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3795 - auc: 0.9239 - accuracy: 0.4030 - val_loss: 2.3664 - val_auc: 0.9241 - val_accuracy: 0.4111\n",
      "Epoch 97/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3739 - auc: 0.9245 - accuracy: 0.4056 - val_loss: 2.3598 - val_auc: 0.9249 - val_accuracy: 0.4115\n",
      "Epoch 98/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3675 - auc: 0.9252 - accuracy: 0.4044 - val_loss: 2.3534 - val_auc: 0.9259 - val_accuracy: 0.4107\n",
      "Epoch 99/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3615 - auc: 0.9263 - accuracy: 0.4047 - val_loss: 2.3482 - val_auc: 0.9265 - val_accuracy: 0.4125\n",
      "Epoch 100/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3539 - auc: 0.9273 - accuracy: 0.4041 - val_loss: 2.3419 - val_auc: 0.9269 - val_accuracy: 0.4134\n",
      "Epoch 101/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3508 - auc: 0.9269 - accuracy: 0.4065 - val_loss: 2.3366 - val_auc: 0.9275 - val_accuracy: 0.4124\n",
      "Epoch 102/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3454 - auc: 0.9274 - accuracy: 0.4044 - val_loss: 2.3304 - val_auc: 0.9280 - val_accuracy: 0.4138\n",
      "Epoch 103/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3391 - auc: 0.9283 - accuracy: 0.4064 - val_loss: 2.3249 - val_auc: 0.9287 - val_accuracy: 0.4141\n",
      "Epoch 104/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3338 - auc: 0.9289 - accuracy: 0.4070 - val_loss: 2.3205 - val_auc: 0.9288 - val_accuracy: 0.4136\n",
      "Epoch 105/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3288 - auc: 0.9296 - accuracy: 0.4061 - val_loss: 2.3145 - val_auc: 0.9295 - val_accuracy: 0.4141\n",
      "Epoch 106/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3224 - auc: 0.9302 - accuracy: 0.4073 - val_loss: 2.3089 - val_auc: 0.9299 - val_accuracy: 0.4138\n",
      "Epoch 107/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3185 - auc: 0.9304 - accuracy: 0.4085 - val_loss: 2.3053 - val_auc: 0.9305 - val_accuracy: 0.4134\n",
      "Epoch 108/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3126 - auc: 0.9309 - accuracy: 0.4074 - val_loss: 2.3001 - val_auc: 0.9306 - val_accuracy: 0.4159\n",
      "Epoch 109/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3080 - auc: 0.9316 - accuracy: 0.4090 - val_loss: 2.2951 - val_auc: 0.9314 - val_accuracy: 0.4121\n",
      "Epoch 110/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3024 - auc: 0.9319 - accuracy: 0.4089 - val_loss: 2.2905 - val_auc: 0.9318 - val_accuracy: 0.4150\n",
      "Epoch 111/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2980 - auc: 0.9323 - accuracy: 0.4088 - val_loss: 2.2853 - val_auc: 0.9325 - val_accuracy: 0.4157\n",
      "Epoch 112/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2952 - auc: 0.9327 - accuracy: 0.4111 - val_loss: 2.2805 - val_auc: 0.9332 - val_accuracy: 0.4157\n",
      "Epoch 113/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2899 - auc: 0.9336 - accuracy: 0.4082 - val_loss: 2.2765 - val_auc: 0.9334 - val_accuracy: 0.4162\n",
      "Epoch 114/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2856 - auc: 0.9336 - accuracy: 0.4113 - val_loss: 2.2718 - val_auc: 0.9341 - val_accuracy: 0.4163\n",
      "Epoch 115/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2823 - auc: 0.9339 - accuracy: 0.4104 - val_loss: 2.2675 - val_auc: 0.9344 - val_accuracy: 0.4159\n",
      "Epoch 116/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2754 - auc: 0.9345 - accuracy: 0.4104 - val_loss: 2.2640 - val_auc: 0.9345 - val_accuracy: 0.4158\n",
      "Epoch 117/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2733 - auc: 0.9348 - accuracy: 0.4105 - val_loss: 2.2593 - val_auc: 0.9348 - val_accuracy: 0.4160\n",
      "Epoch 118/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.2698 - auc: 0.9350 - accuracy: 0.4092 - val_loss: 2.2563 - val_auc: 0.9351 - val_accuracy: 0.4168\n",
      "Epoch 119/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2649 - auc: 0.9354 - accuracy: 0.4117 - val_loss: 2.2520 - val_auc: 0.9354 - val_accuracy: 0.4167\n",
      "Epoch 120/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2604 - auc: 0.9356 - accuracy: 0.4122 - val_loss: 2.2477 - val_auc: 0.9358 - val_accuracy: 0.4171\n",
      "Epoch 121/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2573 - auc: 0.9358 - accuracy: 0.4112 - val_loss: 2.2441 - val_auc: 0.9363 - val_accuracy: 0.4181\n",
      "Epoch 122/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2527 - auc: 0.9363 - accuracy: 0.4117 - val_loss: 2.2408 - val_auc: 0.9363 - val_accuracy: 0.4167\n",
      "Epoch 123/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2496 - auc: 0.9367 - accuracy: 0.4118 - val_loss: 2.2368 - val_auc: 0.9369 - val_accuracy: 0.4149\n",
      "Epoch 124/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2431 - auc: 0.9373 - accuracy: 0.4142 - val_loss: 2.2329 - val_auc: 0.9372 - val_accuracy: 0.4179\n",
      "Epoch 125/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2420 - auc: 0.9373 - accuracy: 0.4123 - val_loss: 2.2296 - val_auc: 0.9376 - val_accuracy: 0.4184\n",
      "Epoch 126/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2390 - auc: 0.9375 - accuracy: 0.4118 - val_loss: 2.2259 - val_auc: 0.9378 - val_accuracy: 0.4177\n",
      "Epoch 127/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2353 - auc: 0.9379 - accuracy: 0.4126 - val_loss: 2.2225 - val_auc: 0.9380 - val_accuracy: 0.4166\n",
      "Epoch 128/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2313 - auc: 0.9379 - accuracy: 0.4119 - val_loss: 2.2192 - val_auc: 0.9383 - val_accuracy: 0.4177\n",
      "Epoch 129/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2284 - auc: 0.9383 - accuracy: 0.4136 - val_loss: 2.2175 - val_auc: 0.9386 - val_accuracy: 0.4172\n",
      "Epoch 130/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2248 - auc: 0.9389 - accuracy: 0.4138 - val_loss: 2.2132 - val_auc: 0.9388 - val_accuracy: 0.4185\n",
      "Epoch 131/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2228 - auc: 0.9390 - accuracy: 0.4132 - val_loss: 2.2115 - val_auc: 0.9388 - val_accuracy: 0.4163\n",
      "Epoch 132/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2173 - auc: 0.9395 - accuracy: 0.4141 - val_loss: 2.2076 - val_auc: 0.9392 - val_accuracy: 0.4166\n",
      "Epoch 133/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2158 - auc: 0.9394 - accuracy: 0.4134 - val_loss: 2.2034 - val_auc: 0.9396 - val_accuracy: 0.4174\n",
      "Epoch 134/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2119 - auc: 0.9399 - accuracy: 0.4137 - val_loss: 2.2007 - val_auc: 0.9398 - val_accuracy: 0.4180\n",
      "Epoch 135/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2090 - auc: 0.9402 - accuracy: 0.4129 - val_loss: 2.1979 - val_auc: 0.9403 - val_accuracy: 0.4185\n",
      "Epoch 136/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2073 - auc: 0.9402 - accuracy: 0.4141 - val_loss: 2.1946 - val_auc: 0.9405 - val_accuracy: 0.4175\n",
      "Epoch 137/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2038 - auc: 0.9405 - accuracy: 0.4136 - val_loss: 2.1933 - val_auc: 0.9408 - val_accuracy: 0.4197\n",
      "Epoch 138/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2017 - auc: 0.9407 - accuracy: 0.4140 - val_loss: 2.1896 - val_auc: 0.9410 - val_accuracy: 0.4184\n",
      "Epoch 139/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1974 - auc: 0.9407 - accuracy: 0.4141 - val_loss: 2.1906 - val_auc: 0.9408 - val_accuracy: 0.4172\n",
      "Epoch 140/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1970 - auc: 0.9408 - accuracy: 0.4139 - val_loss: 2.1838 - val_auc: 0.9417 - val_accuracy: 0.4179\n",
      "Epoch 141/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1916 - auc: 0.9417 - accuracy: 0.4160 - val_loss: 2.1819 - val_auc: 0.9418 - val_accuracy: 0.4192\n",
      "Epoch 142/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1909 - auc: 0.9414 - accuracy: 0.4144 - val_loss: 2.1799 - val_auc: 0.9419 - val_accuracy: 0.4177\n",
      "Epoch 143/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1895 - auc: 0.9416 - accuracy: 0.4151 - val_loss: 2.1763 - val_auc: 0.9422 - val_accuracy: 0.4176\n",
      "Epoch 144/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1849 - auc: 0.9419 - accuracy: 0.4142 - val_loss: 2.1735 - val_auc: 0.9425 - val_accuracy: 0.4179\n",
      "Epoch 145/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1835 - auc: 0.9422 - accuracy: 0.4165 - val_loss: 2.1719 - val_auc: 0.9424 - val_accuracy: 0.4176\n",
      "Epoch 146/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1789 - auc: 0.9426 - accuracy: 0.4155 - val_loss: 2.1683 - val_auc: 0.9428 - val_accuracy: 0.4185\n",
      "Epoch 147/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1764 - auc: 0.9427 - accuracy: 0.4151 - val_loss: 2.1664 - val_auc: 0.9429 - val_accuracy: 0.4192\n",
      "Epoch 148/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1755 - auc: 0.9427 - accuracy: 0.4150 - val_loss: 2.1645 - val_auc: 0.9430 - val_accuracy: 0.4206\n",
      "Epoch 149/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1717 - auc: 0.9428 - accuracy: 0.4151 - val_loss: 2.1619 - val_auc: 0.9431 - val_accuracy: 0.4224\n",
      "Epoch 150/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1706 - auc: 0.9431 - accuracy: 0.4189 - val_loss: 2.1607 - val_auc: 0.9431 - val_accuracy: 0.4197\n",
      "Epoch 151/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1684 - auc: 0.9431 - accuracy: 0.4168 - val_loss: 2.1583 - val_auc: 0.9436 - val_accuracy: 0.4194\n",
      "Epoch 152/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1628 - auc: 0.9435 - accuracy: 0.4176 - val_loss: 2.1563 - val_auc: 0.9438 - val_accuracy: 0.4216\n",
      "Epoch 153/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1624 - auc: 0.9437 - accuracy: 0.4156 - val_loss: 2.1535 - val_auc: 0.9437 - val_accuracy: 0.4190\n",
      "Epoch 154/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1604 - auc: 0.9439 - accuracy: 0.4141 - val_loss: 2.1511 - val_auc: 0.9439 - val_accuracy: 0.4222\n",
      "Epoch 155/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1585 - auc: 0.9435 - accuracy: 0.4155 - val_loss: 2.1492 - val_auc: 0.9442 - val_accuracy: 0.4200\n",
      "Epoch 156/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1577 - auc: 0.9440 - accuracy: 0.4161 - val_loss: 2.1467 - val_auc: 0.9445 - val_accuracy: 0.4203\n",
      "Epoch 157/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1549 - auc: 0.9444 - accuracy: 0.4175 - val_loss: 2.1453 - val_auc: 0.9446 - val_accuracy: 0.4192\n",
      "Epoch 158/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1519 - auc: 0.9451 - accuracy: 0.4162 - val_loss: 2.1425 - val_auc: 0.9447 - val_accuracy: 0.4197\n",
      "Epoch 159/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1527 - auc: 0.9445 - accuracy: 0.4167 - val_loss: 2.1407 - val_auc: 0.9450 - val_accuracy: 0.4216\n",
      "Epoch 160/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1486 - auc: 0.9447 - accuracy: 0.4178 - val_loss: 2.1407 - val_auc: 0.9447 - val_accuracy: 0.4196\n",
      "Epoch 161/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 2.1471 - auc: 0.9453 - accuracy: 0.4187 - val_loss: 2.1376 - val_auc: 0.9450 - val_accuracy: 0.4203\n",
      "Epoch 162/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1457 - auc: 0.9450 - accuracy: 0.4175 - val_loss: 2.1353 - val_auc: 0.9453 - val_accuracy: 0.4202\n",
      "Epoch 163/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1411 - auc: 0.9454 - accuracy: 0.4191 - val_loss: 2.1334 - val_auc: 0.9456 - val_accuracy: 0.4224\n",
      "Epoch 164/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1423 - auc: 0.9451 - accuracy: 0.4183 - val_loss: 2.1312 - val_auc: 0.9456 - val_accuracy: 0.4209\n",
      "Epoch 165/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1419 - auc: 0.9453 - accuracy: 0.4181 - val_loss: 2.1298 - val_auc: 0.9457 - val_accuracy: 0.4215\n",
      "Epoch 166/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1365 - auc: 0.9458 - accuracy: 0.4179 - val_loss: 2.1282 - val_auc: 0.9461 - val_accuracy: 0.4216\n",
      "Epoch 167/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1356 - auc: 0.9458 - accuracy: 0.4180 - val_loss: 2.1270 - val_auc: 0.9459 - val_accuracy: 0.4237\n",
      "Epoch 168/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1347 - auc: 0.9457 - accuracy: 0.4188 - val_loss: 2.1257 - val_auc: 0.9458 - val_accuracy: 0.4223\n",
      "Epoch 169/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1335 - auc: 0.9460 - accuracy: 0.4170 - val_loss: 2.1228 - val_auc: 0.9461 - val_accuracy: 0.4222\n",
      "Epoch 170/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1312 - auc: 0.9460 - accuracy: 0.4186 - val_loss: 2.1230 - val_auc: 0.9461 - val_accuracy: 0.4213\n",
      "Epoch 171/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1297 - auc: 0.9461 - accuracy: 0.4176 - val_loss: 2.1201 - val_auc: 0.9463 - val_accuracy: 0.4216\n",
      "Epoch 172/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1268 - auc: 0.9463 - accuracy: 0.4186 - val_loss: 2.1184 - val_auc: 0.9464 - val_accuracy: 0.4231\n",
      "Epoch 173/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1275 - auc: 0.9466 - accuracy: 0.4170 - val_loss: 2.1167 - val_auc: 0.9465 - val_accuracy: 0.4224\n",
      "Epoch 174/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1250 - auc: 0.9466 - accuracy: 0.4202 - val_loss: 2.1152 - val_auc: 0.9465 - val_accuracy: 0.4241\n",
      "Epoch 175/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1236 - auc: 0.9469 - accuracy: 0.4182 - val_loss: 2.1142 - val_auc: 0.9467 - val_accuracy: 0.4239\n",
      "Epoch 176/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1199 - auc: 0.9472 - accuracy: 0.4223 - val_loss: 2.1117 - val_auc: 0.9468 - val_accuracy: 0.4218\n",
      "Epoch 177/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1181 - auc: 0.9472 - accuracy: 0.4184 - val_loss: 2.1123 - val_auc: 0.9467 - val_accuracy: 0.4246\n",
      "Epoch 178/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1203 - auc: 0.9469 - accuracy: 0.4203 - val_loss: 2.1098 - val_auc: 0.9468 - val_accuracy: 0.4253\n",
      "Epoch 179/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1169 - auc: 0.9470 - accuracy: 0.4197 - val_loss: 2.1086 - val_auc: 0.9471 - val_accuracy: 0.4236\n",
      "Epoch 180/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1164 - auc: 0.9472 - accuracy: 0.4199 - val_loss: 2.1083 - val_auc: 0.9470 - val_accuracy: 0.4241\n",
      "Epoch 181/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1135 - auc: 0.9471 - accuracy: 0.4230 - val_loss: 2.1066 - val_auc: 0.9472 - val_accuracy: 0.4252\n",
      "Epoch 182/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1133 - auc: 0.9476 - accuracy: 0.4194 - val_loss: 2.1040 - val_auc: 0.9474 - val_accuracy: 0.4262\n",
      "Epoch 183/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1101 - auc: 0.9475 - accuracy: 0.4199 - val_loss: 2.1024 - val_auc: 0.9475 - val_accuracy: 0.4245\n",
      "Epoch 184/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1112 - auc: 0.9475 - accuracy: 0.4202 - val_loss: 2.1007 - val_auc: 0.9477 - val_accuracy: 0.4246\n",
      "Epoch 185/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1073 - auc: 0.9478 - accuracy: 0.4218 - val_loss: 2.1004 - val_auc: 0.9475 - val_accuracy: 0.4240\n",
      "Epoch 186/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1070 - auc: 0.9476 - accuracy: 0.4228 - val_loss: 2.0982 - val_auc: 0.9478 - val_accuracy: 0.4254\n",
      "Epoch 187/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1063 - auc: 0.9479 - accuracy: 0.4226 - val_loss: 2.0982 - val_auc: 0.9479 - val_accuracy: 0.4249\n",
      "Epoch 188/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1063 - auc: 0.9478 - accuracy: 0.4246 - val_loss: 2.0957 - val_auc: 0.9479 - val_accuracy: 0.4237\n",
      "Epoch 189/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1053 - auc: 0.9478 - accuracy: 0.4202 - val_loss: 2.0955 - val_auc: 0.9480 - val_accuracy: 0.4253\n",
      "Epoch 190/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1025 - auc: 0.9481 - accuracy: 0.4213 - val_loss: 2.0936 - val_auc: 0.9480 - val_accuracy: 0.4237\n",
      "Epoch 191/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0989 - auc: 0.9480 - accuracy: 0.4216 - val_loss: 2.0928 - val_auc: 0.9481 - val_accuracy: 0.4271\n",
      "Epoch 192/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0981 - auc: 0.9483 - accuracy: 0.4224 - val_loss: 2.0943 - val_auc: 0.9479 - val_accuracy: 0.4262\n",
      "Epoch 193/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0974 - auc: 0.9483 - accuracy: 0.4214 - val_loss: 2.0899 - val_auc: 0.9481 - val_accuracy: 0.4245\n",
      "Epoch 194/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0966 - auc: 0.9485 - accuracy: 0.4227 - val_loss: 2.0889 - val_auc: 0.9486 - val_accuracy: 0.4270\n",
      "Epoch 195/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0967 - auc: 0.9487 - accuracy: 0.4232 - val_loss: 2.0884 - val_auc: 0.9482 - val_accuracy: 0.4267\n",
      "Epoch 196/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0968 - auc: 0.9484 - accuracy: 0.4211 - val_loss: 2.0879 - val_auc: 0.9486 - val_accuracy: 0.4274\n",
      "Epoch 197/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0926 - auc: 0.9489 - accuracy: 0.4233 - val_loss: 2.0861 - val_auc: 0.9485 - val_accuracy: 0.4266\n",
      "Epoch 198/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0918 - auc: 0.9486 - accuracy: 0.4233 - val_loss: 2.0854 - val_auc: 0.9483 - val_accuracy: 0.4262\n",
      "Epoch 199/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0891 - auc: 0.9485 - accuracy: 0.4238 - val_loss: 2.0848 - val_auc: 0.9484 - val_accuracy: 0.4263\n",
      "Epoch 200/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0908 - auc: 0.9485 - accuracy: 0.4236 - val_loss: 2.0821 - val_auc: 0.9486 - val_accuracy: 0.4261\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(learning_rate = .003), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['AUC','accuracy'])\n",
    "run_hist_1 = model_1.fit(nmf_features_1, y_train, \n",
    "                         validation_data=(nmf_test_features_1, y_test), \n",
    "                         batch_size = 30,\n",
    "                         epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "marine-sierra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "plt.title('Model 1')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-practitioner",
   "metadata": {},
   "source": [
    "Here, we can see that the both model train loss and validation loss have plataeued, indicating either we need to increase the epochs, learning rate or otherwise we have reached the optimal loss level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-summary",
   "metadata": {},
   "source": [
    "#### Model 2 (10 Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "romance-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(1000,input_shape = (10,),activation = 'relu'))\n",
    "model_2.add(Dropout(0.25, seed = 1000))\n",
    "model_2.add(Dense(1000,activation = 'relu'))\n",
    "model_2.add(Dropout(0.25, seed = 500))\n",
    "model_2.add(Dense(70,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "interpreted-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 1000)              11000     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 70)                70070     \n",
      "=================================================================\n",
      "Total params: 1,082,070\n",
      "Trainable params: 1,082,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "common-subject",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "597/597 [==============================] - 5s 7ms/step - loss: 4.1804 - auc: 0.6530 - accuracy: 0.1416 - val_loss: 4.1068 - val_auc: 0.7364 - val_accuracy: 0.1391\n",
      "Epoch 2/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 4.0254 - auc: 0.7473 - accuracy: 0.1392 - val_loss: 3.9317 - val_auc: 0.7519 - val_accuracy: 0.1391\n",
      "Epoch 3/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.8325 - auc: 0.7621 - accuracy: 0.1392 - val_loss: 3.7306 - val_auc: 0.7859 - val_accuracy: 0.1391\n",
      "Epoch 4/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.6657 - auc: 0.8028 - accuracy: 0.1392 - val_loss: 3.6060 - val_auc: 0.8089 - val_accuracy: 0.1391\n",
      "Epoch 5/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.5730 - auc: 0.8039 - accuracy: 0.1396 - val_loss: 3.5358 - val_auc: 0.8015 - val_accuracy: 0.1391\n",
      "Epoch 6/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.5179 - auc: 0.8095 - accuracy: 0.1413 - val_loss: 3.4926 - val_auc: 0.8207 - val_accuracy: 0.1391\n",
      "Epoch 7/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.4852 - auc: 0.8197 - accuracy: 0.1434 - val_loss: 3.4661 - val_auc: 0.8252 - val_accuracy: 0.1391\n",
      "Epoch 8/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.4631 - auc: 0.8233 - accuracy: 0.1456 - val_loss: 3.4482 - val_auc: 0.8242 - val_accuracy: 0.1405\n",
      "Epoch 9/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.4481 - auc: 0.8240 - accuracy: 0.1502 - val_loss: 3.4349 - val_auc: 0.8252 - val_accuracy: 0.1400\n",
      "Epoch 10/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.4358 - auc: 0.8246 - accuracy: 0.1544 - val_loss: 3.4235 - val_auc: 0.8272 - val_accuracy: 0.1416\n",
      "Epoch 11/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.4262 - auc: 0.8247 - accuracy: 0.1587 - val_loss: 3.4133 - val_auc: 0.8275 - val_accuracy: 0.1400\n",
      "Epoch 12/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.4171 - auc: 0.8258 - accuracy: 0.1592 - val_loss: 3.4036 - val_auc: 0.8290 - val_accuracy: 0.1424\n",
      "Epoch 13/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.4082 - auc: 0.8265 - accuracy: 0.1639 - val_loss: 3.3939 - val_auc: 0.8299 - val_accuracy: 0.1577\n",
      "Epoch 14/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.3970 - auc: 0.8272 - accuracy: 0.1731 - val_loss: 3.3840 - val_auc: 0.8298 - val_accuracy: 0.1631\n",
      "Epoch 15/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.3892 - auc: 0.8277 - accuracy: 0.1760 - val_loss: 3.3738 - val_auc: 0.8308 - val_accuracy: 0.1688\n",
      "Epoch 16/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.3761 - auc: 0.8291 - accuracy: 0.1825 - val_loss: 3.3626 - val_auc: 0.8309 - val_accuracy: 0.1828\n",
      "Epoch 17/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 3.3645 - auc: 0.8294 - accuracy: 0.1921 - val_loss: 3.3507 - val_auc: 0.8323 - val_accuracy: 0.1884\n",
      "Epoch 18/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.3541 - auc: 0.8301 - accuracy: 0.1951 - val_loss: 3.3374 - val_auc: 0.8326 - val_accuracy: 0.1933\n",
      "Epoch 19/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.3402 - auc: 0.8314 - accuracy: 0.2031 - val_loss: 3.3231 - val_auc: 0.8334 - val_accuracy: 0.2162\n",
      "Epoch 20/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.3263 - auc: 0.8319 - accuracy: 0.2090 - val_loss: 3.3074 - val_auc: 0.8352 - val_accuracy: 0.2231\n",
      "Epoch 21/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.3105 - auc: 0.8331 - accuracy: 0.2142 - val_loss: 3.2903 - val_auc: 0.8363 - val_accuracy: 0.2347\n",
      "Epoch 22/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.2932 - auc: 0.8341 - accuracy: 0.2222 - val_loss: 3.2715 - val_auc: 0.8368 - val_accuracy: 0.2380\n",
      "Epoch 23/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.2718 - auc: 0.8355 - accuracy: 0.2313 - val_loss: 3.2516 - val_auc: 0.8378 - val_accuracy: 0.2332\n",
      "Epoch 24/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.2500 - auc: 0.8369 - accuracy: 0.2361 - val_loss: 3.2296 - val_auc: 0.8387 - val_accuracy: 0.2487\n",
      "Epoch 25/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 3.2305 - auc: 0.8381 - accuracy: 0.2439 - val_loss: 3.2069 - val_auc: 0.8402 - val_accuracy: 0.2552\n",
      "Epoch 26/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.2072 - auc: 0.8396 - accuracy: 0.2510 - val_loss: 3.1829 - val_auc: 0.8419 - val_accuracy: 0.2621\n",
      "Epoch 27/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.1826 - auc: 0.8418 - accuracy: 0.2580 - val_loss: 3.1586 - val_auc: 0.8432 - val_accuracy: 0.2689\n",
      "Epoch 28/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.1599 - auc: 0.8431 - accuracy: 0.2670 - val_loss: 3.1340 - val_auc: 0.8459 - val_accuracy: 0.2685\n",
      "Epoch 29/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.1372 - auc: 0.8451 - accuracy: 0.2736 - val_loss: 3.1093 - val_auc: 0.8475 - val_accuracy: 0.2789\n",
      "Epoch 30/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.1093 - auc: 0.8470 - accuracy: 0.2841 - val_loss: 3.0846 - val_auc: 0.8502 - val_accuracy: 0.2962\n",
      "Epoch 31/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.0843 - auc: 0.8489 - accuracy: 0.2966 - val_loss: 3.0604 - val_auc: 0.8519 - val_accuracy: 0.3065\n",
      "Epoch 32/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 3.0639 - auc: 0.8512 - accuracy: 0.3046 - val_loss: 3.0363 - val_auc: 0.8539 - val_accuracy: 0.3204\n",
      "Epoch 33/200\n",
      "597/597 [==============================] - 4s 6ms/step - loss: 3.0386 - auc: 0.8535 - accuracy: 0.3156 - val_loss: 3.0132 - val_auc: 0.8556 - val_accuracy: 0.3264\n",
      "Epoch 34/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 3.0160 - auc: 0.8551 - accuracy: 0.3244 - val_loss: 2.9904 - val_auc: 0.8573 - val_accuracy: 0.3451\n",
      "Epoch 35/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9906 - auc: 0.8579 - accuracy: 0.3319 - val_loss: 2.9681 - val_auc: 0.8588 - val_accuracy: 0.3606\n",
      "Epoch 36/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9686 - auc: 0.8597 - accuracy: 0.3443 - val_loss: 2.9455 - val_auc: 0.8613 - val_accuracy: 0.3587\n",
      "Epoch 37/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9481 - auc: 0.8619 - accuracy: 0.3470 - val_loss: 2.9239 - val_auc: 0.8638 - val_accuracy: 0.3638\n",
      "Epoch 38/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9250 - auc: 0.8645 - accuracy: 0.3569 - val_loss: 2.9019 - val_auc: 0.8656 - val_accuracy: 0.3703\n",
      "Epoch 39/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.9057 - auc: 0.8660 - accuracy: 0.3618 - val_loss: 2.8805 - val_auc: 0.8680 - val_accuracy: 0.3727\n",
      "Epoch 40/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.8816 - auc: 0.8686 - accuracy: 0.3656 - val_loss: 2.8595 - val_auc: 0.8703 - val_accuracy: 0.3741\n",
      "Epoch 41/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.8608 - auc: 0.8705 - accuracy: 0.3714 - val_loss: 2.8379 - val_auc: 0.8726 - val_accuracy: 0.3766\n",
      "Epoch 42/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.8402 - auc: 0.8726 - accuracy: 0.3752 - val_loss: 2.8172 - val_auc: 0.8747 - val_accuracy: 0.3865\n",
      "Epoch 43/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.8188 - auc: 0.8746 - accuracy: 0.3827 - val_loss: 2.7963 - val_auc: 0.8763 - val_accuracy: 0.3917\n",
      "Epoch 44/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7960 - auc: 0.8765 - accuracy: 0.3868 - val_loss: 2.7755 - val_auc: 0.8779 - val_accuracy: 0.3929\n",
      "Epoch 45/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7756 - auc: 0.8792 - accuracy: 0.3907 - val_loss: 2.7553 - val_auc: 0.8801 - val_accuracy: 0.4132\n",
      "Epoch 46/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7550 - auc: 0.8805 - accuracy: 0.3972 - val_loss: 2.7350 - val_auc: 0.8831 - val_accuracy: 0.4215\n",
      "Epoch 47/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7369 - auc: 0.8829 - accuracy: 0.4016 - val_loss: 2.7151 - val_auc: 0.8847 - val_accuracy: 0.4200\n",
      "Epoch 48/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.7167 - auc: 0.8844 - accuracy: 0.4079 - val_loss: 2.6951 - val_auc: 0.8867 - val_accuracy: 0.4226\n",
      "Epoch 49/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6968 - auc: 0.8863 - accuracy: 0.4099 - val_loss: 2.6751 - val_auc: 0.8884 - val_accuracy: 0.4263\n",
      "Epoch 50/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6740 - auc: 0.8889 - accuracy: 0.4189 - val_loss: 2.6557 - val_auc: 0.8902 - val_accuracy: 0.4284\n",
      "Epoch 51/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6565 - auc: 0.8902 - accuracy: 0.4195 - val_loss: 2.6364 - val_auc: 0.8920 - val_accuracy: 0.4304\n",
      "Epoch 52/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6352 - auc: 0.8926 - accuracy: 0.4235 - val_loss: 2.6178 - val_auc: 0.8936 - val_accuracy: 0.4321\n",
      "Epoch 53/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.6192 - auc: 0.8936 - accuracy: 0.4251 - val_loss: 2.5992 - val_auc: 0.8955 - val_accuracy: 0.4348\n",
      "Epoch 54/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5998 - auc: 0.8959 - accuracy: 0.4296 - val_loss: 2.5810 - val_auc: 0.8971 - val_accuracy: 0.4357\n",
      "Epoch 55/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5834 - auc: 0.8975 - accuracy: 0.4326 - val_loss: 2.5632 - val_auc: 0.8985 - val_accuracy: 0.4355\n",
      "Epoch 56/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5650 - auc: 0.8991 - accuracy: 0.4333 - val_loss: 2.5458 - val_auc: 0.9005 - val_accuracy: 0.4379\n",
      "Epoch 57/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5470 - auc: 0.9007 - accuracy: 0.4361 - val_loss: 2.5290 - val_auc: 0.9019 - val_accuracy: 0.4394\n",
      "Epoch 58/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5298 - auc: 0.9023 - accuracy: 0.4381 - val_loss: 2.5117 - val_auc: 0.9037 - val_accuracy: 0.4416\n",
      "Epoch 59/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.5136 - auc: 0.9040 - accuracy: 0.4378 - val_loss: 2.4952 - val_auc: 0.9051 - val_accuracy: 0.4434\n",
      "Epoch 60/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4968 - auc: 0.9050 - accuracy: 0.4400 - val_loss: 2.4794 - val_auc: 0.9068 - val_accuracy: 0.4442\n",
      "Epoch 61/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4813 - auc: 0.9064 - accuracy: 0.4410 - val_loss: 2.4642 - val_auc: 0.9078 - val_accuracy: 0.4434\n",
      "Epoch 62/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4659 - auc: 0.9084 - accuracy: 0.4406 - val_loss: 2.4486 - val_auc: 0.9093 - val_accuracy: 0.4444\n",
      "Epoch 63/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4483 - auc: 0.9095 - accuracy: 0.4420 - val_loss: 2.4345 - val_auc: 0.9106 - val_accuracy: 0.4444\n",
      "Epoch 64/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4360 - auc: 0.9108 - accuracy: 0.4415 - val_loss: 2.4189 - val_auc: 0.9119 - val_accuracy: 0.4467\n",
      "Epoch 65/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4210 - auc: 0.9121 - accuracy: 0.4430 - val_loss: 2.4038 - val_auc: 0.9132 - val_accuracy: 0.4476\n",
      "Epoch 66/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.4053 - auc: 0.9135 - accuracy: 0.4458 - val_loss: 2.3899 - val_auc: 0.9142 - val_accuracy: 0.4478\n",
      "Epoch 67/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3926 - auc: 0.9147 - accuracy: 0.4470 - val_loss: 2.3762 - val_auc: 0.9153 - val_accuracy: 0.4489\n",
      "Epoch 68/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3787 - auc: 0.9163 - accuracy: 0.4478 - val_loss: 2.3624 - val_auc: 0.9164 - val_accuracy: 0.4487\n",
      "Epoch 69/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3661 - auc: 0.9172 - accuracy: 0.4474 - val_loss: 2.3487 - val_auc: 0.9178 - val_accuracy: 0.4489\n",
      "Epoch 70/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3502 - auc: 0.9185 - accuracy: 0.4492 - val_loss: 2.3363 - val_auc: 0.9189 - val_accuracy: 0.4494\n",
      "Epoch 71/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3387 - auc: 0.9196 - accuracy: 0.4505 - val_loss: 2.3235 - val_auc: 0.9201 - val_accuracy: 0.4507\n",
      "Epoch 72/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3253 - auc: 0.9208 - accuracy: 0.4529 - val_loss: 2.3105 - val_auc: 0.9219 - val_accuracy: 0.4515\n",
      "Epoch 73/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3164 - auc: 0.9219 - accuracy: 0.4516 - val_loss: 2.2991 - val_auc: 0.9227 - val_accuracy: 0.4499\n",
      "Epoch 74/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.3013 - auc: 0.9236 - accuracy: 0.4545 - val_loss: 2.2885 - val_auc: 0.9240 - val_accuracy: 0.4508\n",
      "Epoch 75/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2889 - auc: 0.9240 - accuracy: 0.4547 - val_loss: 2.2741 - val_auc: 0.9252 - val_accuracy: 0.4533\n",
      "Epoch 76/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2785 - auc: 0.9257 - accuracy: 0.4554 - val_loss: 2.2632 - val_auc: 0.9262 - val_accuracy: 0.4521\n",
      "Epoch 77/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2685 - auc: 0.9263 - accuracy: 0.4566 - val_loss: 2.2520 - val_auc: 0.9273 - val_accuracy: 0.4534\n",
      "Epoch 78/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2551 - auc: 0.9276 - accuracy: 0.4588 - val_loss: 2.2410 - val_auc: 0.9281 - val_accuracy: 0.4533\n",
      "Epoch 79/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2447 - auc: 0.9282 - accuracy: 0.4584 - val_loss: 2.2303 - val_auc: 0.9293 - val_accuracy: 0.4558\n",
      "Epoch 80/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2327 - auc: 0.9293 - accuracy: 0.4618 - val_loss: 2.2204 - val_auc: 0.9300 - val_accuracy: 0.4567\n",
      "Epoch 81/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2245 - auc: 0.9297 - accuracy: 0.4612 - val_loss: 2.2099 - val_auc: 0.9310 - val_accuracy: 0.4622\n",
      "Epoch 82/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2139 - auc: 0.9310 - accuracy: 0.4610 - val_loss: 2.1999 - val_auc: 0.9315 - val_accuracy: 0.4639\n",
      "Epoch 83/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.2050 - auc: 0.9317 - accuracy: 0.4623 - val_loss: 2.1902 - val_auc: 0.9322 - val_accuracy: 0.4648\n",
      "Epoch 84/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1947 - auc: 0.9329 - accuracy: 0.4657 - val_loss: 2.1823 - val_auc: 0.9329 - val_accuracy: 0.4661\n",
      "Epoch 85/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1866 - auc: 0.9331 - accuracy: 0.4657 - val_loss: 2.1722 - val_auc: 0.9335 - val_accuracy: 0.4662\n",
      "Epoch 86/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1786 - auc: 0.9338 - accuracy: 0.4658 - val_loss: 2.1633 - val_auc: 0.9346 - val_accuracy: 0.4675\n",
      "Epoch 87/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1681 - auc: 0.9349 - accuracy: 0.4683 - val_loss: 2.1543 - val_auc: 0.9351 - val_accuracy: 0.4678\n",
      "Epoch 88/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1576 - auc: 0.9356 - accuracy: 0.4696 - val_loss: 2.1465 - val_auc: 0.9360 - val_accuracy: 0.4680\n",
      "Epoch 89/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.1506 - auc: 0.9362 - accuracy: 0.4702 - val_loss: 2.1391 - val_auc: 0.9368 - val_accuracy: 0.4695\n",
      "Epoch 90/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 2.1421 - auc: 0.9366 - accuracy: 0.4687 - val_loss: 2.1299 - val_auc: 0.9374 - val_accuracy: 0.4682\n",
      "Epoch 91/200\n",
      "597/597 [==============================] - 6s 10ms/step - loss: 2.1356 - auc: 0.9373 - accuracy: 0.4731 - val_loss: 2.1237 - val_auc: 0.9378 - val_accuracy: 0.4713\n",
      "Epoch 92/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.1272 - auc: 0.9381 - accuracy: 0.4731 - val_loss: 2.1145 - val_auc: 0.9386 - val_accuracy: 0.4713\n",
      "Epoch 93/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.1209 - auc: 0.9381 - accuracy: 0.4731 - val_loss: 2.1079 - val_auc: 0.9393 - val_accuracy: 0.4711\n",
      "Epoch 94/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.1133 - auc: 0.9389 - accuracy: 0.4743 - val_loss: 2.1007 - val_auc: 0.9399 - val_accuracy: 0.4706\n",
      "Epoch 95/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.1057 - auc: 0.9397 - accuracy: 0.4758 - val_loss: 2.0932 - val_auc: 0.9405 - val_accuracy: 0.4714\n",
      "Epoch 96/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.0974 - auc: 0.9407 - accuracy: 0.4737 - val_loss: 2.0863 - val_auc: 0.9411 - val_accuracy: 0.4726\n",
      "Epoch 97/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0913 - auc: 0.9404 - accuracy: 0.4770 - val_loss: 2.0798 - val_auc: 0.9415 - val_accuracy: 0.4738\n",
      "Epoch 98/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.0841 - auc: 0.9413 - accuracy: 0.4772 - val_loss: 2.0742 - val_auc: 0.9422 - val_accuracy: 0.4741\n",
      "Epoch 99/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.0772 - auc: 0.9416 - accuracy: 0.4777 - val_loss: 2.0682 - val_auc: 0.9425 - val_accuracy: 0.4732\n",
      "Epoch 100/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 2.0719 - auc: 0.9424 - accuracy: 0.4780 - val_loss: 2.0613 - val_auc: 0.9433 - val_accuracy: 0.4757\n",
      "Epoch 101/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 2.0650 - auc: 0.9431 - accuracy: 0.4778 - val_loss: 2.0555 - val_auc: 0.9436 - val_accuracy: 0.4771\n",
      "Epoch 102/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0600 - auc: 0.9430 - accuracy: 0.4806 - val_loss: 2.0501 - val_auc: 0.9441 - val_accuracy: 0.4757\n",
      "Epoch 103/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0535 - auc: 0.9436 - accuracy: 0.4786 - val_loss: 2.0442 - val_auc: 0.9445 - val_accuracy: 0.4771\n",
      "Epoch 104/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0473 - auc: 0.9446 - accuracy: 0.4803 - val_loss: 2.0388 - val_auc: 0.9447 - val_accuracy: 0.4777\n",
      "Epoch 105/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0452 - auc: 0.9445 - accuracy: 0.4812 - val_loss: 2.0337 - val_auc: 0.9452 - val_accuracy: 0.4769\n",
      "Epoch 106/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0389 - auc: 0.9447 - accuracy: 0.4805 - val_loss: 2.0287 - val_auc: 0.9457 - val_accuracy: 0.4773\n",
      "Epoch 107/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0328 - auc: 0.9453 - accuracy: 0.4836 - val_loss: 2.0234 - val_auc: 0.9459 - val_accuracy: 0.4782\n",
      "Epoch 108/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0240 - auc: 0.9458 - accuracy: 0.4829 - val_loss: 2.0186 - val_auc: 0.9462 - val_accuracy: 0.4797\n",
      "Epoch 109/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0226 - auc: 0.9464 - accuracy: 0.4820 - val_loss: 2.0141 - val_auc: 0.9467 - val_accuracy: 0.4782\n",
      "Epoch 110/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0172 - auc: 0.9467 - accuracy: 0.4837 - val_loss: 2.0087 - val_auc: 0.9470 - val_accuracy: 0.4812\n",
      "Epoch 111/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0133 - auc: 0.9466 - accuracy: 0.4844 - val_loss: 2.0051 - val_auc: 0.9475 - val_accuracy: 0.4827\n",
      "Epoch 112/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0082 - auc: 0.9476 - accuracy: 0.4847 - val_loss: 2.0006 - val_auc: 0.9476 - val_accuracy: 0.4827\n",
      "Epoch 113/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 2.0044 - auc: 0.9481 - accuracy: 0.4845 - val_loss: 1.9961 - val_auc: 0.9479 - val_accuracy: 0.4843\n",
      "Epoch 114/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9998 - auc: 0.9478 - accuracy: 0.4862 - val_loss: 1.9919 - val_auc: 0.9488 - val_accuracy: 0.4827\n",
      "Epoch 115/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9983 - auc: 0.9480 - accuracy: 0.4858 - val_loss: 1.9889 - val_auc: 0.9492 - val_accuracy: 0.4797\n",
      "Epoch 116/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9907 - auc: 0.9485 - accuracy: 0.4857 - val_loss: 1.9841 - val_auc: 0.9496 - val_accuracy: 0.4818\n",
      "Epoch 117/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9877 - auc: 0.9484 - accuracy: 0.4864 - val_loss: 1.9802 - val_auc: 0.9499 - val_accuracy: 0.4839\n",
      "Epoch 118/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9832 - auc: 0.9493 - accuracy: 0.4875 - val_loss: 1.9757 - val_auc: 0.9503 - val_accuracy: 0.4843\n",
      "Epoch 119/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9789 - auc: 0.9494 - accuracy: 0.4864 - val_loss: 1.9728 - val_auc: 0.9507 - val_accuracy: 0.4848\n",
      "Epoch 120/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9757 - auc: 0.9499 - accuracy: 0.4885 - val_loss: 1.9682 - val_auc: 0.9508 - val_accuracy: 0.4857\n",
      "Epoch 121/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9727 - auc: 0.9502 - accuracy: 0.4907 - val_loss: 1.9647 - val_auc: 0.9512 - val_accuracy: 0.4859\n",
      "Epoch 122/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9685 - auc: 0.9504 - accuracy: 0.4885 - val_loss: 1.9617 - val_auc: 0.9512 - val_accuracy: 0.4851\n",
      "Epoch 123/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9658 - auc: 0.9508 - accuracy: 0.4883 - val_loss: 1.9583 - val_auc: 0.9514 - val_accuracy: 0.4863\n",
      "Epoch 124/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9617 - auc: 0.9509 - accuracy: 0.4890 - val_loss: 1.9546 - val_auc: 0.9520 - val_accuracy: 0.4861\n",
      "Epoch 125/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9570 - auc: 0.9513 - accuracy: 0.4915 - val_loss: 1.9518 - val_auc: 0.9524 - val_accuracy: 0.4865\n",
      "Epoch 126/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9545 - auc: 0.9515 - accuracy: 0.4897 - val_loss: 1.9499 - val_auc: 0.9523 - val_accuracy: 0.4852\n",
      "Epoch 127/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 1.9514 - auc: 0.9515 - accuracy: 0.4905 - val_loss: 1.9459 - val_auc: 0.9528 - val_accuracy: 0.4864\n",
      "Epoch 128/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.9472 - auc: 0.9519 - accuracy: 0.4901 - val_loss: 1.9442 - val_auc: 0.9530 - val_accuracy: 0.4866\n",
      "Epoch 129/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9452 - auc: 0.9522 - accuracy: 0.4907 - val_loss: 1.9390 - val_auc: 0.9534 - val_accuracy: 0.4891\n",
      "Epoch 130/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9420 - auc: 0.9525 - accuracy: 0.4921 - val_loss: 1.9367 - val_auc: 0.9535 - val_accuracy: 0.4876\n",
      "Epoch 131/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9389 - auc: 0.9526 - accuracy: 0.4922 - val_loss: 1.9350 - val_auc: 0.9537 - val_accuracy: 0.4877\n",
      "Epoch 132/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9373 - auc: 0.9529 - accuracy: 0.4903 - val_loss: 1.9304 - val_auc: 0.9539 - val_accuracy: 0.4891\n",
      "Epoch 133/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9332 - auc: 0.9527 - accuracy: 0.4937 - val_loss: 1.9281 - val_auc: 0.9540 - val_accuracy: 0.4878\n",
      "Epoch 134/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9308 - auc: 0.9529 - accuracy: 0.4922 - val_loss: 1.9260 - val_auc: 0.9543 - val_accuracy: 0.4878\n",
      "Epoch 135/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9284 - auc: 0.9534 - accuracy: 0.4909 - val_loss: 1.9223 - val_auc: 0.9545 - val_accuracy: 0.4889\n",
      "Epoch 136/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9246 - auc: 0.9536 - accuracy: 0.4942 - val_loss: 1.9209 - val_auc: 0.9546 - val_accuracy: 0.4882\n",
      "Epoch 137/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9214 - auc: 0.9537 - accuracy: 0.4926 - val_loss: 1.9176 - val_auc: 0.9548 - val_accuracy: 0.4883\n",
      "Epoch 138/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9182 - auc: 0.9540 - accuracy: 0.4948 - val_loss: 1.9151 - val_auc: 0.9547 - val_accuracy: 0.4900\n",
      "Epoch 139/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9168 - auc: 0.9538 - accuracy: 0.4954 - val_loss: 1.9117 - val_auc: 0.9549 - val_accuracy: 0.4912\n",
      "Epoch 140/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9122 - auc: 0.9543 - accuracy: 0.4941 - val_loss: 1.9094 - val_auc: 0.9551 - val_accuracy: 0.4909\n",
      "Epoch 141/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9096 - auc: 0.9542 - accuracy: 0.4952 - val_loss: 1.9070 - val_auc: 0.9552 - val_accuracy: 0.4908\n",
      "Epoch 142/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.9095 - auc: 0.9543 - accuracy: 0.4937 - val_loss: 1.9059 - val_auc: 0.9553 - val_accuracy: 0.4893\n",
      "Epoch 143/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.9060 - auc: 0.9544 - accuracy: 0.4965 - val_loss: 1.9025 - val_auc: 0.9554 - val_accuracy: 0.4909\n",
      "Epoch 144/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9055 - auc: 0.9551 - accuracy: 0.4944 - val_loss: 1.9010 - val_auc: 0.9556 - val_accuracy: 0.4902\n",
      "Epoch 145/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.9008 - auc: 0.9550 - accuracy: 0.4951 - val_loss: 1.8976 - val_auc: 0.9558 - val_accuracy: 0.4915\n",
      "Epoch 146/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8959 - auc: 0.9554 - accuracy: 0.4941 - val_loss: 1.8954 - val_auc: 0.9558 - val_accuracy: 0.4908\n",
      "Epoch 147/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8963 - auc: 0.9554 - accuracy: 0.4944 - val_loss: 1.8931 - val_auc: 0.9561 - val_accuracy: 0.4913\n",
      "Epoch 148/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 1.8945 - auc: 0.9554 - accuracy: 0.4961 - val_loss: 1.8910 - val_auc: 0.9563 - val_accuracy: 0.4926\n",
      "Epoch 149/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8919 - auc: 0.9554 - accuracy: 0.4956 - val_loss: 1.8894 - val_auc: 0.9565 - val_accuracy: 0.4920\n",
      "Epoch 150/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8888 - auc: 0.9557 - accuracy: 0.4973 - val_loss: 1.8876 - val_auc: 0.9566 - val_accuracy: 0.4921\n",
      "Epoch 151/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8866 - auc: 0.9557 - accuracy: 0.4955 - val_loss: 1.8854 - val_auc: 0.9565 - val_accuracy: 0.4925\n",
      "Epoch 152/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8824 - auc: 0.9562 - accuracy: 0.4964 - val_loss: 1.8839 - val_auc: 0.9565 - val_accuracy: 0.4911\n",
      "Epoch 153/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8825 - auc: 0.9560 - accuracy: 0.4970 - val_loss: 1.8819 - val_auc: 0.9565 - val_accuracy: 0.4928\n",
      "Epoch 154/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8824 - auc: 0.9559 - accuracy: 0.4968 - val_loss: 1.8794 - val_auc: 0.9570 - val_accuracy: 0.4924\n",
      "Epoch 155/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8772 - auc: 0.9563 - accuracy: 0.4978 - val_loss: 1.8775 - val_auc: 0.9571 - val_accuracy: 0.4920\n",
      "Epoch 156/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8767 - auc: 0.9566 - accuracy: 0.4985 - val_loss: 1.8757 - val_auc: 0.9570 - val_accuracy: 0.4928\n",
      "Epoch 157/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8751 - auc: 0.9564 - accuracy: 0.4982 - val_loss: 1.8733 - val_auc: 0.9572 - val_accuracy: 0.4921\n",
      "Epoch 158/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8717 - auc: 0.9568 - accuracy: 0.4996 - val_loss: 1.8731 - val_auc: 0.9572 - val_accuracy: 0.4920\n",
      "Epoch 159/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 1.8711 - auc: 0.9568 - accuracy: 0.4980 - val_loss: 1.8701 - val_auc: 0.9575 - val_accuracy: 0.4924\n",
      "Epoch 160/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.8680 - auc: 0.9572 - accuracy: 0.4999 - val_loss: 1.8697 - val_auc: 0.9576 - val_accuracy: 0.4920\n",
      "Epoch 161/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8708 - auc: 0.9569 - accuracy: 0.4983 - val_loss: 1.8669 - val_auc: 0.9574 - val_accuracy: 0.4929\n",
      "Epoch 162/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8672 - auc: 0.9568 - accuracy: 0.4994 - val_loss: 1.8659 - val_auc: 0.9576 - val_accuracy: 0.4943\n",
      "Epoch 163/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8636 - auc: 0.9570 - accuracy: 0.4988 - val_loss: 1.8638 - val_auc: 0.9578 - val_accuracy: 0.4934\n",
      "Epoch 164/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8623 - auc: 0.9573 - accuracy: 0.4987 - val_loss: 1.8611 - val_auc: 0.9580 - val_accuracy: 0.4928\n",
      "Epoch 165/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8604 - auc: 0.9573 - accuracy: 0.4986 - val_loss: 1.8608 - val_auc: 0.9578 - val_accuracy: 0.4930\n",
      "Epoch 166/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8603 - auc: 0.9575 - accuracy: 0.4996 - val_loss: 1.8585 - val_auc: 0.9581 - val_accuracy: 0.4936\n",
      "Epoch 167/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8590 - auc: 0.9572 - accuracy: 0.4991 - val_loss: 1.8572 - val_auc: 0.9580 - val_accuracy: 0.4936\n",
      "Epoch 168/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.8551 - auc: 0.9578 - accuracy: 0.4980 - val_loss: 1.8554 - val_auc: 0.9580 - val_accuracy: 0.4939\n",
      "Epoch 169/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8543 - auc: 0.9577 - accuracy: 0.4988 - val_loss: 1.8533 - val_auc: 0.9583 - val_accuracy: 0.4939\n",
      "Epoch 170/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8533 - auc: 0.9577 - accuracy: 0.4976 - val_loss: 1.8524 - val_auc: 0.9584 - val_accuracy: 0.4937\n",
      "Epoch 171/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8529 - auc: 0.9578 - accuracy: 0.4988 - val_loss: 1.8511 - val_auc: 0.9585 - val_accuracy: 0.4929\n",
      "Epoch 172/200\n",
      "597/597 [==============================] - 5s 9ms/step - loss: 1.8518 - auc: 0.9580 - accuracy: 0.5004 - val_loss: 1.8494 - val_auc: 0.9585 - val_accuracy: 0.4943\n",
      "Epoch 173/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8465 - auc: 0.9578 - accuracy: 0.4992 - val_loss: 1.8486 - val_auc: 0.9585 - val_accuracy: 0.4952\n",
      "Epoch 174/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8482 - auc: 0.9579 - accuracy: 0.5002 - val_loss: 1.8478 - val_auc: 0.9584 - val_accuracy: 0.4942\n",
      "Epoch 175/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8437 - auc: 0.9583 - accuracy: 0.5013 - val_loss: 1.8450 - val_auc: 0.9589 - val_accuracy: 0.4937\n",
      "Epoch 176/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8440 - auc: 0.9582 - accuracy: 0.5001 - val_loss: 1.8445 - val_auc: 0.9588 - val_accuracy: 0.4954\n",
      "Epoch 177/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8415 - auc: 0.9584 - accuracy: 0.5016 - val_loss: 1.8422 - val_auc: 0.9592 - val_accuracy: 0.4963\n",
      "Epoch 178/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8418 - auc: 0.9585 - accuracy: 0.4994 - val_loss: 1.8409 - val_auc: 0.9593 - val_accuracy: 0.4951\n",
      "Epoch 179/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8389 - auc: 0.9582 - accuracy: 0.5001 - val_loss: 1.8396 - val_auc: 0.9594 - val_accuracy: 0.4947\n",
      "Epoch 180/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8359 - auc: 0.9588 - accuracy: 0.4997 - val_loss: 1.8390 - val_auc: 0.9593 - val_accuracy: 0.4950\n",
      "Epoch 181/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8368 - auc: 0.9588 - accuracy: 0.5015 - val_loss: 1.8369 - val_auc: 0.9594 - val_accuracy: 0.4952\n",
      "Epoch 182/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.8349 - auc: 0.9588 - accuracy: 0.5011 - val_loss: 1.8360 - val_auc: 0.9596 - val_accuracy: 0.4952\n",
      "Epoch 183/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.8317 - auc: 0.9591 - accuracy: 0.5006 - val_loss: 1.8351 - val_auc: 0.9596 - val_accuracy: 0.4959\n",
      "Epoch 184/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8321 - auc: 0.9593 - accuracy: 0.5004 - val_loss: 1.8347 - val_auc: 0.9595 - val_accuracy: 0.4952\n",
      "Epoch 185/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8332 - auc: 0.9590 - accuracy: 0.5012 - val_loss: 1.8319 - val_auc: 0.9598 - val_accuracy: 0.4954\n",
      "Epoch 186/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8309 - auc: 0.9590 - accuracy: 0.5010 - val_loss: 1.8312 - val_auc: 0.9597 - val_accuracy: 0.4958\n",
      "Epoch 187/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8288 - auc: 0.9592 - accuracy: 0.5013 - val_loss: 1.8297 - val_auc: 0.9598 - val_accuracy: 0.4960\n",
      "Epoch 188/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8264 - auc: 0.9591 - accuracy: 0.5020 - val_loss: 1.8284 - val_auc: 0.9598 - val_accuracy: 0.4964\n",
      "Epoch 189/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8256 - auc: 0.9595 - accuracy: 0.5017 - val_loss: 1.8272 - val_auc: 0.9600 - val_accuracy: 0.4972\n",
      "Epoch 190/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8220 - auc: 0.9596 - accuracy: 0.5034 - val_loss: 1.8252 - val_auc: 0.9599 - val_accuracy: 0.4951\n",
      "Epoch 191/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8226 - auc: 0.9597 - accuracy: 0.5012 - val_loss: 1.8259 - val_auc: 0.9599 - val_accuracy: 0.4959\n",
      "Epoch 192/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8238 - auc: 0.9597 - accuracy: 0.5023 - val_loss: 1.8238 - val_auc: 0.9600 - val_accuracy: 0.4981\n",
      "Epoch 193/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8209 - auc: 0.9598 - accuracy: 0.5023 - val_loss: 1.8221 - val_auc: 0.9602 - val_accuracy: 0.4973\n",
      "Epoch 194/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8196 - auc: 0.9596 - accuracy: 0.5037 - val_loss: 1.8238 - val_auc: 0.9599 - val_accuracy: 0.4979\n",
      "Epoch 195/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8200 - auc: 0.9594 - accuracy: 0.5021 - val_loss: 1.8197 - val_auc: 0.9603 - val_accuracy: 0.4976\n",
      "Epoch 196/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8185 - auc: 0.9598 - accuracy: 0.5024 - val_loss: 1.8191 - val_auc: 0.9604 - val_accuracy: 0.4979\n",
      "Epoch 197/200\n",
      "597/597 [==============================] - 4s 8ms/step - loss: 1.8154 - auc: 0.9601 - accuracy: 0.5013 - val_loss: 1.8188 - val_auc: 0.9601 - val_accuracy: 0.4971\n",
      "Epoch 198/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8168 - auc: 0.9598 - accuracy: 0.5049 - val_loss: 1.8167 - val_auc: 0.9606 - val_accuracy: 0.4980\n",
      "Epoch 199/200\n",
      "597/597 [==============================] - 5s 8ms/step - loss: 1.8120 - auc: 0.9601 - accuracy: 0.5037 - val_loss: 1.8156 - val_auc: 0.9607 - val_accuracy: 0.4984\n",
      "Epoch 200/200\n",
      "597/597 [==============================] - 4s 7ms/step - loss: 1.8115 - auc: 0.9601 - accuracy: 0.5046 - val_loss: 1.8149 - val_auc: 0.9606 - val_accuracy: 0.4967\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(SGD(learning_rate = .003), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['AUC','accuracy'])\n",
    "run_hist_2 = model_2.fit(nmf_features_2, y_train, \n",
    "                         validation_data=(nmf_test_features_2, y_test), \n",
    "                         batch_size = 30,\n",
    "                         epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "regulated-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "plt.title('Model 2')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-manhattan",
   "metadata": {},
   "source": [
    "Once again we can see that the loss reduction is painfully slow, so we may need to increase learning rate or epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "supported-breath",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17910, 1303)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feature_matrix_array = final_feature_matrix.values\n",
    "final_feature_matrix_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "attached-personality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7677, 1303)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_final_features_matrix_array = test_set_final_features_matrix.values\n",
    "test_set_final_features_matrix_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-turning",
   "metadata": {},
   "source": [
    "#### Model 3 (1303 Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "behind-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(1000,input_shape = (1303,),activation = 'relu'))\n",
    "model_3.add(Dropout(0.25, seed = 1000))\n",
    "model_3.add(Dense(1000,activation = 'relu'))\n",
    "model_3.add(Dropout(0.25, seed = 500))\n",
    "model_3.add(Dense(70,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "southwest-personal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 1000)              1304000   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 70)                70070     \n",
      "=================================================================\n",
      "Total params: 2,375,070\n",
      "Trainable params: 2,375,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "utility-knight",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "597/597 [==============================] - 9s 14ms/step - loss: 4.1858 - auc: 0.6492 - accuracy: 0.1199 - val_loss: 4.1172 - val_auc: 0.7449 - val_accuracy: 0.1391\n",
      "Epoch 2/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 4.0353 - auc: 0.7578 - accuracy: 0.1392 - val_loss: 3.9414 - val_auc: 0.7668 - val_accuracy: 0.1391\n",
      "Epoch 3/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 3.8346 - auc: 0.7760 - accuracy: 0.1392 - val_loss: 3.7260 - val_auc: 0.7973 - val_accuracy: 0.1391\n",
      "Epoch 4/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 3.6501 - auc: 0.8064 - accuracy: 0.1392 - val_loss: 3.5877 - val_auc: 0.8077 - val_accuracy: 0.1391\n",
      "Epoch 5/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.5515 - auc: 0.8026 - accuracy: 0.1393 - val_loss: 3.5124 - val_auc: 0.8043 - val_accuracy: 0.1391\n",
      "Epoch 6/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.4921 - auc: 0.8119 - accuracy: 0.1410 - val_loss: 3.4641 - val_auc: 0.8221 - val_accuracy: 0.1391\n",
      "Epoch 7/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 3.4524 - auc: 0.8222 - accuracy: 0.1456 - val_loss: 3.4300 - val_auc: 0.8268 - val_accuracy: 0.1392\n",
      "Epoch 8/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.4212 - auc: 0.8265 - accuracy: 0.1526 - val_loss: 3.4011 - val_auc: 0.8305 - val_accuracy: 0.1409\n",
      "Epoch 9/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.3937 - auc: 0.8295 - accuracy: 0.1613 - val_loss: 3.3732 - val_auc: 0.8325 - val_accuracy: 0.1549\n",
      "Epoch 10/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.3674 - auc: 0.8306 - accuracy: 0.1710 - val_loss: 3.3438 - val_auc: 0.8354 - val_accuracy: 0.2007\n",
      "Epoch 11/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.3344 - auc: 0.8340 - accuracy: 0.1943 - val_loss: 3.3109 - val_auc: 0.8372 - val_accuracy: 0.1912\n",
      "Epoch 12/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.3018 - auc: 0.8362 - accuracy: 0.2052 - val_loss: 3.2742 - val_auc: 0.8393 - val_accuracy: 0.2212\n",
      "Epoch 13/200\n",
      "597/597 [==============================] - 7s 11ms/step - loss: 3.2642 - auc: 0.8380 - accuracy: 0.2197 - val_loss: 3.2337 - val_auc: 0.8416 - val_accuracy: 0.2347\n",
      "Epoch 14/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 3.2246 - auc: 0.8409 - accuracy: 0.2315 - val_loss: 3.1905 - val_auc: 0.8448 - val_accuracy: 0.2395\n",
      "Epoch 15/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 3.1792 - auc: 0.8447 - accuracy: 0.2381 - val_loss: 3.1461 - val_auc: 0.8475 - val_accuracy: 0.2416\n",
      "Epoch 16/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 3.1366 - auc: 0.8475 - accuracy: 0.2444 - val_loss: 3.1027 - val_auc: 0.8503 - val_accuracy: 0.2453\n",
      "Epoch 17/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 3.0955 - auc: 0.8515 - accuracy: 0.2515 - val_loss: 3.0610 - val_auc: 0.8534 - val_accuracy: 0.2550\n",
      "Epoch 18/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 3.0545 - auc: 0.8554 - accuracy: 0.2649 - val_loss: 3.0212 - val_auc: 0.8573 - val_accuracy: 0.2600\n",
      "Epoch 19/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 3.0172 - auc: 0.8595 - accuracy: 0.2782 - val_loss: 2.9829 - val_auc: 0.8619 - val_accuracy: 0.2769\n",
      "Epoch 20/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 2.9786 - auc: 0.8632 - accuracy: 0.2937 - val_loss: 2.9450 - val_auc: 0.8655 - val_accuracy: 0.2958\n",
      "Epoch 21/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 2.9397 - auc: 0.8672 - accuracy: 0.3147 - val_loss: 2.9067 - val_auc: 0.8700 - val_accuracy: 0.3348\n",
      "Epoch 22/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 2.9029 - auc: 0.8704 - accuracy: 0.3368 - val_loss: 2.8681 - val_auc: 0.8739 - val_accuracy: 0.3598\n",
      "Epoch 23/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 2.8625 - auc: 0.8742 - accuracy: 0.3577 - val_loss: 2.8281 - val_auc: 0.8777 - val_accuracy: 0.3804\n",
      "Epoch 24/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.8235 - auc: 0.8784 - accuracy: 0.3759 - val_loss: 2.7873 - val_auc: 0.8804 - val_accuracy: 0.3901\n",
      "Epoch 25/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.7808 - auc: 0.8817 - accuracy: 0.3883 - val_loss: 2.7449 - val_auc: 0.8853 - val_accuracy: 0.4090\n",
      "Epoch 26/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.7389 - auc: 0.8858 - accuracy: 0.4027 - val_loss: 2.7014 - val_auc: 0.8886 - val_accuracy: 0.4188\n",
      "Epoch 27/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.6949 - auc: 0.8893 - accuracy: 0.4161 - val_loss: 2.6573 - val_auc: 0.8925 - val_accuracy: 0.4279\n",
      "Epoch 28/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.6484 - auc: 0.8929 - accuracy: 0.4274 - val_loss: 2.6117 - val_auc: 0.8966 - val_accuracy: 0.4421\n",
      "Epoch 29/200\n",
      "597/597 [==============================] - 7s 13ms/step - loss: 2.6040 - auc: 0.8970 - accuracy: 0.4386 - val_loss: 2.5658 - val_auc: 0.9003 - val_accuracy: 0.4513\n",
      "Epoch 30/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.5580 - auc: 0.9007 - accuracy: 0.4479 - val_loss: 2.5192 - val_auc: 0.9037 - val_accuracy: 0.4659\n",
      "Epoch 31/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 2.5146 - auc: 0.9043 - accuracy: 0.4587 - val_loss: 2.4728 - val_auc: 0.9071 - val_accuracy: 0.4754\n",
      "Epoch 32/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.4675 - auc: 0.9085 - accuracy: 0.4666 - val_loss: 2.4269 - val_auc: 0.9108 - val_accuracy: 0.4829\n",
      "Epoch 33/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 2.4226 - auc: 0.9119 - accuracy: 0.4747 - val_loss: 2.3806 - val_auc: 0.9146 - val_accuracy: 0.4861\n",
      "Epoch 34/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 2.3782 - auc: 0.9157 - accuracy: 0.4823 - val_loss: 2.3359 - val_auc: 0.9175 - val_accuracy: 0.4950\n",
      "Epoch 35/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.3294 - auc: 0.9191 - accuracy: 0.4911 - val_loss: 2.2918 - val_auc: 0.9205 - val_accuracy: 0.5023\n",
      "Epoch 36/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.2846 - auc: 0.9226 - accuracy: 0.4970 - val_loss: 2.2487 - val_auc: 0.9236 - val_accuracy: 0.5064\n",
      "Epoch 37/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.2450 - auc: 0.9260 - accuracy: 0.5034 - val_loss: 2.2074 - val_auc: 0.9266 - val_accuracy: 0.5130\n",
      "Epoch 38/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.2063 - auc: 0.9283 - accuracy: 0.5097 - val_loss: 2.1675 - val_auc: 0.9301 - val_accuracy: 0.5222\n",
      "Epoch 39/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.1656 - auc: 0.9314 - accuracy: 0.5171 - val_loss: 2.1292 - val_auc: 0.9334 - val_accuracy: 0.5273\n",
      "Epoch 40/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.1267 - auc: 0.9346 - accuracy: 0.5228 - val_loss: 2.0915 - val_auc: 0.9361 - val_accuracy: 0.5377\n",
      "Epoch 41/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.0904 - auc: 0.9369 - accuracy: 0.5286 - val_loss: 2.0557 - val_auc: 0.9388 - val_accuracy: 0.5424\n",
      "Epoch 42/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.0569 - auc: 0.9396 - accuracy: 0.5333 - val_loss: 2.0209 - val_auc: 0.9412 - val_accuracy: 0.5470\n",
      "Epoch 43/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 2.0221 - auc: 0.9419 - accuracy: 0.5398 - val_loss: 1.9883 - val_auc: 0.9430 - val_accuracy: 0.5545\n",
      "Epoch 44/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.9893 - auc: 0.9443 - accuracy: 0.5463 - val_loss: 1.9561 - val_auc: 0.9450 - val_accuracy: 0.5612\n",
      "Epoch 45/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.9616 - auc: 0.9460 - accuracy: 0.5501 - val_loss: 1.9254 - val_auc: 0.9469 - val_accuracy: 0.5668\n",
      "Epoch 46/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 1.9262 - auc: 0.9483 - accuracy: 0.5587 - val_loss: 1.8947 - val_auc: 0.9487 - val_accuracy: 0.5720\n",
      "Epoch 47/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.8992 - auc: 0.9500 - accuracy: 0.5617 - val_loss: 1.8656 - val_auc: 0.9507 - val_accuracy: 0.5785\n",
      "Epoch 48/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 1.8691 - auc: 0.9515 - accuracy: 0.5680 - val_loss: 1.8375 - val_auc: 0.9523 - val_accuracy: 0.5873\n",
      "Epoch 49/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.8413 - auc: 0.9533 - accuracy: 0.5748 - val_loss: 1.8097 - val_auc: 0.9540 - val_accuracy: 0.5927\n",
      "Epoch 50/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.8139 - auc: 0.9551 - accuracy: 0.5842 - val_loss: 1.7833 - val_auc: 0.9548 - val_accuracy: 0.5975\n",
      "Epoch 51/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.7883 - auc: 0.9564 - accuracy: 0.5860 - val_loss: 1.7571 - val_auc: 0.9562 - val_accuracy: 0.6011\n",
      "Epoch 52/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.7615 - auc: 0.9576 - accuracy: 0.5945 - val_loss: 1.7326 - val_auc: 0.9574 - val_accuracy: 0.6065\n",
      "Epoch 53/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.7368 - auc: 0.9589 - accuracy: 0.5991 - val_loss: 1.7076 - val_auc: 0.9591 - val_accuracy: 0.6142\n",
      "Epoch 54/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.7136 - auc: 0.9600 - accuracy: 0.6049 - val_loss: 1.6833 - val_auc: 0.9603 - val_accuracy: 0.6207\n",
      "Epoch 55/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.6884 - auc: 0.9617 - accuracy: 0.6109 - val_loss: 1.6598 - val_auc: 0.9616 - val_accuracy: 0.6236\n",
      "Epoch 56/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.6646 - auc: 0.9629 - accuracy: 0.6141 - val_loss: 1.6371 - val_auc: 0.9626 - val_accuracy: 0.6318\n",
      "Epoch 57/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.6447 - auc: 0.9639 - accuracy: 0.6227 - val_loss: 1.6142 - val_auc: 0.9636 - val_accuracy: 0.6344\n",
      "Epoch 58/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.6221 - auc: 0.9646 - accuracy: 0.6272 - val_loss: 1.5917 - val_auc: 0.9651 - val_accuracy: 0.6423\n",
      "Epoch 59/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.5964 - auc: 0.9658 - accuracy: 0.6349 - val_loss: 1.5697 - val_auc: 0.9661 - val_accuracy: 0.6441\n",
      "Epoch 60/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.5797 - auc: 0.9663 - accuracy: 0.6360 - val_loss: 1.5488 - val_auc: 0.9669 - val_accuracy: 0.6505\n",
      "Epoch 61/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.5538 - auc: 0.9676 - accuracy: 0.6419 - val_loss: 1.5273 - val_auc: 0.9677 - val_accuracy: 0.6564\n",
      "Epoch 62/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.5346 - auc: 0.9682 - accuracy: 0.6482 - val_loss: 1.5073 - val_auc: 0.9687 - val_accuracy: 0.6592\n",
      "Epoch 63/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.5128 - auc: 0.9694 - accuracy: 0.6537 - val_loss: 1.4874 - val_auc: 0.9694 - val_accuracy: 0.6643\n",
      "Epoch 64/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.4935 - auc: 0.9698 - accuracy: 0.6559 - val_loss: 1.4674 - val_auc: 0.9704 - val_accuracy: 0.6681\n",
      "Epoch 65/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.4757 - auc: 0.9708 - accuracy: 0.6591 - val_loss: 1.4480 - val_auc: 0.9711 - val_accuracy: 0.6711\n",
      "Epoch 66/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.4517 - auc: 0.9716 - accuracy: 0.6681 - val_loss: 1.4281 - val_auc: 0.9717 - val_accuracy: 0.6776\n",
      "Epoch 67/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.4358 - auc: 0.9720 - accuracy: 0.6682 - val_loss: 1.4090 - val_auc: 0.9724 - val_accuracy: 0.6843\n",
      "Epoch 68/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.4153 - auc: 0.9732 - accuracy: 0.6753 - val_loss: 1.3918 - val_auc: 0.9730 - val_accuracy: 0.6854\n",
      "Epoch 69/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.3968 - auc: 0.9738 - accuracy: 0.6806 - val_loss: 1.3741 - val_auc: 0.9736 - val_accuracy: 0.6891\n",
      "Epoch 70/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.3815 - auc: 0.9743 - accuracy: 0.6842 - val_loss: 1.3554 - val_auc: 0.9741 - val_accuracy: 0.6940\n",
      "Epoch 71/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.3619 - auc: 0.9748 - accuracy: 0.6844 - val_loss: 1.3378 - val_auc: 0.9748 - val_accuracy: 0.7009\n",
      "Epoch 72/200\n",
      "597/597 [==============================] - 7s 13ms/step - loss: 1.3440 - auc: 0.9759 - accuracy: 0.6898 - val_loss: 1.3207 - val_auc: 0.9755 - val_accuracy: 0.7026\n",
      "Epoch 73/200\n",
      "597/597 [==============================] - 7s 13ms/step - loss: 1.3246 - auc: 0.9764 - accuracy: 0.6983 - val_loss: 1.3042 - val_auc: 0.9759 - val_accuracy: 0.7059\n",
      "Epoch 74/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.3100 - auc: 0.9772 - accuracy: 0.6984 - val_loss: 1.2884 - val_auc: 0.9765 - val_accuracy: 0.7070\n",
      "Epoch 75/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.2926 - auc: 0.9775 - accuracy: 0.7020 - val_loss: 1.2722 - val_auc: 0.9768 - val_accuracy: 0.7107\n",
      "Epoch 76/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.2777 - auc: 0.9781 - accuracy: 0.7035 - val_loss: 1.2564 - val_auc: 0.9774 - val_accuracy: 0.7134\n",
      "Epoch 77/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.2613 - auc: 0.9785 - accuracy: 0.7068 - val_loss: 1.2410 - val_auc: 0.9779 - val_accuracy: 0.7179\n",
      "Epoch 78/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.2474 - auc: 0.9785 - accuracy: 0.7118 - val_loss: 1.2260 - val_auc: 0.9787 - val_accuracy: 0.7225\n",
      "Epoch 79/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.2320 - auc: 0.9795 - accuracy: 0.7120 - val_loss: 1.2128 - val_auc: 0.9790 - val_accuracy: 0.7223\n",
      "Epoch 80/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.2172 - auc: 0.9800 - accuracy: 0.7154 - val_loss: 1.1974 - val_auc: 0.9794 - val_accuracy: 0.7278\n",
      "Epoch 81/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 1.1988 - auc: 0.9807 - accuracy: 0.7207 - val_loss: 1.1838 - val_auc: 0.9799 - val_accuracy: 0.7295\n",
      "Epoch 82/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.1886 - auc: 0.9809 - accuracy: 0.7233 - val_loss: 1.1704 - val_auc: 0.9805 - val_accuracy: 0.7323\n",
      "Epoch 83/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.1723 - auc: 0.9815 - accuracy: 0.7259 - val_loss: 1.1575 - val_auc: 0.9810 - val_accuracy: 0.7331\n",
      "Epoch 84/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 1.1631 - auc: 0.9817 - accuracy: 0.7271 - val_loss: 1.1449 - val_auc: 0.9813 - val_accuracy: 0.7371\n",
      "Epoch 85/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.1483 - auc: 0.9822 - accuracy: 0.7295 - val_loss: 1.1324 - val_auc: 0.9816 - val_accuracy: 0.7383\n",
      "Epoch 86/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.1373 - auc: 0.9823 - accuracy: 0.7309 - val_loss: 1.1210 - val_auc: 0.9819 - val_accuracy: 0.7410\n",
      "Epoch 87/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 1.1245 - auc: 0.9829 - accuracy: 0.7341 - val_loss: 1.1080 - val_auc: 0.9822 - val_accuracy: 0.7433\n",
      "Epoch 88/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.1124 - auc: 0.9830 - accuracy: 0.7399 - val_loss: 1.0971 - val_auc: 0.9825 - val_accuracy: 0.7465\n",
      "Epoch 89/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0966 - auc: 0.9836 - accuracy: 0.7389 - val_loss: 1.0862 - val_auc: 0.9828 - val_accuracy: 0.7482\n",
      "Epoch 90/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0878 - auc: 0.9837 - accuracy: 0.7418 - val_loss: 1.0758 - val_auc: 0.9830 - val_accuracy: 0.7498\n",
      "Epoch 91/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0742 - auc: 0.9841 - accuracy: 0.7448 - val_loss: 1.0650 - val_auc: 0.9835 - val_accuracy: 0.7508\n",
      "Epoch 92/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0664 - auc: 0.9846 - accuracy: 0.7476 - val_loss: 1.0547 - val_auc: 0.9837 - val_accuracy: 0.7520\n",
      "Epoch 93/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0555 - auc: 0.9847 - accuracy: 0.7503 - val_loss: 1.0439 - val_auc: 0.9841 - val_accuracy: 0.7534\n",
      "Epoch 94/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0435 - auc: 0.9850 - accuracy: 0.7505 - val_loss: 1.0349 - val_auc: 0.9842 - val_accuracy: 0.7539\n",
      "Epoch 95/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0357 - auc: 0.9851 - accuracy: 0.7514 - val_loss: 1.0266 - val_auc: 0.9843 - val_accuracy: 0.7568\n",
      "Epoch 96/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0221 - auc: 0.9856 - accuracy: 0.7549 - val_loss: 1.0160 - val_auc: 0.9846 - val_accuracy: 0.7586\n",
      "Epoch 97/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0170 - auc: 0.9856 - accuracy: 0.7561 - val_loss: 1.0072 - val_auc: 0.9847 - val_accuracy: 0.7602\n",
      "Epoch 98/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 1.0046 - auc: 0.9861 - accuracy: 0.7578 - val_loss: 0.9975 - val_auc: 0.9850 - val_accuracy: 0.7624\n",
      "Epoch 99/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.9972 - auc: 0.9860 - accuracy: 0.7614 - val_loss: 0.9888 - val_auc: 0.9852 - val_accuracy: 0.7644\n",
      "Epoch 100/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.9855 - auc: 0.9865 - accuracy: 0.7632 - val_loss: 0.9810 - val_auc: 0.9854 - val_accuracy: 0.7638\n",
      "Epoch 101/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9771 - auc: 0.9866 - accuracy: 0.7641 - val_loss: 0.9724 - val_auc: 0.9854 - val_accuracy: 0.7657\n",
      "Epoch 102/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9680 - auc: 0.9867 - accuracy: 0.7646 - val_loss: 0.9646 - val_auc: 0.9857 - val_accuracy: 0.7700\n",
      "Epoch 103/200\n",
      "597/597 [==============================] - 7s 13ms/step - loss: 0.9627 - auc: 0.9868 - accuracy: 0.7667 - val_loss: 0.9569 - val_auc: 0.9858 - val_accuracy: 0.7707\n",
      "Epoch 104/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9483 - auc: 0.9870 - accuracy: 0.7691 - val_loss: 0.9493 - val_auc: 0.9860 - val_accuracy: 0.7705\n",
      "Epoch 105/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9445 - auc: 0.9873 - accuracy: 0.7717 - val_loss: 0.9420 - val_auc: 0.9861 - val_accuracy: 0.7711\n",
      "Epoch 106/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9360 - auc: 0.9872 - accuracy: 0.7717 - val_loss: 0.9357 - val_auc: 0.9862 - val_accuracy: 0.7745\n",
      "Epoch 107/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9295 - auc: 0.9878 - accuracy: 0.7728 - val_loss: 0.9281 - val_auc: 0.9863 - val_accuracy: 0.7749\n",
      "Epoch 108/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9220 - auc: 0.9875 - accuracy: 0.7762 - val_loss: 0.9205 - val_auc: 0.9866 - val_accuracy: 0.7784\n",
      "Epoch 109/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.9145 - auc: 0.9879 - accuracy: 0.7748 - val_loss: 0.9127 - val_auc: 0.9869 - val_accuracy: 0.7796\n",
      "Epoch 110/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.9086 - auc: 0.9879 - accuracy: 0.7784 - val_loss: 0.9067 - val_auc: 0.9869 - val_accuracy: 0.7795\n",
      "Epoch 111/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8992 - auc: 0.9879 - accuracy: 0.7822 - val_loss: 0.9001 - val_auc: 0.9871 - val_accuracy: 0.7806\n",
      "Epoch 112/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8939 - auc: 0.9883 - accuracy: 0.7776 - val_loss: 0.8931 - val_auc: 0.9874 - val_accuracy: 0.7844\n",
      "Epoch 113/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8855 - auc: 0.9884 - accuracy: 0.7839 - val_loss: 0.8870 - val_auc: 0.9875 - val_accuracy: 0.7844\n",
      "Epoch 114/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8777 - auc: 0.9886 - accuracy: 0.7837 - val_loss: 0.8807 - val_auc: 0.9875 - val_accuracy: 0.7852\n",
      "Epoch 115/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8703 - auc: 0.9888 - accuracy: 0.7886 - val_loss: 0.8746 - val_auc: 0.9877 - val_accuracy: 0.7883\n",
      "Epoch 116/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8673 - auc: 0.9888 - accuracy: 0.7864 - val_loss: 0.8689 - val_auc: 0.9878 - val_accuracy: 0.7888\n",
      "Epoch 117/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8613 - auc: 0.9886 - accuracy: 0.7883 - val_loss: 0.8637 - val_auc: 0.9879 - val_accuracy: 0.7886\n",
      "Epoch 118/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8521 - auc: 0.9891 - accuracy: 0.7898 - val_loss: 0.8575 - val_auc: 0.9879 - val_accuracy: 0.7902\n",
      "Epoch 119/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8481 - auc: 0.9891 - accuracy: 0.7905 - val_loss: 0.8515 - val_auc: 0.9881 - val_accuracy: 0.7924\n",
      "Epoch 120/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8377 - auc: 0.9892 - accuracy: 0.7942 - val_loss: 0.8486 - val_auc: 0.9880 - val_accuracy: 0.7915\n",
      "Epoch 121/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8336 - auc: 0.9891 - accuracy: 0.7926 - val_loss: 0.8412 - val_auc: 0.9882 - val_accuracy: 0.7930\n",
      "Epoch 122/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8265 - auc: 0.9895 - accuracy: 0.7949 - val_loss: 0.8358 - val_auc: 0.9881 - val_accuracy: 0.7933\n",
      "Epoch 123/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8223 - auc: 0.9894 - accuracy: 0.7917 - val_loss: 0.8304 - val_auc: 0.9883 - val_accuracy: 0.7945\n",
      "Epoch 124/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8129 - auc: 0.9896 - accuracy: 0.7975 - val_loss: 0.8244 - val_auc: 0.9886 - val_accuracy: 0.7981\n",
      "Epoch 125/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8108 - auc: 0.9894 - accuracy: 0.8004 - val_loss: 0.8198 - val_auc: 0.9886 - val_accuracy: 0.7960\n",
      "Epoch 126/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.8069 - auc: 0.9895 - accuracy: 0.7983 - val_loss: 0.8158 - val_auc: 0.9886 - val_accuracy: 0.7974\n",
      "Epoch 127/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.7998 - auc: 0.9898 - accuracy: 0.8017 - val_loss: 0.8096 - val_auc: 0.9888 - val_accuracy: 0.7987\n",
      "Epoch 128/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.7945 - auc: 0.9897 - accuracy: 0.8025 - val_loss: 0.8058 - val_auc: 0.9888 - val_accuracy: 0.7977\n",
      "Epoch 129/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.7883 - auc: 0.9901 - accuracy: 0.8003 - val_loss: 0.8008 - val_auc: 0.9892 - val_accuracy: 0.8030\n",
      "Epoch 130/200\n",
      "597/597 [==============================] - 7s 12ms/step - loss: 0.7858 - auc: 0.9899 - accuracy: 0.8017 - val_loss: 0.7954 - val_auc: 0.9890 - val_accuracy: 0.8017\n",
      "Epoch 131/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7783 - auc: 0.9901 - accuracy: 0.8034 - val_loss: 0.7910 - val_auc: 0.9892 - val_accuracy: 0.8027\n",
      "Epoch 132/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 0.7767 - auc: 0.9904 - accuracy: 0.8032 - val_loss: 0.7859 - val_auc: 0.9894 - val_accuracy: 0.8045\n",
      "Epoch 133/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.7667 - auc: 0.9905 - accuracy: 0.8084 - val_loss: 0.7812 - val_auc: 0.9893 - val_accuracy: 0.8051\n",
      "Epoch 134/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.7636 - auc: 0.9905 - accuracy: 0.8082 - val_loss: 0.7775 - val_auc: 0.9893 - val_accuracy: 0.8060\n",
      "Epoch 135/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7592 - auc: 0.9905 - accuracy: 0.8085 - val_loss: 0.7729 - val_auc: 0.9893 - val_accuracy: 0.8059\n",
      "Epoch 136/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7570 - auc: 0.9906 - accuracy: 0.8092 - val_loss: 0.7698 - val_auc: 0.9892 - val_accuracy: 0.8067\n",
      "Epoch 137/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7537 - auc: 0.9908 - accuracy: 0.8077 - val_loss: 0.7653 - val_auc: 0.9893 - val_accuracy: 0.8073\n",
      "Epoch 138/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7470 - auc: 0.9905 - accuracy: 0.8110 - val_loss: 0.7610 - val_auc: 0.9896 - val_accuracy: 0.8086\n",
      "Epoch 139/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7424 - auc: 0.9908 - accuracy: 0.8130 - val_loss: 0.7569 - val_auc: 0.9895 - val_accuracy: 0.8094\n",
      "Epoch 140/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7355 - auc: 0.9909 - accuracy: 0.8120 - val_loss: 0.7526 - val_auc: 0.9899 - val_accuracy: 0.8132\n",
      "Epoch 141/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7307 - auc: 0.9915 - accuracy: 0.8140 - val_loss: 0.7499 - val_auc: 0.9897 - val_accuracy: 0.8114\n",
      "Epoch 142/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7271 - auc: 0.9913 - accuracy: 0.8146 - val_loss: 0.7465 - val_auc: 0.9898 - val_accuracy: 0.8124\n",
      "Epoch 143/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7243 - auc: 0.9913 - accuracy: 0.8148 - val_loss: 0.7416 - val_auc: 0.9899 - val_accuracy: 0.8146\n",
      "Epoch 144/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 0.7176 - auc: 0.9910 - accuracy: 0.8165 - val_loss: 0.7374 - val_auc: 0.9900 - val_accuracy: 0.8139\n",
      "Epoch 145/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7151 - auc: 0.9916 - accuracy: 0.8163 - val_loss: 0.7348 - val_auc: 0.9900 - val_accuracy: 0.8137\n",
      "Epoch 146/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7130 - auc: 0.9914 - accuracy: 0.8171 - val_loss: 0.7307 - val_auc: 0.9901 - val_accuracy: 0.8161\n",
      "Epoch 147/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7032 - auc: 0.9916 - accuracy: 0.8193 - val_loss: 0.7264 - val_auc: 0.9904 - val_accuracy: 0.8167\n",
      "Epoch 148/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.7030 - auc: 0.9916 - accuracy: 0.8183 - val_loss: 0.7232 - val_auc: 0.9905 - val_accuracy: 0.8180\n",
      "Epoch 149/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.6983 - auc: 0.9917 - accuracy: 0.8199 - val_loss: 0.7203 - val_auc: 0.9905 - val_accuracy: 0.8180\n",
      "Epoch 150/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.6948 - auc: 0.9918 - accuracy: 0.8213 - val_loss: 0.7165 - val_auc: 0.9905 - val_accuracy: 0.8191\n",
      "Epoch 151/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.6894 - auc: 0.9917 - accuracy: 0.8221 - val_loss: 0.7137 - val_auc: 0.9905 - val_accuracy: 0.8196\n",
      "Epoch 152/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.6868 - auc: 0.9919 - accuracy: 0.8217 - val_loss: 0.7100 - val_auc: 0.9905 - val_accuracy: 0.8196\n",
      "Epoch 153/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.6796 - auc: 0.9922 - accuracy: 0.8238 - val_loss: 0.7062 - val_auc: 0.9906 - val_accuracy: 0.8215\n",
      "Epoch 154/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 0.6765 - auc: 0.9920 - accuracy: 0.8240 - val_loss: 0.7027 - val_auc: 0.9907 - val_accuracy: 0.8225\n",
      "Epoch 155/200\n",
      "597/597 [==============================] - 8s 14ms/step - loss: 0.6754 - auc: 0.9921 - accuracy: 0.8227 - val_loss: 0.6996 - val_auc: 0.9907 - val_accuracy: 0.8227\n",
      "Epoch 156/200\n",
      "597/597 [==============================] - 8s 13ms/step - loss: 0.6744 - auc: 0.9920 - accuracy: 0.8238 - val_loss: 0.6966 - val_auc: 0.9907 - val_accuracy: 0.8240\n",
      "Epoch 157/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6705 - auc: 0.9921 - accuracy: 0.8251 - val_loss: 0.6937 - val_auc: 0.9908 - val_accuracy: 0.8251\n",
      "Epoch 158/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6659 - auc: 0.9924 - accuracy: 0.8275 - val_loss: 0.6910 - val_auc: 0.9909 - val_accuracy: 0.8264\n",
      "Epoch 159/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6638 - auc: 0.9921 - accuracy: 0.8271 - val_loss: 0.6882 - val_auc: 0.9910 - val_accuracy: 0.8244\n",
      "Epoch 160/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6563 - auc: 0.9922 - accuracy: 0.8281 - val_loss: 0.6849 - val_auc: 0.9911 - val_accuracy: 0.8258\n",
      "Epoch 161/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6533 - auc: 0.9923 - accuracy: 0.8302 - val_loss: 0.6818 - val_auc: 0.9910 - val_accuracy: 0.8286\n",
      "Epoch 162/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6490 - auc: 0.9926 - accuracy: 0.8304 - val_loss: 0.6797 - val_auc: 0.9910 - val_accuracy: 0.8286\n",
      "Epoch 163/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6465 - auc: 0.9926 - accuracy: 0.8307 - val_loss: 0.6761 - val_auc: 0.9910 - val_accuracy: 0.8300\n",
      "Epoch 164/200\n",
      "597/597 [==============================] - 10s 17ms/step - loss: 0.6440 - auc: 0.9926 - accuracy: 0.8310 - val_loss: 0.6735 - val_auc: 0.9912 - val_accuracy: 0.8292\n",
      "Epoch 165/200\n",
      "597/597 [==============================] - 10s 17ms/step - loss: 0.6405 - auc: 0.9925 - accuracy: 0.8328 - val_loss: 0.6704 - val_auc: 0.9912 - val_accuracy: 0.8311\n",
      "Epoch 166/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6375 - auc: 0.9927 - accuracy: 0.8331 - val_loss: 0.6680 - val_auc: 0.9911 - val_accuracy: 0.8304\n",
      "Epoch 167/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6337 - auc: 0.9928 - accuracy: 0.8325 - val_loss: 0.6649 - val_auc: 0.9912 - val_accuracy: 0.8314\n",
      "Epoch 168/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6305 - auc: 0.9930 - accuracy: 0.8315 - val_loss: 0.6625 - val_auc: 0.9912 - val_accuracy: 0.8322\n",
      "Epoch 169/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6270 - auc: 0.9929 - accuracy: 0.8357 - val_loss: 0.6595 - val_auc: 0.9913 - val_accuracy: 0.8331\n",
      "Epoch 170/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6248 - auc: 0.9928 - accuracy: 0.8339 - val_loss: 0.6577 - val_auc: 0.9914 - val_accuracy: 0.8338\n",
      "Epoch 171/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6208 - auc: 0.9930 - accuracy: 0.8362 - val_loss: 0.6558 - val_auc: 0.9913 - val_accuracy: 0.8330\n",
      "Epoch 172/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6171 - auc: 0.9929 - accuracy: 0.8366 - val_loss: 0.6518 - val_auc: 0.9914 - val_accuracy: 0.8342\n",
      "Epoch 173/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6158 - auc: 0.9929 - accuracy: 0.8370 - val_loss: 0.6491 - val_auc: 0.9914 - val_accuracy: 0.8339\n",
      "Epoch 174/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6102 - auc: 0.9932 - accuracy: 0.8375 - val_loss: 0.6472 - val_auc: 0.9915 - val_accuracy: 0.8356\n",
      "Epoch 175/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.6087 - auc: 0.9932 - accuracy: 0.8400 - val_loss: 0.6440 - val_auc: 0.9915 - val_accuracy: 0.8340\n",
      "Epoch 176/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6092 - auc: 0.9927 - accuracy: 0.8406 - val_loss: 0.6418 - val_auc: 0.9915 - val_accuracy: 0.8357\n",
      "Epoch 177/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6052 - auc: 0.9931 - accuracy: 0.8398 - val_loss: 0.6396 - val_auc: 0.9915 - val_accuracy: 0.8380\n",
      "Epoch 178/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.6003 - auc: 0.9933 - accuracy: 0.8414 - val_loss: 0.6385 - val_auc: 0.9915 - val_accuracy: 0.8361\n",
      "Epoch 179/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5947 - auc: 0.9937 - accuracy: 0.8421 - val_loss: 0.6350 - val_auc: 0.9914 - val_accuracy: 0.8364\n",
      "Epoch 180/200\n",
      "597/597 [==============================] - 10s 17ms/step - loss: 0.5927 - auc: 0.9934 - accuracy: 0.8430 - val_loss: 0.6331 - val_auc: 0.9916 - val_accuracy: 0.8382\n",
      "Epoch 181/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5928 - auc: 0.9933 - accuracy: 0.8430 - val_loss: 0.6315 - val_auc: 0.9915 - val_accuracy: 0.8378\n",
      "Epoch 182/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5906 - auc: 0.9933 - accuracy: 0.8429 - val_loss: 0.6290 - val_auc: 0.9916 - val_accuracy: 0.8385\n",
      "Epoch 183/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5876 - auc: 0.9937 - accuracy: 0.8438 - val_loss: 0.6269 - val_auc: 0.9915 - val_accuracy: 0.8380\n",
      "Epoch 184/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5815 - auc: 0.9935 - accuracy: 0.8433 - val_loss: 0.6243 - val_auc: 0.9916 - val_accuracy: 0.8393\n",
      "Epoch 185/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5798 - auc: 0.9937 - accuracy: 0.8448 - val_loss: 0.6224 - val_auc: 0.9917 - val_accuracy: 0.8403\n",
      "Epoch 186/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5829 - auc: 0.9935 - accuracy: 0.8442 - val_loss: 0.6198 - val_auc: 0.9917 - val_accuracy: 0.8403\n",
      "Epoch 187/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5767 - auc: 0.9938 - accuracy: 0.8452 - val_loss: 0.6179 - val_auc: 0.9916 - val_accuracy: 0.8397\n",
      "Epoch 188/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5746 - auc: 0.9937 - accuracy: 0.8441 - val_loss: 0.6170 - val_auc: 0.9916 - val_accuracy: 0.8406\n",
      "Epoch 189/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5719 - auc: 0.9940 - accuracy: 0.8445 - val_loss: 0.6144 - val_auc: 0.9916 - val_accuracy: 0.8397\n",
      "Epoch 190/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5715 - auc: 0.9940 - accuracy: 0.8483 - val_loss: 0.6123 - val_auc: 0.9917 - val_accuracy: 0.8398\n",
      "Epoch 191/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5669 - auc: 0.9939 - accuracy: 0.8470 - val_loss: 0.6113 - val_auc: 0.9916 - val_accuracy: 0.8404\n",
      "Epoch 192/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5641 - auc: 0.9941 - accuracy: 0.8479 - val_loss: 0.6080 - val_auc: 0.9916 - val_accuracy: 0.8413\n",
      "Epoch 193/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5625 - auc: 0.9939 - accuracy: 0.8491 - val_loss: 0.6066 - val_auc: 0.9916 - val_accuracy: 0.8402\n",
      "Epoch 194/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5634 - auc: 0.9937 - accuracy: 0.8494 - val_loss: 0.6046 - val_auc: 0.9917 - val_accuracy: 0.8421\n",
      "Epoch 195/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5603 - auc: 0.9939 - accuracy: 0.8500 - val_loss: 0.6028 - val_auc: 0.9917 - val_accuracy: 0.8421\n",
      "Epoch 196/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5574 - auc: 0.9940 - accuracy: 0.8495 - val_loss: 0.6008 - val_auc: 0.9917 - val_accuracy: 0.8429\n",
      "Epoch 197/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5527 - auc: 0.9942 - accuracy: 0.8513 - val_loss: 0.6000 - val_auc: 0.9918 - val_accuracy: 0.8419\n",
      "Epoch 198/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5547 - auc: 0.9939 - accuracy: 0.8505 - val_loss: 0.5990 - val_auc: 0.9917 - val_accuracy: 0.8437\n",
      "Epoch 199/200\n",
      "597/597 [==============================] - 9s 16ms/step - loss: 0.5507 - auc: 0.9941 - accuracy: 0.8511 - val_loss: 0.5959 - val_auc: 0.9919 - val_accuracy: 0.8436\n",
      "Epoch 200/200\n",
      "597/597 [==============================] - 9s 15ms/step - loss: 0.5449 - auc: 0.9945 - accuracy: 0.8543 - val_loss: 0.5939 - val_auc: 0.9918 - val_accuracy: 0.8430\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(SGD(learning_rate = .003), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['AUC','accuracy'])\n",
    "run_hist_3 = model_3.fit(final_feature_matrix_array, y_train, \n",
    "                         validation_data=(test_set_final_features_matrix_array, y_test), \n",
    "                         batch_size = 30,\n",
    "                         epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "verified-albert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_3.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_3.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "plt.title('Model 3')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-palestinian",
   "metadata": {},
   "source": [
    "So, in this model we actually started seeing a deviation in train and validation loss, which probably means we have acheived optimal modelling performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-revolution",
   "metadata": {},
   "source": [
    "Let us create a small function to generate a dataframe for comparing all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "driving-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_model_1 = model_1.predict(nmf_test_features_1)>0.5\n",
    "y_pred_model_1_prob = model_1.predict(nmf_test_features_1)\n",
    "\n",
    "y_pred_model_2 = model_2.predict(nmf_test_features_2)>0.5\n",
    "y_pred_model_2_prob = model_2.predict(nmf_test_features_2)\n",
    "\n",
    "y_pred_model_3 = model_3.predict(test_set_final_features_matrix_array)>0.5\n",
    "y_pred_model_3_prob = model_3.predict(test_set_final_features_matrix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "expanded-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, \\\n",
    "roc_curve, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "desperate-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(model, y_true, y_pred, label_encoder):\n",
    "    ML_model = pd.DataFrame(classification_report(y_test, y_pred,\n",
    "                                                  output_dict = True, \n",
    "                                                  target_names = list(label_encoder.classes_),\n",
    "                                                  zero_division= 0\n",
    "                                                 )).T\n",
    "    level0 = pd.MultiIndex.from_product([[model], ML_model.columns])\n",
    "    ML_model.columns = level0\n",
    "    return ML_model.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "tired-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_1 = model_report('NN Model 1 (n_components = 5)', y_test, y_pred_model_1, labelencoder)\n",
    "NN_model_2 = model_report('NN Model 2 (n_components = 10)', y_test, y_pred_model_2, labelencoder)\n",
    "NN_model_3 = model_report('NN Model 3 (n_components = 1303)', y_test, y_pred_model_3, labelencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "understood-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparision_table = pd.concat([NN_model_1, NN_model_2, NN_model_3], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "victorian-evolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">NN Model 1 (n_components = 5)</th>\n",
       "      <th colspan=\"4\" halign=\"left\">NN Model 2 (n_components = 10)</th>\n",
       "      <th colspan=\"4\" halign=\"left\">NN Model 3 (n_components = 1303)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.704947</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.691323</td>\n",
       "      <td>0.351830</td>\n",
       "      <td>0.466333</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.889822</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.827432</td>\n",
       "      <td>7677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.050067</td>\n",
       "      <td>0.044537</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.098482</td>\n",
       "      <td>0.079131</td>\n",
       "      <td>0.082222</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.555545</td>\n",
       "      <td>0.426130</td>\n",
       "      <td>0.463574</td>\n",
       "      <td>7677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.262660</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.251446</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.380102</td>\n",
       "      <td>0.351830</td>\n",
       "      <td>0.351640</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.850100</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.794048</td>\n",
       "      <td>7677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.351830</td>\n",
       "      <td>0.351830</td>\n",
       "      <td>0.351830</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>7677.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             NN Model 1 (n_components = 5)                              \\\n",
       "                                 precision    recall  f1-score support   \n",
       "micro avg                         0.704947  0.259867  0.379747  7677.0   \n",
       "macro avg                         0.050067  0.044537  0.044189  7677.0   \n",
       "weighted avg                      0.262660  0.259867  0.251446  7677.0   \n",
       "samples avg                       0.259867  0.259867  0.259867  7677.0   \n",
       "\n",
       "             NN Model 2 (n_components = 10)                              \\\n",
       "                                  precision    recall  f1-score support   \n",
       "micro avg                          0.691323  0.351830  0.466333  7677.0   \n",
       "macro avg                          0.098482  0.079131  0.082222  7677.0   \n",
       "weighted avg                       0.380102  0.351830  0.351640  7677.0   \n",
       "samples avg                        0.351830  0.351830  0.351830  7677.0   \n",
       "\n",
       "             NN Model 3 (n_components = 1303)                              \n",
       "                                    precision    recall  f1-score support  \n",
       "micro avg                            0.889822  0.773219  0.827432  7677.0  \n",
       "macro avg                            0.555545  0.426130  0.463574  7677.0  \n",
       "weighted avg                         0.850100  0.773219  0.794048  7677.0  \n",
       "samples avg                          0.773219  0.773219  0.773219  7677.0  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparision_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-glasgow",
   "metadata": {},
   "source": [
    "## Model Evaluation and Selection - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-spotlight",
   "metadata": {},
   "source": [
    "We tried to train 3 models,\n",
    ">1. Model 1 (n_components = 5) \n",
    ">2. Model 2 (n_compenents = 10)\n",
    ">3. Model 3 (n_compoenents = 1303)\n",
    "\n",
    "We set forth our expectations clearly,\n",
    ">1. High Precision and Recall\n",
    ">2. Higher micro average f1-score.\n",
    "\n",
    "Both of which were met by Model 3 with 1303 components (input features). The model had micro precison, recall and f1 score of 0.889, 0.773 and 0.827 respectively\n",
    "\n",
    "In general, Micro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).\n",
    "\n",
    "In our case, class imbalance is definitely present and a high micro average definitely preferred, which our model acheives.\n",
    "\n",
    "As explained earlier, there is huge imbalance in dataset and trying to classify each minority class correctly may lead to overfitting. Thus it would be better to avoid false alarms by opting for better precision. Additionally, correct prediction of majority class was must; let me explain in details\n",
    "\n",
    ">Almost all safety systems are based on basic philosphy of identifying the root causes and correcting them. This means that if you have truly identified a root cause and treated it appropiately, there would be no repition of similar observation. This is of course an ideal situation. However, in real life these incidents may stll recur and it may not be possible to completely eliminate them due to various factors. Thus the next best measure which is adopted by most organisations is counting repetition of root causes. Higher the count more serious the issue and thus more immediate action required. This frequency is then multiplied with probable consequence to get the actual risk of incident. However, this model is concerned with only getting the frequency right.\n",
    "\n",
    "<P>Thus, it make sense to identify majority classes with higher precision and recall, hence the chose of measure - Micro F1 Score for selecting best estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-feelings",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-california",
   "metadata": {},
   "source": [
    "In conclusion, we chose Model 3 for purpose of deployment for future predictions. The model not only shows great prediction power, it gives lower false positives at the same time has higher micro average f1-score, precision and recall.\n",
    "\n",
    "Going further, in future we may improve our modelling performance by,\n",
    "\n",
    "* Oversampling or Undersampling, or combining both to check whether we can acheive better classification.\n",
    "* At the time of feature extraction we only used unigrams and bigrams, we can also use further higher degree of feature such as trigram (combination of three words)\n",
    "* While training models with lesser components, the accuracy achieved was very poor. Additionally, the change or decrease in losses was very slow. Thus we may try increasing the learning rate to check where actually we acheive optimal results for Model 1 (5 components) and Model 2 (10 components).\n",
    "* We may also try to increase the number of epochs parallelly to check the convergence.\n",
    "* The Error metrics used for fitting were AUC and Accuracy. We may try other metrics; refer - https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "* Finally we can actaully re-work on our input data to maybe eliminate some of the labels by combining with other similar labels, thus reducing some granularity in data when not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-improvement",
   "metadata": {},
   "source": [
    "## Thank You, for visiting my page and going through this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-replication",
   "metadata": {},
   "source": [
    "# CIAO !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
